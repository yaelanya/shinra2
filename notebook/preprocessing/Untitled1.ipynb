{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import MeCab\n",
    "\n",
    "from gensim.parsing.preprocessing import strip_tags, split_alphanum, remove_stopwords, strip_multiple_whitespaces, strip_punctuation\n",
    "from gensim.summarization.textcleaner import clean_text_by_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda multi_list: [item for sublist in multi_list for item in sublist if (not isinstance(item, str)) or (len(item) is not 0)]\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def labeling(sentence_df: pd.DataFrame, train_dict: dict):\n",
    "    _sentence_df = sentence_df.assign(label = False)\n",
    "    for _id, train_values in train_dict.items():\n",
    "        if len(train_values) is 0:\n",
    "            continue\n",
    "\n",
    "        _sentence_df.loc[_sentence_df._id == _id, 'label'] = \\\n",
    "            _sentence_df.loc[_sentence_df._id == _id].sentence.str.contains(isin_pat(train_values))\n",
    "\n",
    "    return _sentence_df\n",
    "\n",
    "def get_annotation(annotation_data: list, attribute: str):\n",
    "    train_dict = {}\n",
    "    for entry in annotation_data:\n",
    "        train_dict[str(entry['WikipediaID'])] = flatten([re.findall(r'([^。]+)', item) for item in entry['Attributes'][attribute]])\n",
    "\n",
    "    return train_dict\n",
    "\n",
    "def isin_pat(matching: [str, list]):\n",
    "    if isinstance(matching, str):\n",
    "        return re.escape(\"%s\" % str)\n",
    "    elif isinstance(matching, list):\n",
    "        return \"|\".join([re.escape(t) for t in matching])\n",
    "\n",
    "def wakati(s: str):\n",
    "    return [w for w in mecab.parse(s).strip().split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(s):\n",
    "    _s = strip_tags(s.lower())\n",
    "    _s = split_alphanum(_s)\n",
    "    _s = remove_stopwords(_s)\n",
    "    _s = strip_punctuation(_s)\n",
    "    _s = strip_multiple_whitespaces(_s)\n",
    "    \n",
    "    return _s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv(\"../../data/train.csv\", dtype={'_id': str})\n",
    "valid_df = pd.read_csv(\"../../data/valid.csv\", dtype={'_id': str})\n",
    "\n",
    "with open(\"../../data/compound_train.json\", 'r', encoding='utf-8') as f:\n",
    "    train_raw = json.load(f)['entry']\n",
    "    \n",
    "with open(\"../../data/cas_number.json\", 'r') as f:\n",
    "    pageid2cas_table = json.load(f)\n",
    "\n",
    "with open(\"../../data/cas2cid.json\", 'r') as f:\n",
    "    cas2cid_table = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製造方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sentences: 7435\n",
      "True: 508 \tFalse: 6927\n",
      "Number of valid sentences: 1564\n",
      "True: 88 \tFalse: 1476\n"
     ]
    }
   ],
   "source": [
    "train_manufacturing_dict = get_annotation(train_raw, '製造方法')\n",
    "train_df = labeling(train_df, train_manufacturing_dict)\n",
    "valid_df = labeling(valid_df, train_manufacturing_dict)\n",
    "\n",
    "print(\"Number of train sentences:\", len(train_df))\n",
    "print(\"True:\", len(train_df[train_df.label == True]), \"\\tFalse:\", len(train_df[train_df.label == False]))\n",
    "print(\"Number of valid sentences:\", len(valid_df))\n",
    "print(\"True:\", len(valid_df[valid_df.label == True]), \"\\tFalse:\", len(valid_df[valid_df.label == False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/manufacturings.pkl\", 'rb') as f:\n",
    "    manufacturings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文を文ごとに分割しクリーニングする\n",
    "m_sentences = {}\n",
    "for cid, methods in manufacturings.items():\n",
    "    m_sentences[cid] = [clean_sentence(s.text) for doc in methods for s in clean_text_by_sentences(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['personal sampler gases air adapted measurement nitrogen dioxide ',\n",
       " ' nitrogen dioxide ',\n",
       " 'analyte nitrogen dioxide matrix air collection tea triethanolamine coated molecular sieve desorption tea ',\n",
       " ' nitrogen dioxide ',\n",
       " 'sampler passive palmes tube triethanolamine treated screens sampling time min 15 minutes 5 ppm max 8 hr 10 ppm ',\n",
       " 'sample stability use sampler 1 mo preparation analyze 1 month sampling ',\n",
       " 'range studied 1 2 80 ppm hr 0 13 8 5 ug nitrogen dioxide sample overall precision 0 06 ',\n",
       " ' nitrogen dioxide ']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_sentences['25352']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageid2manufacturing_table = \\\n",
    "{pageid: m_sentences.get(str(cid))\n",
    "    for pageid, cas_list in pageid2cas_table.items()\n",
    "    for cas in cas_list\n",
    "    for cid in (cas2cid_table.get(cas) if cas2cid_table.get(cas) else [])\n",
    "    if m_sentences.get(str(cid))\n",
    "}\n",
    "\n",
    "manufacturing_df = pd.DataFrame()\n",
    "for pageid, methods in pageid2manufacturing_table.items():\n",
    "    manufacturing_df = manufacturing_df.append(pd.DataFrame({'_id': [pageid] * len(methods), 'manufacturing': methods}))\n",
    "\n",
    "manufacturing_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, manufacturing_df, on='_id')\n",
    "valid_df = pd.merge(valid_df, manufacturing_df, on='_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文の分かち書き\n",
    "train_df['manufacturing_words'] = train_df.manufacturing.str.split()\n",
    "valid_df['manufacturing_words'] = valid_df.manufacturing.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語文の分かち書き\n",
    "train_df['words'] = train_df.sentence.apply(lambda x: wakati(x))\n",
    "valid_df['words'] = valid_df.sentence.apply(lambda x: wakati(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../../data/train_split_words.csv\", index=False)\n",
    "valid_df.to_csv(\"../../data/valid_split_words.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
