{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1230d73d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, tag_to_ix):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding_dim = 5\n",
    "        self.hidden_dim = 4\n",
    "        \n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.word_lstm_units = word_lstm_units\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_lstm_units = char_lstm_units\n",
    "        self.n_class = len(tag_to_ix) + 2  # +2 means <START> and <STOP> tags.\n",
    "        \n",
    "        # Character input\n",
    "        self.char_embeddings = nn.Embedding(num_embeddings=char_vocab_size\n",
    "                                            , embedding_dim=char_emb_dim\n",
    "                                            , padding_idx=0\n",
    "                                           )\n",
    "        self.char_lstm = nn.LSTM(input_size=char_emb_dim\n",
    "                                 , hidden_size=char_lstm_units\n",
    "                                 , bidirectional=True\n",
    "                                )\n",
    "        \n",
    "        # Word input\n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=word_vocab_size\n",
    "                                            , embedding_dim=word_emb_dim\n",
    "                                            , padding_idx=0\n",
    "                                           )\n",
    "        \n",
    "        self.word_lstm = nn.LSTM(input_size=word_emb_dim + (char_emb_dim * 2)\n",
    "                                 , hidden_size=word_lstm_units\n",
    "                                 , bidirectional=True\n",
    "                                )\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=word_lstm_units, out_features=self.n_class)\n",
    "\n",
    "        self.crf = CRF.CRF(tag_to_ix)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_feats = self._get_lstm_features(x)\n",
    "        score, tag_seq = self.crf(lstm_feats)\n",
    "        return lstm_feats, tag_seq\n",
    "    \n",
    "    def _get_lstm_features(self, chars, char_lengths):\n",
    "        '''\n",
    "        chars: (sentence_length, word_length) \n",
    "        char_lengths: (sentence_length)\n",
    "        '''\n",
    "        char_lengths, sorted_index = char_lengths.sort(0, descending=True)\n",
    "        \n",
    "        char_embs = self.char_embeddings(chars[sorted_index])\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(char_embs, word_lengths, batch_first=True)\n",
    "        \n",
    "        l_packed_out, _ = self.char_lstm(packed)\n",
    "        l_output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(l_packed_out)\n",
    "        l_output.transpose_(0, 1)\n",
    "        \n",
    "        l_char_embs = torch.Tensor(torch.zeros((output.size(0), output.size(2))))\n",
    "        l_char_embs[sorted_index] = torch.Tensor([self._cat_lstm_last(char_feat, length) for word_feat, length in zip(l_output, char_lengths)])\n",
    "        \n",
    "        \n",
    "    def _cat_lstm_last(self, output, length):\n",
    "        return torch.cat((output[length - 1, :self.char_lstm_dim], output[0, self.char_lstm_dim:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 4.,  5.,  7.]],\n",
       "\n",
       "        [[10., 20., 31.],\n",
       "         [14., 15., 61.],\n",
       "         [41., 15., 17.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[[1, 2, 3], [4, 5, 6], [4, 5, 7]], [[10, 20, 31], [14, 15, 61], [41, 15, 17]]])\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pack_padded_sequence() missing 1 required positional argument: 'lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-daa854043b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: pack_padded_sequence() missing 1 required positional argument: 'lengths'"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5])\n",
    "c = torch.Tensor([6])\n",
    "torch.nn.utils.rnn.pack_padded_sequence([10, 10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
