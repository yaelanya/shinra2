{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1230d73d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self\n",
    "                 , tag_to_ix\n",
    "                 , word_vocab_size\n",
    "                 , word_emb_dim\n",
    "                 , word_lstm_units\n",
    "                 , char_vocab_size\n",
    "                 , char_emb_dim\n",
    "                 , char_lstm_units\n",
    "                ):\n",
    "        super(Model, self).__init__()\n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.word_emb_dim = word_emb_dim\n",
    "        self.word_lstm_units = word_lstm_units\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_lstm_units = char_lstm_units\n",
    "        self.n_class = len(tag_to_ix) + 2  # +2 means <START> and <STOP> tags.\n",
    "        \n",
    "        # Character input\n",
    "        self.char_embeddings = nn.Embedding(num_embeddings=char_vocab_size\n",
    "                                            , embedding_dim=char_emb_dim\n",
    "                                            , padding_idx=0\n",
    "                                           )\n",
    "        self.char_lstm = nn.LSTM(input_size=char_emb_dim\n",
    "                                 , hidden_size=char_lstm_units\n",
    "                                 , bidirectional=True\n",
    "                                 , batch_first=True\n",
    "                                )\n",
    "        \n",
    "        # Word input\n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=word_vocab_size\n",
    "                                            , embedding_dim=word_emb_dim\n",
    "                                            , padding_idx=0\n",
    "                                           )\n",
    "        \n",
    "        self.word_lstm = nn.LSTM(input_size=word_emb_dim + (char_emb_dim * 2)\n",
    "                                 , hidden_size=word_lstm_units\n",
    "                                 , bidirectional=True\n",
    "                                 , batch_first=True\n",
    "                                )\n",
    "        \n",
    "        self.hidden_to_tag = nn.Linear(in_features=word_lstm_units, out_features=self.n_class)\n",
    "\n",
    "        self.crf = CRF.CRF(tag_to_ix)\n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        lstm_feats = self._get_word_lstm_features(sentence)\n",
    "        feats = self.hidden_to_tag(lstm_feats)\n",
    "        _, tag_seq = self.crf(feats)\n",
    "        \n",
    "        return feats, tag_seq\n",
    "    \n",
    "    def _get_word_lstm_features(self, sentence, chars):\n",
    "        '''\n",
    "        sentence: (sentence_length) \n",
    "        chars: (sentence_length, word_length)\n",
    "        '''\n",
    "        word_embs = self.word_embeddings(sentence)\n",
    "        chars_embs = self._get_char_lstm_features(chars)\n",
    "        \n",
    "        return torch.cat((word_embs, chars_embs), 1)\n",
    "    \n",
    "    def _get_char_lstm_features(self, chars):\n",
    "        '''\n",
    "        chars: (sentence_length, word_length) \n",
    "        '''\n",
    "        #chars = chars.view(self.batch_size * chars.size()[1], -1)\n",
    "        \n",
    "        char_lengths = (chars > 0).sum(1)\n",
    "        char_lengths, sorted_index = char_lengths.sort(0, descending=True)\n",
    "        \n",
    "        char_embs = self.char_embeddings(chars[sorted_index])\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(char_embs, word_lengths, batch_first=True)\n",
    "        \n",
    "        l_packed_out, _ = self.char_lstm(packed)\n",
    "        l_output, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(l_packed_out, batch_first=True)\n",
    "        \n",
    "        l_char_embs = torch.Tensor(torch.zeros((l_output.size(0), l_output.size(2))))\n",
    "        l_char_embs[sorted_index] = torch.stack([self._cat_lstm_last(word_feat, length) for word_feat, length in zip(l_output, char_lengths)])\n",
    "        \n",
    "        return l_char_embs\n",
    "        \n",
    "    def _cat_lstm_last(self, output, length):\n",
    "        return torch.cat((output[length - 1, :self.char_lstm_dim], output[0, self.char_lstm_dim:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (char_embeddings): Embedding(4, 5, padding_idx=0)\n",
       "  (char_lstm): LSTM(5, 6, batch_first=True, bidirectional=True)\n",
       "  (word_embeddings): Embedding(1, 2, padding_idx=0)\n",
       "  (word_lstm): LSTM(12, 3, batch_first=True, bidirectional=True)\n",
       "  (hidden_to_tag): Linear(in_features=3, out_features=3, bias=True)\n",
       "  (crf): CRF()\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model({1: 2} ,1, 2, 3, 4, 5, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
