{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Embedding, TimeDistributed, Bidirectional, LSTM, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras_contrib.layers import CRF\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 1024\n",
    "CHAR_VOCAB_SIZE = None\n",
    "CHAR_EMBEDDING_SIZE = 25\n",
    "CHAR_LSTM_UNITS = 25\n",
    "WORD_VOCAB_SIZE = None\n",
    "WORD_EMBEDDING_SIZE = 100\n",
    "WORD_LSTM_UNITS = 100\n",
    "NUM_TAGS = 4\n",
    "\n",
    "TMP_NUM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = Input(batch_shape=(None, None), dtype='int32')\n",
    "char_ids = Input(batch_shape=(None, None, None), dtype='int32')\n",
    "elmo_embeddings = Input(shape=(None, EMBEDDING_DIM), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embeddings = Embedding(input_dim=TMP_NUM\n",
    "                            , output_dim=CHAR_EMBEDDING_SIZE\n",
    "                            , mask_zero=True)(char_ids)\n",
    "char_embeddings = TimeDistributed(Bidirectional(LSTM(CHAR_LSTM_UNITS)))(char_embeddings)\n",
    "\n",
    "word_embeddings = Embedding(input_dim=TMP_NUM\n",
    "                            , output_dim=WORD_EMBEDDING_SIZE\n",
    "                            , mask_zero=True)(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = concatenate([char_embeddings, word_embeddings, elmo_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Bidirectional(LSTM(WORD_LSTM_UNITS, return_sequences=True))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(WORD_LSTM_UNITS, activation='tanh')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(NUM_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = crf.loss_function\n",
    "predict = crf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[word_ids, char_ids, elmo_embeddings], outputs=predict)\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, None, 2 3200        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     10200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    12800       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 1024)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 1174)   0           time_distributed_1[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 200)    1020000     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      428         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,066,728\n",
      "Trainable params: 1,066,728\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d8a1e8c105e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mIndexTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \"\"\"Convert a collection of raw documents to a document id matrix.\n\u001b[1;32m    155\u001b[0m     \u001b[0mAttributes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"A vocabulary that maps tokens to ints (storing a vocabulary).\n",
    "    Attributes:\n",
    "        _token_count: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocabulary.\n",
    "        _token2id: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        _id2token: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size=None, lower=True, unk_token=True, specials=('<pad>',)):\n",
    "        \"\"\"Create a Vocabulary object.\n",
    "        Args:\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            lower: boolean. Whether to convert the texts to lowercase.\n",
    "            unk_token: boolean. Whether to add unknown token.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary. Default: ('<pad>',)\n",
    "        \"\"\"\n",
    "        self._max_size = max_size\n",
    "        self._lower = lower\n",
    "        self._unk = unk_token\n",
    "        self._token2id = {token: i for i, token in enumerate(specials)}\n",
    "        self._id2token = list(specials)\n",
    "        self._token_count = Counter()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token2id)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Add token to vocabulary.\n",
    "        Args:\n",
    "            token (str): token to add.\n",
    "        \"\"\"\n",
    "        token = self.process_token(token)\n",
    "        self._token_count.update([token])\n",
    "\n",
    "    def add_documents(self, docs):\n",
    "        \"\"\"Update dictionary from a collection of documents. Each document is a list\n",
    "        of tokens.\n",
    "        Args:\n",
    "            docs (list): documents to add.\n",
    "        \"\"\"\n",
    "        for sent in docs:\n",
    "            sent = map(self.process_token, sent)\n",
    "            self._token_count.update(sent)\n",
    "\n",
    "    def doc2id(self, doc):\n",
    "        \"\"\"Get the list of token_id given doc.\n",
    "        Args:\n",
    "            doc (list): document.\n",
    "        Returns:\n",
    "            list: int id of doc.\n",
    "        \"\"\"\n",
    "        doc = map(self.process_token, doc)\n",
    "        return [self.token_to_id(token) for token in doc]\n",
    "\n",
    "    def id2doc(self, ids):\n",
    "        \"\"\"Get the token list.\n",
    "        Args:\n",
    "            ids (list): token ids.\n",
    "        Returns:\n",
    "            list: token list.\n",
    "        \"\"\"\n",
    "        return [self.id_to_token(idx) for idx in ids]\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build vocabulary.\n",
    "        \"\"\"\n",
    "        token_freq = self._token_count.most_common(self._max_size)\n",
    "        idx = len(self.vocab)\n",
    "        for token, _ in token_freq:\n",
    "            self._token2id[token] = idx\n",
    "            self._id2token.append(token)\n",
    "            idx += 1\n",
    "        if self._unk:\n",
    "            unk = '<unk>'\n",
    "            self._token2id[unk] = idx\n",
    "            self._id2token.append(unk)\n",
    "\n",
    "    def process_token(self, token):\n",
    "        \"\"\"Process token before following methods:\n",
    "        * add_token\n",
    "        * add_documents\n",
    "        * doc2id\n",
    "        * token_to_id\n",
    "        Args:\n",
    "            token (str): token to process.\n",
    "        Returns:\n",
    "            str: processed token string.\n",
    "        \"\"\"\n",
    "        if self._lower:\n",
    "            token = token.lower()\n",
    "\n",
    "        return token\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"Get the token_id of given token.\n",
    "        Args:\n",
    "            token (str): token from vocabulary.\n",
    "        Returns:\n",
    "            int: int id of token.\n",
    "        \"\"\"\n",
    "        token = self.process_token(token)\n",
    "        return self._token2id.get(token, len(self._token2id) - 1)\n",
    "\n",
    "    def id_to_token(self, idx):\n",
    "        \"\"\"token-id to token (string).\n",
    "        Args:\n",
    "            idx (int): token id.\n",
    "        Returns:\n",
    "            str: string of given token id.\n",
    "        \"\"\"\n",
    "        return self._id2token[idx]\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        \"\"\"Return the vocabulary.\n",
    "        Returns:\n",
    "            dict: get the dict object of the vocabulary.\n",
    "        \"\"\"\n",
    "        return self._token2id\n",
    "\n",
    "    @property\n",
    "    def reverse_vocab(self):\n",
    "        \"\"\"Return the vocabulary as a reversed dict object.\n",
    "        Returns:\n",
    "            dict: reversed vocabulary object.\n",
    "        \"\"\"\n",
    "        return self._id2token\n",
    "\n",
    "\n",
    "def filter_embeddings(embeddings, vocab, dim):\n",
    "    \"\"\"Loads word vectors in numpy array.\n",
    "    Args:\n",
    "        embeddings (dict): a dictionary of numpy array.\n",
    "        vocab (dict): word_index lookup table.\n",
    "    Returns:\n",
    "        numpy array: an array of word embeddings.\n",
    "    \"\"\"\n",
    "    if not isinstance(embeddings, dict):\n",
    "        return\n",
    "    _embeddings = np.zeros([len(vocab), dim])\n",
    "    for word in vocab:\n",
    "        if word in embeddings:\n",
    "            word_idx = vocab[word]\n",
    "            _embeddings[word_idx] = embeddings[word]\n",
    "\n",
    "    return _embeddings\n",
    "\n",
    "class IndexTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Convert a collection of raw documents to a document id matrix.\n",
    "    Attributes:\n",
    "        _use_char: boolean. Whether to use char feature.\n",
    "        _num_norm: boolean. Whether to normalize text.\n",
    "        _word_vocab: dict. A mapping of words to feature indices.\n",
    "        _char_vocab: dict. A mapping of chars to feature indices.\n",
    "        _label_vocab: dict. A mapping of labels to feature indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lower=True, num_norm=True,\n",
    "                 use_char=True, initial_vocab=None):\n",
    "        \"\"\"Create a preprocessor object.\n",
    "        Args:\n",
    "            lower: boolean. Whether to convert the texts to lowercase.\n",
    "            use_char: boolean. Whether to use char feature.\n",
    "            num_norm: boolean. Whether to normalize text.\n",
    "            initial_vocab: Iterable. Initial vocabulary for expanding word_vocab.\n",
    "        \"\"\"\n",
    "        self._num_norm = num_norm\n",
    "        self._use_char = use_char\n",
    "        self._word_vocab = Vocabulary(lower=lower)\n",
    "        self._char_vocab = Vocabulary(lower=False)\n",
    "        self._label_vocab = Vocabulary(lower=False, unk_token=False)\n",
    "\n",
    "        if initial_vocab:\n",
    "            self._word_vocab.add_documents([initial_vocab])\n",
    "            self._char_vocab.add_documents(initial_vocab)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Learn vocabulary from training set.\n",
    "        Args:\n",
    "            X : iterable. An iterable which yields either str, unicode or file objects.\n",
    "        Returns:\n",
    "            self : IndexTransformer.\n",
    "        \"\"\"\n",
    "        self._word_vocab.add_documents(X)\n",
    "        self._label_vocab.add_documents(y)\n",
    "        if self._use_char:\n",
    "            for doc in X:\n",
    "                self._char_vocab.add_documents(doc)\n",
    "\n",
    "        self._word_vocab.build()\n",
    "        self._char_vocab.build()\n",
    "        self._label_vocab.build()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transform documents to document ids.\n",
    "        Uses the vocabulary learned by fit.\n",
    "        Args:\n",
    "            X : iterable\n",
    "            an iterable which yields either str, unicode or file objects.\n",
    "            y : iterabl, label strings.\n",
    "        Returns:\n",
    "            features: document id matrix.\n",
    "            y: label id matrix.\n",
    "        \"\"\"\n",
    "        word_ids = [self._word_vocab.doc2id(doc) for doc in X]\n",
    "        word_ids = pad_sequences(word_ids, padding='post')\n",
    "\n",
    "        if self._use_char:\n",
    "            char_ids = [[self._char_vocab.doc2id(w) for w in doc] for doc in X]\n",
    "            char_ids = pad_nested_sequences(char_ids)\n",
    "            features = [word_ids, char_ids]\n",
    "        else:\n",
    "            features = word_ids\n",
    "\n",
    "        if y is not None:\n",
    "            y = [self._label_vocab.doc2id(doc) for doc in y]\n",
    "            y = pad_sequences(y, padding='post')\n",
    "            y = to_categorical(y, self.label_size).astype(int)\n",
    "            # In 2018/06/01, to_categorical is a bit strange.\n",
    "            # >>> to_categorical([[1,3]], num_classes=4).shape\n",
    "            # (1, 2, 4)\n",
    "            # >>> to_categorical([[1]], num_classes=4).shape\n",
    "            # (1, 4)\n",
    "            # So, I expand dimensions when len(y.shape) == 2.\n",
    "            y = y if len(y.shape) == 3 else np.expand_dims(y, axis=0)\n",
    "            return features, y\n",
    "        else:\n",
    "            return features\n",
    "\n",
    "    def fit_transform(self, X, y=None, **params):\n",
    "        \"\"\"Learn vocabulary and return document id matrix.\n",
    "        This is equivalent to fit followed by transform.\n",
    "        Args:\n",
    "            X : iterable\n",
    "            an iterable which yields either str, unicode or file objects.\n",
    "        Returns:\n",
    "            list : document id matrix.\n",
    "            list: label id matrix.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)\n",
    "\n",
    "    def inverse_transform(self, y, lengths=None):\n",
    "        \"\"\"Return label strings.\n",
    "        Args:\n",
    "            y: label id matrix.\n",
    "            lengths: sentences length.\n",
    "        Returns:\n",
    "            list: list of list of strings.\n",
    "        \"\"\"\n",
    "        y = np.argmax(y, -1)\n",
    "        inverse_y = [self._label_vocab.id2doc(ids) for ids in y]\n",
    "        if lengths is not None:\n",
    "            inverse_y = [iy[:l] for iy, l in zip(inverse_y, lengths)]\n",
    "\n",
    "        return inverse_y\n",
    "\n",
    "    @property\n",
    "    def word_vocab_size(self):\n",
    "        return len(self._word_vocab)\n",
    "\n",
    "    @property\n",
    "    def char_vocab_size(self):\n",
    "        return len(self._char_vocab)\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._label_vocab)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        joblib.dump(self, file_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file_path):\n",
    "        p = joblib.load(file_path)\n",
    "\n",
    "        return p\n",
    "    \n",
    "class ELMoTransformer(IndexTransformer):\n",
    "\n",
    "    def __init__(self, lower=True, num_norm=True,\n",
    "                 use_char=True, initial_vocab=None):\n",
    "        super(ELMoTransformer, self).__init__(lower, num_norm, use_char, initial_vocab)\n",
    "        self._elmo = elmo\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Transform documents to document ids.\n",
    "        Uses the vocabulary learned by fit.\n",
    "        Args:\n",
    "            X : iterable\n",
    "            an iterable which yields either str, unicode or file objects.\n",
    "            y : iterabl, label strings.\n",
    "        Returns:\n",
    "            features: document id matrix.\n",
    "            y: label id matrix.\n",
    "        \"\"\"\n",
    "        word_ids = [self._word_vocab.doc2id(doc) for doc in X]\n",
    "        word_ids = pad_sequences(word_ids, padding='post')\n",
    "\n",
    "        char_ids = [[self._char_vocab.doc2id(w) for w in doc] for doc in X]\n",
    "        char_ids = pad_nested_sequences(char_ids)\n",
    "\n",
    "        character_ids = batch_to_ids(X)\n",
    "        elmo_embeddings = self._elmo(character_ids)['elmo_representations'][1]\n",
    "        elmo_embeddings = elmo_embeddings.detach().numpy()\n",
    "\n",
    "        features = [word_ids, char_ids, elmo_embeddings]\n",
    "\n",
    "        if y is not None:\n",
    "            y = [self._label_vocab.doc2id(doc) for doc in y]\n",
    "            y = pad_sequences(y, padding='post')\n",
    "            y = to_categorical(y, self.label_size).astype(int)\n",
    "            # In 2018/06/01, to_categorical is a bit strange.\n",
    "            # >>> to_categorical([[1,3]], num_classes=4).shape\n",
    "            # (1, 2, 4)\n",
    "            # >>> to_categorical([[1]], num_classes=4).shape\n",
    "            # (1, 4)\n",
    "            # So, I expand dimensions when len(y.shape) == 2.\n",
    "            y = y if len(y.shape) == 3 else np.expand_dims(y, axis=0)\n",
    "            return features, y\n",
    "        else:\n",
    "            return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename, encoding='utf-8'):\n",
    "    \"\"\"Loads data and label from a file.\n",
    "    Args:\n",
    "        filename (str): path to the file.\n",
    "        encoding (str): file encoding format.\n",
    "        The file format is tab-separated values.\n",
    "        A blank line is required at the end of a sentence.\n",
    "        For example:\n",
    "        ```\n",
    "        EU\tB-ORG\n",
    "        rejects\tO\n",
    "        German\tB-MISC\n",
    "        call\tO\n",
    "        to\tO\n",
    "        boycott\tO\n",
    "        British\tB-MISC\n",
    "        lamb\tO\n",
    "        .\tO\n",
    "        Peter\tB-PER\n",
    "        Blackburn\tI-PER\n",
    "        ...\n",
    "        ```\n",
    "    Returns:\n",
    "        tuple(numpy array, numpy array): data and labels.\n",
    "    Example:\n",
    "        >>> filename = 'conll2003/en/ner/train.txt'\n",
    "        >>> data, labels = load_data_and_labels(filename)\n",
    "    \"\"\"\n",
    "    sents, labels = [], []\n",
    "    words, tags = [], []\n",
    "    with open(filename, encoding=encoding) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                word, tag = line.split('\\t')\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                sents.append(words)\n",
    "                labels.append(tags)\n",
    "                words, tags = [], []\n",
    "\n",
    "    return sents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERSequence(Sequence):\n",
    "\n",
    "    def __init__(self, x, y, batch_size=1, preprocess=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "\n",
    "        return self.preprocess(batch_x, batch_y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "class F1score(Callback):\n",
    "\n",
    "    def __init__(self, seq, preprocessor=None):\n",
    "        super(F1score, self).__init__()\n",
    "        self.seq = seq\n",
    "        self.p = preprocessor\n",
    "\n",
    "    def get_lengths(self, y_true):\n",
    "        lengths = []\n",
    "        for y in np.argmax(y_true, -1):\n",
    "            try:\n",
    "                i = list(y).index(0)\n",
    "            except ValueError:\n",
    "                i = len(y)\n",
    "            lengths.append(i)\n",
    "\n",
    "        return lengths\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        label_true = []\n",
    "        label_pred = []\n",
    "        for i in range(len(self.seq)):\n",
    "            x_true, y_true = self.seq[i]\n",
    "            lengths = self.get_lengths(y_true)\n",
    "            y_pred = self.model.predict_on_batch(x_true)\n",
    "\n",
    "            y_true = self.p.inverse_transform(y_true, lengths)\n",
    "            y_pred = self.p.inverse_transform(y_pred, lengths)\n",
    "\n",
    "            label_true.extend(y_true)\n",
    "            label_pred.extend(y_pred)\n",
    "\n",
    "        score = f1_score(label_true, label_pred)\n",
    "        print(' - f1: {:04.2f}'.format(score * 100))\n",
    "        print(classification_report(label_true, label_pred))\n",
    "        logs['f1'] = score\n",
    "    \n",
    "class Trainer(object):\n",
    "    \"\"\"A trainer that train the model.\n",
    "    Attributes:\n",
    "        _model: Model.\n",
    "        _preprocessor: Transformer. Preprocessing data for feature extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, preprocessor=None):\n",
    "        self._model = model\n",
    "        self._preprocessor = preprocessor\n",
    "\n",
    "    def train(self, x_train, y_train, x_valid=None, y_valid=None,\n",
    "              epochs=1, batch_size=32, verbose=1, callbacks=None, shuffle=True):\n",
    "        \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "        Args:\n",
    "            x_train: list of training data.\n",
    "            y_train: list of training target (label) data.\n",
    "            x_valid: list of validation data.\n",
    "            y_valid: list of validation target (label) data.\n",
    "            batch_size: Integer.\n",
    "                Number of samples per gradient update.\n",
    "                If unspecified, `batch_size` will default to 32.\n",
    "            epochs: Integer. Number of epochs to train the model.\n",
    "            verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
    "                0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "            callbacks: List of `keras.callbacks.Callback` instances.\n",
    "                List of callbacks to apply during training.\n",
    "            shuffle: Boolean (whether to shuffle the training data\n",
    "                before each epoch). `shuffle` will default to True.\n",
    "        \"\"\"\n",
    "\n",
    "        train_seq = NERSequence(x_train, y_train, batch_size, self._preprocessor.transform)\n",
    "\n",
    "        if x_valid and y_valid:\n",
    "            valid_seq = NERSequence(x_valid, y_valid, batch_size, self._preprocessor.transform)\n",
    "            f1 = F1score(valid_seq, preprocessor=self._preprocessor)\n",
    "            callbacks = [f1] + callbacks if callbacks else [f1]\n",
    "\n",
    "        self._model.fit_generator(generator=train_seq,\n",
    "                                  epochs=epochs,\n",
    "                                  callbacks=callbacks,\n",
    "                                  verbose=verbose,\n",
    "                                  shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_data_and_labels(\"train.txt\")\n",
    "x_valid, y_valid = load_data_and_labels(\"valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ELMoTransformer(initial_vocab=None, lower=None, num_norm=None, use_char=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = ELMoTransformer()\n",
    "p.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_SIZE = p.char_vocab_size\n",
    "WORD_VOCAB_SIZE = p.word_vocab_size\n",
    "NUM_TAGS = p.label_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "439/439 [==============================] - 2268s 5s/step - loss: 15.2983\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, preprocessor=p)\n",
    "trainer.train(x_train, y_train, x_valid, y_valid, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "`pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1860\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1862\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    706\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                                 \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_executable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1866\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1867\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4ec841e08460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         raise OSError(\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
      "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
