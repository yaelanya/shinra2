{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda, Dense, Embedding, TimeDistributed, Bidirectional, LSTM, merge, concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import metrics\n",
    "from gensim.models.poincare import PoincareModel\n",
    "from wikipedia2vec import Wikipedia2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train rows: 7435\n",
      "True: 508 \tFalse: 6927\n",
      "Number of validete rows: 1564\n",
      "True: 88 \tFalse: 1476\n"
     ]
    }
   ],
   "source": [
    "# load Production sentence data\n",
    "train_df = pd.read_csv(\"../data/train_split_words.csv\", dtype={'_id': str})\n",
    "valid_df = pd.read_csv(\"../data/valid_split_words.csv\", dtype={'_id': str})\n",
    "\n",
    "print(\"Number of train rows:\", len(train_df))\n",
    "print(\"True:\", len(train_df[train_df.label == True]), \"\\tFalse:\", len(train_df[train_df.label == False]))\n",
    "\n",
    "print(\"Number of validete rows:\", len(valid_df))\n",
    "print(\"True:\", len(valid_df[valid_df.label == True]), \"\\tFalse:\", len(valid_df[valid_df.label == False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/pageid2ChEBI.json\", 'r') as f:\n",
    "    pageid2ChEBI_table = json.load(f)\n",
    "\n",
    "ChEBI_df = pd.DataFrame()\n",
    "for _id, ChEBIs in pageid2ChEBI_table.items():\n",
    "    new_df = pd.DataFrame({'_id': [_id] * len(ChEBIs), 'ChEBI': ChEBIs})\n",
    "    ChEBI_df = ChEBI_df.append(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train rows: 8672\n",
      "True: 605 \tFalse: 8067\n",
      "Number of validete rows: 1918\n",
      "True: 103 \tFalse: 1815\n"
     ]
    }
   ],
   "source": [
    "# merge ChEBI DataFrame\n",
    "train_df = pd.merge(train_df, ChEBI_df, on='_id', how='left')\n",
    "valid_df = pd.merge(valid_df, ChEBI_df, on='_id', how='left')\n",
    "\n",
    "print(\"Number of train rows:\", len(train_df))\n",
    "print(\"True:\", len(train_df[train_df.label == True]), \"\\tFalse:\", len(train_df[train_df.label == False]))\n",
    "\n",
    "print(\"Number of validete rows:\", len(valid_df))\n",
    "print(\"True:\", len(valid_df[valid_df.label == True]), \"\\tFalse:\", len(valid_df[valid_df.label == False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki2vec = Wikipedia2Vec.load('../model/jawiki_20180420_300d.pkl')\n",
    "poincare_model = PoincareModel.load(\"../model/poincare.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2vec(s: str):\n",
    "    return [_w2v(w) for w in s]\n",
    "\n",
    "def _w2v(w):\n",
    "    try:\n",
    "        return np.array(wiki2vec.get_word_vector(w).tolist())\n",
    "    except KeyError:\n",
    "        return np.zeros(WORD_EMBEDDING_DIM)\n",
    "\n",
    "def ontology2vec(ChEBI: str):\n",
    "    '''\n",
    "    Using Poincaré embedding.\n",
    "    '''\n",
    "    if ChEBI is np.nan:\n",
    "        return np.zeros(ONTOLOGY_EMBEDDING_DIM)\n",
    "    \n",
    "    try:\n",
    "        return poincare_model.kv[ChEBI]\n",
    "    except KeyError:\n",
    "        return np.zeros(ONTOLOGY_EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words = pad_sequences(\n",
    "    train_df.words.apply(lambda x: sentence2vec(x)).tolist()\n",
    "    , dtype='float32'\n",
    "    , padding='post'\n",
    "    , truncating='pre'\n",
    "    , maxlen=50\n",
    ")\n",
    "\n",
    "X_train_ontology = train_df.ChEBI.apply(lambda x: ontology2vec(x)).tolist()\n",
    "X_train_ontology = np.array(X_train_ontology)\n",
    "\n",
    "y_train = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_words = pad_sequences(\n",
    "    valid_df.words.apply(lambda x: sentence2vec(x)).tolist()\n",
    "    , dtype='float32'\n",
    "    , padding='post'\n",
    "    , truncating='pre'\n",
    "    , maxlen=50\n",
    ")\n",
    "\n",
    "X_valid_ontology = valid_df.ChEBI.apply(lambda x: ontology2vec(x)).tolist()\n",
    "X_valid_ontology = np.array(X_valid_ontology)\n",
    "\n",
    "y_valid = valid_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8672, 50, 300)\n",
      "(8672, 10)\n",
      "(8672,)\n"
     ]
    }
   ],
   "source": [
    "# check dimensions\n",
    "print(X_train_words.shape)\n",
    "print(X_train_ontology.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        \n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_EMBEDDING_DIM = 300\n",
    "WORD_LSTM_UNIT = 512\n",
    "ONTOLOGY_EMBEDDING_DIM = 10\n",
    "FC_DIM = 128\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = Input(shape=(None, WORD_EMBEDDING_DIM,), dtype='float32')\n",
    "ontology_embeddings = Input(shape=(ONTOLOGY_EMBEDDING_DIM,), dtype='float32')\n",
    "\n",
    "l_lstm = Bidirectional(LSTM(WORD_LSTM_UNIT, return_sequences=True))(word_embeddings)\n",
    "l_max = Lambda(lambda x: K.max(x, axis=1))(l_lstm)\n",
    "x = concatenate([l_max, ontology_embeddings])\n",
    "\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "x = Dense(FC_DIM, activation='relu')(x)\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "x = Dense(FC_DIM, activation='relu')(x)\n",
    "x = Dropout(DROPOUT_RATE)(x)\n",
    "x = Dense(FC_DIM, activation='relu')(x)\n",
    "\n",
    "pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[word_embeddings, ontology_embeddings], outputs=pred)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.binary_accuracy, f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import pydot\n",
    "plot_model(model, show_shapes=True, to_file='model3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8672/8672 [==============================] - 5s 559us/step - loss: 1.1236 - binary_accuracy: 0.8148 - f1: 0.0939\n",
      "Epoch 2/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 1.1164 - binary_accuracy: 0.8523 - f1: 0.0947\n",
      "Epoch 3/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 1.0625 - binary_accuracy: 0.7979 - f1: 0.1658\n",
      "Epoch 4/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.9601 - binary_accuracy: 0.8018 - f1: 0.2732\n",
      "Epoch 5/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.8136 - binary_accuracy: 0.8285 - f1: 0.3798\n",
      "Epoch 6/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.7322 - binary_accuracy: 0.8461 - f1: 0.4235\n",
      "Epoch 7/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.6656 - binary_accuracy: 0.8624 - f1: 0.4540\n",
      "Epoch 8/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.6112 - binary_accuracy: 0.8720 - f1: 0.4724\n",
      "Epoch 9/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.5671 - binary_accuracy: 0.8773 - f1: 0.4920\n",
      "Epoch 10/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.5355 - binary_accuracy: 0.8790 - f1: 0.4973\n",
      "Epoch 11/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.5053 - binary_accuracy: 0.8810 - f1: 0.5008\n",
      "Epoch 12/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.5034 - binary_accuracy: 0.8707 - f1: 0.4920\n",
      "Epoch 13/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.4797 - binary_accuracy: 0.8715 - f1: 0.4992\n",
      "Epoch 14/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.4356 - binary_accuracy: 0.8818 - f1: 0.5188\n",
      "Epoch 15/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.3987 - binary_accuracy: 0.8951 - f1: 0.5521\n",
      "Epoch 16/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.3716 - binary_accuracy: 0.9008 - f1: 0.5693\n",
      "Epoch 17/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.4149 - binary_accuracy: 0.8787 - f1: 0.5232\n",
      "Epoch 18/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.3866 - binary_accuracy: 0.8989 - f1: 0.5749\n",
      "Epoch 19/100\n",
      "8672/8672 [==============================] - 3s 306us/step - loss: 0.3316 - binary_accuracy: 0.9062 - f1: 0.5840\n",
      "Epoch 20/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.3267 - binary_accuracy: 0.9097 - f1: 0.5965\n",
      "Epoch 21/100\n",
      "8672/8672 [==============================] - 3s 305us/step - loss: 0.3342 - binary_accuracy: 0.9046 - f1: 0.5856\n",
      "Epoch 22/100\n",
      "8672/8672 [==============================] - 3s 305us/step - loss: 0.3058 - binary_accuracy: 0.9157 - f1: 0.6173\n",
      "Epoch 23/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.3410 - binary_accuracy: 0.9128 - f1: 0.6127\n",
      "Epoch 24/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.3344 - binary_accuracy: 0.8999 - f1: 0.5805\n",
      "Epoch 25/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.2863 - binary_accuracy: 0.9171 - f1: 0.6233\n",
      "Epoch 26/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2793 - binary_accuracy: 0.9220 - f1: 0.6315\n",
      "Epoch 27/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2901 - binary_accuracy: 0.9182 - f1: 0.6222\n",
      "Epoch 28/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2612 - binary_accuracy: 0.9283 - f1: 0.6533\n",
      "Epoch 29/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.2970 - binary_accuracy: 0.9101 - f1: 0.6076\n",
      "Epoch 30/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.2840 - binary_accuracy: 0.9287 - f1: 0.6618\n",
      "Epoch 31/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2517 - binary_accuracy: 0.9304 - f1: 0.6594\n",
      "Epoch 32/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2320 - binary_accuracy: 0.9361 - f1: 0.6803\n",
      "Epoch 33/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.2293 - binary_accuracy: 0.9370 - f1: 0.6836\n",
      "Epoch 34/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2232 - binary_accuracy: 0.9402 - f1: 0.6934\n",
      "Epoch 35/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.2147 - binary_accuracy: 0.9395 - f1: 0.6887\n",
      "Epoch 36/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.2136 - binary_accuracy: 0.9411 - f1: 0.6989\n",
      "Epoch 37/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2274 - binary_accuracy: 0.9450 - f1: 0.7163\n",
      "Epoch 38/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2059 - binary_accuracy: 0.9403 - f1: 0.6967\n",
      "Epoch 39/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1902 - binary_accuracy: 0.9480 - f1: 0.7240\n",
      "Epoch 40/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.2154 - binary_accuracy: 0.9408 - f1: 0.6992\n",
      "Epoch 41/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.2411 - binary_accuracy: 0.9333 - f1: 0.6806\n",
      "Epoch 42/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1928 - binary_accuracy: 0.9463 - f1: 0.7202\n",
      "Epoch 43/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1890 - binary_accuracy: 0.9496 - f1: 0.7289\n",
      "Epoch 44/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1892 - binary_accuracy: 0.9510 - f1: 0.7346\n",
      "Epoch 45/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.2042 - binary_accuracy: 0.9408 - f1: 0.6978\n",
      "Epoch 46/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1804 - binary_accuracy: 0.9521 - f1: 0.7410\n",
      "Epoch 47/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1592 - binary_accuracy: 0.9591 - f1: 0.7740\n",
      "Epoch 48/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1861 - binary_accuracy: 0.9530 - f1: 0.7416\n",
      "Epoch 49/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1631 - binary_accuracy: 0.9540 - f1: 0.7483\n",
      "Epoch 50/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1787 - binary_accuracy: 0.9551 - f1: 0.7603\n",
      "Epoch 51/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1833 - binary_accuracy: 0.9525 - f1: 0.7399\n",
      "Epoch 52/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1778 - binary_accuracy: 0.9530 - f1: 0.7413\n",
      "Epoch 53/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1784 - binary_accuracy: 0.9564 - f1: 0.7600\n",
      "Epoch 54/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1531 - binary_accuracy: 0.9584 - f1: 0.7656\n",
      "Epoch 55/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1565 - binary_accuracy: 0.9632 - f1: 0.7888\n",
      "Epoch 56/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1537 - binary_accuracy: 0.9599 - f1: 0.7732\n",
      "Epoch 57/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1608 - binary_accuracy: 0.9614 - f1: 0.7783\n",
      "Epoch 58/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.1510 - binary_accuracy: 0.9599 - f1: 0.7756\n",
      "Epoch 59/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1496 - binary_accuracy: 0.9641 - f1: 0.7891\n",
      "Epoch 60/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1373 - binary_accuracy: 0.9615 - f1: 0.7813\n",
      "Epoch 61/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1369 - binary_accuracy: 0.9655 - f1: 0.7965\n",
      "Epoch 62/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1380 - binary_accuracy: 0.9644 - f1: 0.7933\n",
      "Epoch 63/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1440 - binary_accuracy: 0.9662 - f1: 0.8038\n",
      "Epoch 64/100\n",
      "8672/8672 [==============================] - 3s 305us/step - loss: 0.1445 - binary_accuracy: 0.9634 - f1: 0.7884\n",
      "Epoch 65/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1320 - binary_accuracy: 0.9685 - f1: 0.8132\n",
      "Epoch 66/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1175 - binary_accuracy: 0.9686 - f1: 0.8143\n",
      "Epoch 67/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.1402 - binary_accuracy: 0.9666 - f1: 0.8005\n",
      "Epoch 68/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1184 - binary_accuracy: 0.9726 - f1: 0.8354\n",
      "Epoch 69/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1302 - binary_accuracy: 0.9683 - f1: 0.8106\n",
      "Epoch 70/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1159 - binary_accuracy: 0.9698 - f1: 0.8190\n",
      "Epoch 71/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1432 - binary_accuracy: 0.9623 - f1: 0.7862\n",
      "Epoch 72/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1155 - binary_accuracy: 0.9713 - f1: 0.8269\n",
      "Epoch 73/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1206 - binary_accuracy: 0.9709 - f1: 0.8238\n",
      "Epoch 74/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1226 - binary_accuracy: 0.9699 - f1: 0.8191\n",
      "Epoch 75/100\n",
      "8672/8672 [==============================] - 3s 310us/step - loss: 0.1183 - binary_accuracy: 0.9696 - f1: 0.8166\n",
      "Epoch 76/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.1167 - binary_accuracy: 0.9711 - f1: 0.8249\n",
      "Epoch 77/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1414 - binary_accuracy: 0.9601 - f1: 0.7788\n",
      "Epoch 78/100\n",
      "8672/8672 [==============================] - 3s 305us/step - loss: 0.1252 - binary_accuracy: 0.9704 - f1: 0.8226\n",
      "Epoch 79/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1138 - binary_accuracy: 0.9721 - f1: 0.8325\n",
      "Epoch 80/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0906 - binary_accuracy: 0.9766 - f1: 0.8546\n",
      "Epoch 81/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.1029 - binary_accuracy: 0.9734 - f1: 0.8388\n",
      "Epoch 82/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0912 - binary_accuracy: 0.9779 - f1: 0.8614\n",
      "Epoch 83/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0928 - binary_accuracy: 0.9772 - f1: 0.8543\n",
      "Epoch 84/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0823 - binary_accuracy: 0.9799 - f1: 0.8741\n",
      "Epoch 85/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0839 - binary_accuracy: 0.9803 - f1: 0.8750\n",
      "Epoch 86/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0890 - binary_accuracy: 0.9788 - f1: 0.8665\n",
      "Epoch 87/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.0987 - binary_accuracy: 0.9756 - f1: 0.8448\n",
      "Epoch 88/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.0725 - binary_accuracy: 0.9812 - f1: 0.8792\n",
      "Epoch 89/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.0828 - binary_accuracy: 0.9825 - f1: 0.8845\n",
      "Epoch 90/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.0858 - binary_accuracy: 0.9758 - f1: 0.8507\n",
      "Epoch 91/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.0897 - binary_accuracy: 0.9796 - f1: 0.8700\n",
      "Epoch 92/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0715 - binary_accuracy: 0.9845 - f1: 0.9015\n",
      "Epoch 93/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0714 - binary_accuracy: 0.9832 - f1: 0.8915\n",
      "Epoch 94/100\n",
      "8672/8672 [==============================] - 3s 304us/step - loss: 0.0798 - binary_accuracy: 0.9805 - f1: 0.8770\n",
      "Epoch 95/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0784 - binary_accuracy: 0.9797 - f1: 0.8711\n",
      "Epoch 96/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.0718 - binary_accuracy: 0.9812 - f1: 0.8790\n",
      "Epoch 97/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0822 - binary_accuracy: 0.9819 - f1: 0.8844\n",
      "Epoch 98/100\n",
      "8672/8672 [==============================] - 3s 301us/step - loss: 0.1012 - binary_accuracy: 0.9734 - f1: 0.8394\n",
      "Epoch 99/100\n",
      "8672/8672 [==============================] - 3s 303us/step - loss: 0.0822 - binary_accuracy: 0.9803 - f1: 0.8760\n",
      "Epoch 100/100\n",
      "8672/8672 [==============================] - 3s 302us/step - loss: 0.0928 - binary_accuracy: 0.9787 - f1: 0.8671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64cdfec630>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[X_train_words, X_train_ontology], y=y_train, class_weight={0:1, 1: 10}, epochs=100, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(pred_true, pred_false):\n",
    "    TP = pred_true[pred_true.label == True].count()[0]\n",
    "    FP = pred_true[pred_true.label == False].count()[0]\n",
    "    TN = pred_false[pred_false.label == False].count()[0]\n",
    "    FN = pred_false[pred_false.label == True].count()[0]\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"TP:\", TP, \"\\tFP:\", FP, \"\\tTN:\", TN, \"\\tFN:\", FN)\n",
    "    print(\"Precision:\", precision, \"\\tRecall:\", recall, \"\\tF1:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([X_valid_words, X_valid_ontology])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 54 \tFP: 50 \tTN: 1765 \tFN: 49\n",
      "Precision: 0.5192307692307693 \tRecall: 0.5242718446601942 \tF1: 0.5217391304347825\n"
     ]
    }
   ],
   "source": [
    "pred_true = valid_df.loc[np.where(predict >= 0.5)[0]]\n",
    "pred_false = valid_df.loc[np.where(predict < 0.5)[0]]\n",
    "pred_true_uniq = pred_true.drop_duplicates(['_id', 'sentence'])\n",
    "pred_false_uniq = pred_false.drop_duplicates(['_id', 'sentence'])\n",
    "\n",
    "evaluation(pred_true, pred_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_true.loc[:, ['title', 'sentence', 'ChEBI', 'label']].to_csv(\"../dump/pred_true.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_false[pred_false.label == True].loc[:, ['title', 'sentence', 'ChEBI', 'label']].to_csv(\"../dump/pred_false_filter_label_true.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['N-メチルピロリドン',\n",
       "        'N-メチル-2-ピロリドンは、γ-ブチロラクトンとメチルアミンとを縮合させて得る 高い溶解性を持つため、特に高分子化学の分野を中心に様々な物質に対する溶媒として用いられる。'],\n",
       "       ['スチレン',\n",
       "        'かつては、エチルベンゼンを塩素化したのちに脱塩化水素でオレフィンとする方法やエチルベンゼンを酸化したアセトフェノン、還元したフェニルカルビノールを経由して脱水反応オレフィンとする方法なども存在したが、今日では経済的な理由で触媒により脱水素する方法以外は利用されない。'],\n",
       "       ['フルオレセイン', '反応触媒としては、塩化亜鉛の他にスルホン酸も用いられる。'],\n",
       "       ['フルオレセイン', '反応触媒としては、塩化亜鉛の他にスルホン酸も用いられる。'],\n",
       "       ['炭酸ベリリウム',\n",
       "        'Be ( OH ) 2 + CO 2 + 3 H 2 O ⟶ BeCO 3 ⋅ 4 H 2 O 水酸化ベリリウムをアンモニア水に懸濁させて二酸化炭素を通じて飽和させ、放置すると塩基性塩Be2CO3(OH)2が沈殿する。'],\n",
       "       ['塩化ウラン(VI)', '酸化ウラン(VI)はまず塩化ウラン(V)となり、さらに塩素と反応して塩化ウラン(VI)となる。'],\n",
       "       ['塩化ウラン(VI)', '反応に伴って圧力が変化するため、グローブボックスなどの気密容器中で反応させる。'],\n",
       "       ['エーテル (化学)', 'アルコールの共存下、オレフィンに求電子剤を作用させると求電子的付加反応によりエーテルが得られる。'],\n",
       "       ['イノシトールトリスリン酸',\n",
       "        '細胞膜に存在するリン脂質であるホスファチジルイノシトール4,5-ビスリン酸がホスホリパーゼCによって加水分解されると、 IP3とジアシルグリセロールが生成する。'],\n",
       "       ['デカカルボニルジヒドリド三オスミウム',\n",
       "        'Os3(CO)12 のオクタン溶液（または似た沸点をもつ不活性溶媒）を H2 でパージすることによって準備される。'],\n",
       "       ['サッカリン', '元はトルエンから合成されたが、収率は低かった。'],\n",
       "       ['サッカリン', '2-クロロトルエンからも作ることもできる。'],\n",
       "       ['シガトキシン', 'ある種の藻類（有毒渦鞭毛藻）がつくり魚類に蓄積される。'],\n",
       "       ['シガトキシン', 'ポリケチド経路によって生合成され、中員環を含む多数のエーテル環が連結した特異な構造を持つ。'],\n",
       "       ['シガトキシン',\n",
       "        'グラブス触媒を用いたオレフィンメタセシスによる閉環反応を鍵反応とし、13個の連結したエーテル環構造を効率的に合成する手法を確立し、以後の天然物合成における可能性を広げた。'],\n",
       "       ['ペンタセン',\n",
       "        'その後、ペンタセンの薄層を調製する必要が生じると、前駆体から小分子を脱離させる手法を用いて合成されるようになった。'],\n",
       "       ['ペンタセン',\n",
       "        'ペンタセンは一般的な有機溶媒には溶けにくいが、1,2,4-トリクロロベンゼンのようなハロゲン化芳香族炭化水素系溶媒中に高温では溶けることが知られている。'],\n",
       "       ['ペンタセン', 'そこから小さな平板を形成させるために結晶化することができる。'],\n",
       "       ['デキストラン',\n",
       "        'デキストラン (dextran) はグルコースのみからなる多糖類の一種で、スクロースを原料として乳酸菌が生産する。'],\n",
       "       ['トリイソプロピルアミン', 'よって、ジイソプロピルアミンを原料に合成する方法が発見されている（下式参照）。'],\n",
       "       ['硫酸カルシウム', '66℃以下では2水和物、以上では無水物が析出する。'],\n",
       "       ['グルコノラクトン', '生体内ではグルコース-1-デヒドロゲナーゼの作用によりグルコースから変換される。'],\n",
       "       ['イオノン', 'また、三フッ化ホウ素を使うとγ-イオノンが生成する。'],\n",
       "       ['エチレンテトラカルボン酸二無水物',\n",
       "        'エチレンテトラカルボン酸の熱分解による生成は、1981年にジョン・パターソンらによって報告された。'],\n",
       "       ['リン酸三カルシウム', '骨を燃焼させた際に得られる物質でもある（骨灰・骨炭など）。'],\n",
       "       ['イソブチルアミン', '天然には、一部の植物や藻類により自然発生する。'],\n",
       "       ['REBCO',\n",
       "        '電子ビーム共蒸着法 電子ビーム共蒸着法は、各元素ごとに蒸着レートを決定することができ、組成制御が容易に行える方法である。'],\n",
       "       ['REBCO',\n",
       "        '真空容器内で複数の蒸着源を個々に電子ビームで加熱蒸発させ、ヒーターによって加熱された基板上に薄膜として成長させる。'],\n",
       "       ['REBCO',\n",
       "        'レーザアブレーション(PLD)法 PLD法はPVD(物理気相蒸着)法の一種で、装置自体は他のPVD薄膜作製技術と比べ、最も単純で薄膜作製も容易で堆積させた薄膜の組成がターゲットに近く、レーザ光を吸収する物質であれば高融点の物質でも容易に薄膜化できるという利点を有する方法で、真空チャンバー内の焼結体ターゲットにパルスレーザを断続的に照射してターゲットをアブレーションすることにより放出されるフラグメント（イオン、クラスタ、分子、原子）をターゲットと対向して配置された基板上に薄膜を堆積させる。'],\n",
       "       ['REBCO',\n",
       "        '有機金属化学気相蒸着法(MOCVD) 高真空を必要とせず、大面積、複雑な形状の基板にも成膜が可能で生産性が高く、量産性に優れている方法である。'],\n",
       "       ['REBCO',\n",
       "        '原料槽に入れた金属錯体原料をヒーターで加熱し液体状態にし、キャリアガスを原料槽内に流通することにより、気化した原料ガスを反応室へと導き、セラミックヒーターで加熱された基板上に蒸着させ成膜を行う。'],\n",
       "       ['水酸化ベリリウム',\n",
       "        'ベリリウム塩水溶液にアンモニア水を加えてできる沈殿を、アンモニア水の存在下で長時間加熱するとα型の結晶が生成する。'],\n",
       "       ['パチョロール', 'パチョロール (Patchoulol) は、パチョリから抽出されるセスキテルペンアルコールである。'],\n",
       "       ['尿素', '生体内では、尿素回路によりアンモニアから尿素が産生される。'],\n",
       "       ['尿素', '生体内では、尿素回路によりアンモニアから尿素が産生される。'],\n",
       "       ['尿素', '生体内では、尿素回路によりアンモニアから尿素が産生される。'],\n",
       "       ['尿素', '生体内では、尿素回路によりアンモニアから尿素が産生される。'],\n",
       "       ['尿素', 'この合成法はヴェーラー合成と呼ばれている。'],\n",
       "       ['尿素', 'この合成法はヴェーラー合成と呼ばれている。'],\n",
       "       ['尿素', 'この合成法はヴェーラー合成と呼ばれている。'],\n",
       "       ['尿素', 'この合成法はヴェーラー合成と呼ばれている。'],\n",
       "       ['スワインソニン',\n",
       "        'スワインソニンは商業的にはメタリジウム(Metarhizium anisopliaeなどいくつかの植物および菌から抽出される他、全合成も多数報告されている。'],\n",
       "       ['シアン化水銀(II)',\n",
       "        'シアン化水銀(I)は、水銀(I)イオンの水溶液にシアン化カリウム水溶液などを滴下したときに生じるが、この物質は不安定であり、ただちに分解してシアン化水銀(II)と水銀になる。'],\n",
       "       ['塩化ロジウム(III)',\n",
       "        '2 Rh + 3 Cl 2 ⟶ 2 RhCl 3 塩化ロジウム三水和物を塩化水素中で360℃で加熱分解すると水に不溶性の無水物が得られる。'],\n",
       "       ['塩化ロジウム(III)', '一方塩化水素中の180℃の加熱では水溶性の無水物が得られる。'],\n",
       "       ['メチルホスホン酸ジメチル',\n",
       "        '亜リン酸トリメチルとハロメタン（ヨードメタン等）を用いたミカエリス・アルブーゾフ反応により製造が可能である。'],\n",
       "       ['チオアミド', '多くの場合、アミドの酸素を硫黄に置換して合成する。'],\n",
       "       ['チオアミド',\n",
       "        'かつてはアミドと五硫化二リン (P2S5) を加熱する方法が取られていたが、近年ではより穏和な条件で反応が進行するローソン試薬を用いるケースが増えている。'],\n",
       "       ['ピペラジン', '1,2-ジクロロエタンとアンモニアを、水酸化ナトリウム存在下で反応させる。']], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_false[pred_false.label == True][['title', 'sentence']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
