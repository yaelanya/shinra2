{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.LongTensor(\n",
    "    [\n",
    "        [[ 1.,  2.,  3.,  4.,  5.],\n",
    "        [ 6.,  8.,  0.,  0.,  0.],\n",
    "        [ 7.,  7.,  7.,  0.,  0.]]\n",
    "        ,[[ 1.,  2.,  3.,  4.,  5.],\n",
    "        [ 6.,  8.,  0.,  0.,  0.],\n",
    "        [ 0.,  0.,  0.,  0.,  0.]]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, _ = X.size()\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = nn.Embedding(\n",
    "    num_embeddings=10,\n",
    "    embedding_dim=5,\n",
    "    padding_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=5,\n",
    "    hidden_size=10,\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_to_tag = nn.Linear(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5],\n",
       "        [6, 8, 0, 0, 0],\n",
       "        [7, 7, 7, 0, 0],\n",
       "        [1, 2, 3, 4, 5],\n",
       "        [6, 8, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.view(batch_size * seq_len, -1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 5, 3, 2, 2, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lengths = (X > 0).sum(-1)\n",
    "X_lengths, sorted_index = X_lengths.sort(0, descending=True)\n",
    "X_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8975,  1.1562, -0.4889,  0.2837,  1.2762],\n",
       "         [ 1.4487, -0.2751,  1.2067, -0.7646,  1.5479],\n",
       "         [ 0.5451, -0.6426,  1.1354, -0.0422,  0.9274],\n",
       "         [ 0.3077, -1.6722,  0.8468, -0.6089, -1.0505],\n",
       "         [-0.1483,  1.9102,  0.0642, -0.0661, -1.3715]],\n",
       "\n",
       "        [[ 0.8975,  1.1562, -0.4889,  0.2837,  1.2762],\n",
       "         [ 1.4487, -0.2751,  1.2067, -0.7646,  1.5479],\n",
       "         [ 0.5451, -0.6426,  1.1354, -0.0422,  0.9274],\n",
       "         [ 0.3077, -1.6722,  0.8468, -0.6089, -1.0505],\n",
       "         [-0.1483,  1.9102,  0.0642, -0.0661, -1.3715]],\n",
       "\n",
       "        [[-2.0011,  1.8204,  0.2832, -0.5354, -1.6312],\n",
       "         [-2.0011,  1.8204,  0.2832, -0.5354, -1.6312],\n",
       "         [-2.0011,  1.8204,  0.2832, -0.5354, -1.6312],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.1873,  0.2643, -1.1364, -1.7089,  0.0416],\n",
       "         [-0.6830, -0.5823,  0.1573,  0.1522,  1.5413],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.1873,  0.2643, -1.1364, -1.7089,  0.0416],\n",
       "         [-0.6830, -0.5823,  0.1573,  0.1522,  1.5413],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = word_embedding(X[sorted_index][X_lengths > 0])\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pad = X_lengths > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.8975,  1.1562, -0.4889,  0.2837,  1.2762],\n",
       "        [ 0.8975,  1.1562, -0.4889,  0.2837,  1.2762],\n",
       "        [-2.0011,  1.8204,  0.2832, -0.5354, -1.6312],\n",
       "        [-1.1873,  0.2643, -1.1364, -1.7089,  0.0416],\n",
       "        [-1.1873,  0.2643, -1.1364, -1.7089,  0.0416],\n",
       "        [ 1.4487, -0.2751,  1.2067, -0.7646,  1.5479],\n",
       "        [ 1.4487, -0.2751,  1.2067, -0.7646,  1.5479],\n",
       "        [-2.0011,  1.8204,  0.2832, -0.5354, -1.6312],\n",
       "        [-0.6830, -0.5823,  0.1573,  0.1522,  1.5413],\n",
       "        [-0.6830, -0.5823,  0.1573,  0.1522,  1.5413],\n",
       "        [ 0.5451, -0.6426,  1.1354, -0.0422,  0.9274],\n",
       "        [ 0.5451, -0.6426,  1.1354, -0.0422,  0.9274],\n",
       "        [-2.0011,  1.8204,  0.2832, -0.5354, -1.6312],\n",
       "        [ 0.3077, -1.6722,  0.8468, -0.6089, -1.0505],\n",
       "        [ 0.3077, -1.6722,  0.8468, -0.6089, -1.0505],\n",
       "        [-0.1483,  1.9102,  0.0642, -0.0661, -1.3715],\n",
       "        [-0.1483,  1.9102,  0.0642, -0.0661, -1.3715]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([5, 5, 3, 2, 2]))"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = torch.nn.utils.rnn.pack_padded_sequence(X1, X_lengths[X_lengths > 0], batch_first=True)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,  0.0254,\n",
       "          0.0157,  0.0020],\n",
       "        [-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,  0.0254,\n",
       "          0.0157,  0.0020],\n",
       "        [-0.0828, -0.2182,  0.1457, -0.0035,  0.2346,  0.1295,  0.0822,  0.0652,\n",
       "          0.1894, -0.2764],\n",
       "        [-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,  0.0855,\n",
       "         -0.0150, -0.0988],\n",
       "        [-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,  0.0855,\n",
       "         -0.0150, -0.0988],\n",
       "        [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,  0.1123,\n",
       "         -0.0125,  0.0628],\n",
       "        [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,  0.1123,\n",
       "         -0.0125,  0.0628],\n",
       "        [-0.1101, -0.3333,  0.2437,  0.0088,  0.2958,  0.1664,  0.1547,  0.0564,\n",
       "          0.2440, -0.3665],\n",
       "        [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005, -0.0718,\n",
       "          0.1102, -0.0366],\n",
       "        [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005, -0.0718,\n",
       "          0.1102, -0.0366],\n",
       "        [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,  0.0975,\n",
       "          0.0986,  0.0998],\n",
       "        [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,  0.0975,\n",
       "          0.0986,  0.0998],\n",
       "        [-0.1151, -0.3814,  0.2962,  0.0126,  0.3106,  0.1737,  0.2137,  0.0389,\n",
       "          0.2571, -0.3928],\n",
       "        [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,  0.1874,\n",
       "          0.1327,  0.1257],\n",
       "        [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,  0.1874,\n",
       "          0.1327,  0.1257],\n",
       "        [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,  0.2032,\n",
       "          0.1596, -0.1078],\n",
       "        [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,  0.2032,\n",
       "          0.1596, -0.1078]], grad_fn=<CatBackward>), batch_sizes=tensor([5, 5, 3, 2, 2]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3, _ = lstm(X2)\n",
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,\n",
       "           0.0254,  0.0157,  0.0020],\n",
       "         [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,\n",
       "           0.1123, -0.0125,  0.0628],\n",
       "         [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,\n",
       "           0.0975,  0.0986,  0.0998],\n",
       "         [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,\n",
       "           0.1874,  0.1327,  0.1257],\n",
       "         [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,\n",
       "           0.2032,  0.1596, -0.1078]],\n",
       "\n",
       "        [[-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,\n",
       "           0.0254,  0.0157,  0.0020],\n",
       "         [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,\n",
       "           0.1123, -0.0125,  0.0628],\n",
       "         [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,\n",
       "           0.0975,  0.0986,  0.0998],\n",
       "         [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,\n",
       "           0.1874,  0.1327,  0.1257],\n",
       "         [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,\n",
       "           0.2032,  0.1596, -0.1078]],\n",
       "\n",
       "        [[-0.0828, -0.2182,  0.1457, -0.0035,  0.2346,  0.1295,  0.0822,\n",
       "           0.0652,  0.1894, -0.2764],\n",
       "         [-0.1101, -0.3333,  0.2437,  0.0088,  0.2958,  0.1664,  0.1547,\n",
       "           0.0564,  0.2440, -0.3665],\n",
       "         [-0.1151, -0.3814,  0.2962,  0.0126,  0.3106,  0.1737,  0.2137,\n",
       "           0.0389,  0.2571, -0.3928],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,\n",
       "           0.0855, -0.0150, -0.0988],\n",
       "         [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005,\n",
       "          -0.0718,  0.1102, -0.0366],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,\n",
       "           0.0855, -0.0150, -0.0988],\n",
       "         [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005,\n",
       "          -0.0718,  0.1102, -0.0366],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4, _ = torch.nn.utils.rnn.pad_packed_sequence(X3, batch_first=True)\n",
    "X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = torch.zeros(((X_lengths == 0).sum(), 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = torch.cat((X4, zero), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,\n",
       "           0.0254,  0.0157,  0.0020],\n",
       "         [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,\n",
       "           0.1123, -0.0125,  0.0628],\n",
       "         [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,\n",
       "           0.0975,  0.0986,  0.0998],\n",
       "         [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,\n",
       "           0.1874,  0.1327,  0.1257],\n",
       "         [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,\n",
       "           0.2032,  0.1596, -0.1078]],\n",
       "\n",
       "        [[-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,\n",
       "           0.0855, -0.0150, -0.0988],\n",
       "         [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005,\n",
       "          -0.0718,  0.1102, -0.0366],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0828, -0.2182,  0.1457, -0.0035,  0.2346,  0.1295,  0.0822,\n",
       "           0.0652,  0.1894, -0.2764],\n",
       "         [-0.1101, -0.3333,  0.2437,  0.0088,  0.2958,  0.1664,  0.1547,\n",
       "           0.0564,  0.2440, -0.3665],\n",
       "         [-0.1151, -0.3814,  0.2962,  0.0126,  0.3106,  0.1737,  0.2137,\n",
       "           0.0389,  0.2571, -0.3928],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,\n",
       "           0.0254,  0.0157,  0.0020],\n",
       "         [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,\n",
       "           0.1123, -0.0125,  0.0628],\n",
       "         [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,\n",
       "           0.0975,  0.0986,  0.0998],\n",
       "         [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,\n",
       "           0.1874,  0.1327,  0.1257],\n",
       "         [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,\n",
       "           0.2032,  0.1596, -0.1078]],\n",
       "\n",
       "        [[-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,\n",
       "           0.0855, -0.0150, -0.0988],\n",
       "         [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005,\n",
       "          -0.0718,  0.1102, -0.0366],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]], grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X5 = torch.zeros(X4.size())\n",
    "X5[sorted_index] = X4\n",
    "X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,  0.0254,\n",
       "          0.0157,  0.0020],\n",
       "        [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,  0.1123,\n",
       "         -0.0125,  0.0628],\n",
       "        [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,  0.0975,\n",
       "          0.0986,  0.0998],\n",
       "        [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,  0.1874,\n",
       "          0.1327,  0.1257],\n",
       "        [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,  0.2032,\n",
       "          0.1596, -0.1078],\n",
       "        [-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,  0.0855,\n",
       "         -0.0150, -0.0988],\n",
       "        [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005, -0.0718,\n",
       "          0.1102, -0.0366],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.0828, -0.2182,  0.1457, -0.0035,  0.2346,  0.1295,  0.0822,  0.0652,\n",
       "          0.1894, -0.2764],\n",
       "        [-0.1101, -0.3333,  0.2437,  0.0088,  0.2958,  0.1664,  0.1547,  0.0564,\n",
       "          0.2440, -0.3665],\n",
       "        [-0.1151, -0.3814,  0.2962,  0.0126,  0.3106,  0.1737,  0.2137,  0.0389,\n",
       "          0.2571, -0.3928],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [-0.1297,  0.0264,  0.1290,  0.0980, -0.0395, -0.0021, -0.2415,  0.0254,\n",
       "          0.0157,  0.0020],\n",
       "        [-0.2667,  0.1126,  0.0945,  0.0630, -0.0982, -0.1466, -0.2590,  0.1123,\n",
       "         -0.0125,  0.0628],\n",
       "        [-0.2975,  0.1455,  0.1336,  0.0276, -0.1319, -0.2307, -0.2275,  0.0975,\n",
       "          0.0986,  0.0998],\n",
       "        [-0.1589,  0.0867,  0.1411, -0.1722, -0.2041, -0.2370, -0.0277,  0.1874,\n",
       "          0.1327,  0.1257],\n",
       "        [-0.1442, -0.1201,  0.3393, -0.0096,  0.0430, -0.0471, -0.0848,  0.2032,\n",
       "          0.1596, -0.1078],\n",
       "        [-0.0393, -0.0736,  0.0674, -0.0145,  0.1641,  0.1616, -0.0362,  0.0855,\n",
       "         -0.0150, -0.0988],\n",
       "        [-0.1667,  0.0448,  0.0783,  0.0559,  0.0524, -0.0742, -0.2005, -0.0718,\n",
       "          0.1102, -0.0366],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X5 = X5.view(-1, X5.shape[2])\n",
    "X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.3575,  0.2728],\n",
       "          [ 0.3814,  0.3348],\n",
       "          [ 0.4167,  0.3077],\n",
       "          [ 0.3565,  0.1878],\n",
       "          [ 0.3819,  0.0784]],\n",
       "\n",
       "         [[ 0.2913,  0.1325],\n",
       "          [ 0.3766,  0.2446],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013]],\n",
       "\n",
       "         [[ 0.3074,  0.0279],\n",
       "          [ 0.3041, -0.0462],\n",
       "          [ 0.2984, -0.0824],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3575,  0.2728],\n",
       "          [ 0.3814,  0.3348],\n",
       "          [ 0.4167,  0.3077],\n",
       "          [ 0.3565,  0.1878],\n",
       "          [ 0.3819,  0.0784]],\n",
       "\n",
       "         [[ 0.2913,  0.1325],\n",
       "          [ 0.3766,  0.2446],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013]],\n",
       "\n",
       "         [[ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013],\n",
       "          [ 0.2939,  0.2013]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_to_tag(X5).view(batch_size, 3, 5, -1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cat_lstm_last(output, length):\n",
    "    return torch.cat((output[length - 1, 5:], output[0, :5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 5, 10])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = torch.Tensor(torch.zeros((batch_size, seq_len, X5.size(2))))\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-5a28092d744b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msorted_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_cat_lstm_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "embs[sorted_index] = \n",
    "torch.Tensor([_cat_lstm_last(word_feat, length) for word_feat, length in zip(X5, X_lengths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0471, -0.0848,  0.2032,  0.1596, -0.1078, -0.1297,  0.0264,  0.1290,\n",
       "          0.0980, -0.0395], grad_fn=<CatBackward>),\n",
       " tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0393, -0.0736,  0.0674,\n",
       "         -0.0145,  0.1641], grad_fn=<CatBackward>),\n",
       " tensor([ 0.1737,  0.2137,  0.0389,  0.2571, -0.3928, -0.0828, -0.2182,  0.1457,\n",
       "         -0.0035,  0.2346], grad_fn=<CatBackward>),\n",
       " tensor([-0.1466, -0.2590,  0.1123, -0.0125,  0.0628, -0.1297,  0.0264,  0.1290,\n",
       "          0.0980, -0.0395], grad_fn=<CatBackward>),\n",
       " tensor([-0.0742, -0.2005, -0.0718,  0.1102, -0.0366, -0.0393, -0.0736,  0.0674,\n",
       "         -0.0145,  0.1641], grad_fn=<CatBackward>),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<CatBackward>)]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [_cat_lstm_last(word_feat, length) for word_feat, length in zip(X5, X_lengths)]\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0471, -0.0848,  0.2032,  0.1596, -0.1078, -0.1297,  0.0264,  0.1290,\n",
       "          0.0980, -0.0395],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0393, -0.0736,  0.0674,\n",
       "         -0.0145,  0.1641],\n",
       "        [ 0.1737,  0.2137,  0.0389,  0.2571, -0.3928, -0.0828, -0.2182,  0.1457,\n",
       "         -0.0035,  0.2346],\n",
       "        [-0.1466, -0.2590,  0.1123, -0.0125,  0.0628, -0.1297,  0.0264,  0.1290,\n",
       "          0.0980, -0.0395],\n",
       "        [-0.0742, -0.2005, -0.0718,  0.1102, -0.0366, -0.0393, -0.0736,  0.0674,\n",
       "         -0.0145,  0.1641],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
