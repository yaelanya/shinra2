{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Reshape, Dot, Dense, Activation\n",
    "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_file('alice.txt', origin='http://www.gutenberg.org/files/11/11-0.txt')\n",
    "\n",
    "with open(path) as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "corpus = [sentence for sentence in text if sentence.count(' ') >= 2]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
    "EMBED_DIM = 128\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 1, 128)       433664      input_39[0][0]                   \n",
      "                                                                 input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_27 (Dot)                    (None, 1, 1)         0           embedding_24[0][0]               \n",
      "                                                                 embedding_24[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_37 (Reshape)            (None, 1)            0           dot_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            2           reshape_37[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 433,666\n",
      "Trainable params: 433,666\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_target = Input(shape=(1,), dtype='int32')\n",
    "input_context = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "embedding = Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "target = embedding(input_target)\n",
    "context = embedding(input_context)\n",
    "\n",
    "dot = Dot(axes=2)([target, context])\n",
    "dot = Reshape((1,))(dot)\n",
    "output = Dense(1, activation='sigmoid')(dot)\n",
    "\n",
    "model = Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188.6274270415306\n",
      "926.0647531077266\n",
      "893.3706332147121\n",
      "872.4651527181268\n",
      "853.4858864992857\n",
      "832.332134090364\n",
      "806.8422005996108\n",
      "780.1508079171181\n",
      "751.321742631495\n",
      "721.541634503752\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    loss = 0.\n",
    "    for i, doc in enumerate(tokenizer.texts_to_sequences(corpus)):\n",
    "        data, labels = skipgrams(sequence=doc, vocabulary_size=VOCAB_SIZE, window_size=5, negative_samples=5.)\n",
    "        x = [np.array(x) for x in zip(*data)]\n",
    "        y = np.array(labels, dtype=np.int32)\n",
    "        if x:\n",
    "            loss += model.train_on_batch(x, y)\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectors.txt' ,'w') as f:\n",
    "    f.write(f\"{VOCAB_SIZE - 1} {EMBED_DIM}\\n\")\n",
    "    vectors = model.get_weights()[0]\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        f.write(f\"{word} {' '.join(map(str, list(vectors[i, :])))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('serpent', 0.4946148693561554),\n",
       " ('she’ll', 0.48323914408683777),\n",
       " ('ignorant', 0.4303949475288391),\n",
       " ('you’re', 0.4270668625831604),\n",
       " ('whether', 0.4068315327167511),\n",
       " ('‘a', 0.4067261815071106),\n",
       " ('asking', 0.3956163227558136),\n",
       " ('doubtfully', 0.37931880354881287),\n",
       " ('kick', 0.3604133725166321),\n",
       " ('nervous', 0.31047528982162476)]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['girl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
