{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def transform(self, sentences, tags, row_sentences=None):\n",
    "        word_seq = self._transform_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            char_seq = self._transform_char(row_sentences)\n",
    "        else:\n",
    "            char_seq = self._transform_char(sentences)\n",
    "        \n",
    "        tag_seq = self._transform_tag(tags)\n",
    "        \n",
    "        return word_seq, char_seq, tag_seq\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding(self, word_seq, char_seq, tag_seq):\n",
    "        return self._padding_word(word_seq), self._padding_char(char_seq), self._padding_tag(tag_seq)\n",
    "        \n",
    "    def _padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def _padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def _padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def _transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def _transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def _transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, labels, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = [np.array(_input)[shuffle_indices] for _input in data]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X = [_input[start_index: end_index] for _input in shuffled_data]\n",
    "                y = shuffled_labels[start_index: end_index]\n",
    "                \n",
    "                X[0], X[1], y = tokenizer.padding(X[0], X[1], y)\n",
    "                \n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>title</th>\n",
       "      <th>words</th>\n",
       "      <th>repl_words</th>\n",
       "      <th>furigana_tag_seq</th>\n",
       "      <th>another_name_tag_seq</th>\n",
       "      <th>use_tag_seq</th>\n",
       "      <th>type_tag_seq</th>\n",
       "      <th>...</th>\n",
       "      <th>property_tag_seq</th>\n",
       "      <th>raw_material_tag_seq</th>\n",
       "      <th>production_tag_seq</th>\n",
       "      <th>formation_tag_seq</th>\n",
       "      <th>cas_tag_seq</th>\n",
       "      <th>chemical_formula_tag_seq</th>\n",
       "      <th>density_tag_seq</th>\n",
       "      <th>melting_tag_seq</th>\n",
       "      <th>boiling_tag_seq</th>\n",
       "      <th>rational_formula_tag_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10166</td>\n",
       "      <td>False</td>\n",
       "      <td>アンモニア (英: ammonia) は分子式が NH 3 で表される無機化合物。</td>\n",
       "      <td>アンモニア</td>\n",
       "      <td>[アンモニア, (, 英, :, ammonia, ), は, 分子, 式, が, NH, ...</td>\n",
       "      <td>[[title-compound], (, 英, :, [title-compound], ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, B, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10166</td>\n",
       "      <td>False</td>\n",
       "      <td>常温常圧では無色の気体で、特有の強い刺激臭を持つ。</td>\n",
       "      <td>アンモニア</td>\n",
       "      <td>[常温, 常, 圧, で, は, 無色, の, 気体, で, 、, 特有, の, 強い, 刺...</td>\n",
       "      <td>[常温, 常, 圧, で, は, 無色, の, 気体, で, 、, 特有, の, 強い, 刺...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10166</td>\n",
       "      <td>False</td>\n",
       "      <td>水に良く溶けるため、水溶液（アンモニア水）として使用されることも多く、化学工業では基礎的な窒...</td>\n",
       "      <td>アンモニア</td>\n",
       "      <td>[水, に, 良く, 溶ける, ため, 、, 水溶液, （, アンモニア水, ）, として,...</td>\n",
       "      <td>[水, に, 良く, 溶ける, ため, 、, 水溶液, （, [compound], ）, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10166</td>\n",
       "      <td>False</td>\n",
       "      <td>塩基の程度は水酸化ナトリウムより弱い。</td>\n",
       "      <td>アンモニア</td>\n",
       "      <td>[塩基, の, 程度, は, 水酸化ナトリウム, より, 弱い, 。]</td>\n",
       "      <td>[塩基, の, 程度, は, [compound], より, 弱い, 。]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10166</td>\n",
       "      <td>False</td>\n",
       "      <td>窒素原子上の孤立電子対のはたらきにより、金属錯体の配位子となり、その場合はアンミンと呼ばれる。</td>\n",
       "      <td>アンモニア</td>\n",
       "      <td>[窒素, 原子, 上, の, 孤立, 電子, 対, の, はたらき, により, 、, 金属,...</td>\n",
       "      <td>[窒素, 原子, 上, の, 孤立, 電子, 対, の, はたらき, により, 、, 金属,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[B, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _id  label                                           sentence  title  \\\n",
       "0  10166  False          アンモニア (英: ammonia) は分子式が NH 3 で表される無機化合物。  アンモニア   \n",
       "1  10166  False                          常温常圧では無色の気体で、特有の強い刺激臭を持つ。  アンモニア   \n",
       "2  10166  False  水に良く溶けるため、水溶液（アンモニア水）として使用されることも多く、化学工業では基礎的な窒...  アンモニア   \n",
       "3  10166  False                                塩基の程度は水酸化ナトリウムより弱い。  アンモニア   \n",
       "4  10166  False    窒素原子上の孤立電子対のはたらきにより、金属錯体の配位子となり、その場合はアンミンと呼ばれる。  アンモニア   \n",
       "\n",
       "                                               words  \\\n",
       "0  [アンモニア, (, 英, :, ammonia, ), は, 分子, 式, が, NH, ...   \n",
       "1  [常温, 常, 圧, で, は, 無色, の, 気体, で, 、, 特有, の, 強い, 刺...   \n",
       "2  [水, に, 良く, 溶ける, ため, 、, 水溶液, （, アンモニア水, ）, として,...   \n",
       "3                [塩基, の, 程度, は, 水酸化ナトリウム, より, 弱い, 。]   \n",
       "4  [窒素, 原子, 上, の, 孤立, 電子, 対, の, はたらき, により, 、, 金属,...   \n",
       "\n",
       "                                          repl_words  \\\n",
       "0  [[title-compound], (, 英, :, [title-compound], ...   \n",
       "1  [常温, 常, 圧, で, は, 無色, の, 気体, で, 、, 特有, の, 強い, 刺...   \n",
       "2  [水, に, 良く, 溶ける, ため, 、, 水溶液, （, [compound], ）, ...   \n",
       "3              [塩基, の, 程度, は, [compound], より, 弱い, 。]   \n",
       "4  [窒素, 原子, 上, の, 孤立, 電子, 対, の, はたらき, により, 、, 金属,...   \n",
       "\n",
       "                                    furigana_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                another_name_tag_seq  \\\n",
       "0  [O, O, O, O, B, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                         use_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                        type_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                         ...                          \\\n",
       "0                        ...                           \n",
       "1                        ...                           \n",
       "2                        ...                           \n",
       "3                        ...                           \n",
       "4                        ...                           \n",
       "\n",
       "                                    property_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                raw_material_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [B, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                  production_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                   formation_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                         cas_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                            chemical_formula_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                     density_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                     melting_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                     boiling_tag_seq  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                           [O, O, O, O, O, O, O, O]   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                            rational_formula_tag_seq  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3                           [O, O, O, O, O, O, O, O]  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, None, 2 52975       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     10200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    1172300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 150)    0           time_distributed_1[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 150)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 200)    200800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 4)      404         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      44          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,456,823\n",
      "Trainable params: 1,456,823\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "feats = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats = Dense(param['tag_size'])(feats)\n",
    "\n",
    "crf = CRF(param['tag_size'])\n",
    "pred = crf(feats)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=[pred])\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=crf.loss_function, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr = \"原材料\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_train, x_char_train, y_train = \\\n",
    "tokenizer.transform(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=train_df[target_col_name[target_attr]].tolist()\n",
    ")\n",
    "# one-hot encoding\n",
    "y_train = np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in y_train])\n",
    "\n",
    "x_word_test, x_char_test, y_test = \\\n",
    "tokenizer.transform(\n",
    "    sentences=test_df.repl_words.tolist()\n",
    "    , row_sentences=test_df.words.tolist()\n",
    "    , tags=test_df[target_col_name[target_attr]].tolist()\n",
    ")\n",
    "# one-hot encoding\n",
    "y_test = np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1984.0, 851.0, 197801.0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = np.array([[y[:, 0].sum(), y[:, 1].sum(), y[:, 2].sum(), y[:, 3].sum()] for y in y_train])\n",
    "count[:, 0].sum(), count[:, 1].sum(), count[:, 2].sum(), count[:, 3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "233/233 [==============================] - 158s 676ms/step - loss: 4.4364\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - 128s 550ms/step - loss: 4.3281\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - 132s 567ms/step - loss: 4.2677\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - 128s 551ms/step - loss: 4.2166\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - 131s 561ms/step - loss: 4.2177\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - 132s 567ms/step - loss: 4.3291\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - 132s 566ms/step - loss: 4.3201\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - 140s 600ms/step - loss: 4.2711\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - 140s 600ms/step - loss: 4.3105\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - 149s 641ms/step - loss: 4.2067\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - 160s 689ms/step - loss: 4.2410\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - 149s 641ms/step - loss: 4.2864\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - 155s 665ms/step - loss: 4.2109\n",
      "Epoch 14/100\n",
      "233/233 [==============================] - 162s 696ms/step - loss: 4.2440\n",
      "Epoch 15/100\n",
      "233/233 [==============================] - 142s 608ms/step - loss: 4.2691\n",
      "Epoch 16/100\n",
      "233/233 [==============================] - 160s 688ms/step - loss: 4.2447\n",
      "Epoch 17/100\n",
      "233/233 [==============================] - 169s 727ms/step - loss: 4.3326\n",
      "Epoch 18/100\n",
      "233/233 [==============================] - 186s 797ms/step - loss: 4.2608\n",
      "Epoch 19/100\n",
      "233/233 [==============================] - 152s 652ms/step - loss: 4.2853\n",
      "Epoch 20/100\n",
      "233/233 [==============================] - 169s 726ms/step - loss: 4.1907\n",
      "Epoch 21/100\n",
      "233/233 [==============================] - 157s 673ms/step - loss: 4.3489\n",
      "Epoch 22/100\n",
      "233/233 [==============================] - 168s 719ms/step - loss: 4.2851\n",
      "Epoch 23/100\n",
      "233/233 [==============================] - 137s 589ms/step - loss: 4.3114\n",
      "Epoch 24/100\n",
      "233/233 [==============================] - 148s 635ms/step - loss: 4.2574\n",
      "Epoch 25/100\n",
      "233/233 [==============================] - 144s 620ms/step - loss: 4.3479\n",
      "Epoch 26/100\n",
      "233/233 [==============================] - 137s 587ms/step - loss: 4.2605\n",
      "Epoch 27/100\n",
      "233/233 [==============================] - 137s 589ms/step - loss: 4.2652\n",
      "Epoch 28/100\n",
      "233/233 [==============================] - 138s 591ms/step - loss: 4.2552\n",
      "Epoch 29/100\n",
      "233/233 [==============================] - 138s 593ms/step - loss: 4.2591\n",
      "Epoch 30/100\n",
      "233/233 [==============================] - 141s 605ms/step - loss: 4.2963\n",
      "Epoch 31/100\n",
      "233/233 [==============================] - 176s 757ms/step - loss: 4.2666\n",
      "Epoch 32/100\n",
      "233/233 [==============================] - 167s 716ms/step - loss: 4.1936\n",
      "Epoch 33/100\n",
      "233/233 [==============================] - 189s 810ms/step - loss: 4.2698\n",
      "Epoch 34/100\n",
      "233/233 [==============================] - 190s 817ms/step - loss: 4.1986\n",
      "Epoch 35/100\n",
      "233/233 [==============================] - 166s 711ms/step - loss: 4.2293\n",
      "Epoch 36/100\n",
      "233/233 [==============================] - 163s 699ms/step - loss: 4.3348\n",
      "Epoch 37/100\n",
      "233/233 [==============================] - 145s 621ms/step - loss: 4.3245\n",
      "Epoch 38/100\n",
      "233/233 [==============================] - 145s 623ms/step - loss: 4.2141\n",
      "Epoch 39/100\n",
      "233/233 [==============================] - 151s 646ms/step - loss: 4.2840\n",
      "Epoch 40/100\n",
      "233/233 [==============================] - 142s 611ms/step - loss: 4.2213\n",
      "Epoch 41/100\n",
      "233/233 [==============================] - 144s 617ms/step - loss: 4.2159\n",
      "Epoch 42/100\n",
      "233/233 [==============================] - 146s 625ms/step - loss: 4.2819\n",
      "Epoch 43/100\n",
      "233/233 [==============================] - 145s 623ms/step - loss: 4.2943\n",
      "Epoch 44/100\n",
      "233/233 [==============================] - 170s 731ms/step - loss: 4.1522\n",
      "Epoch 45/100\n",
      "233/233 [==============================] - 153s 656ms/step - loss: 4.3355\n",
      "Epoch 46/100\n",
      "233/233 [==============================] - 149s 638ms/step - loss: 4.3169\n",
      "Epoch 47/100\n",
      "233/233 [==============================] - 148s 636ms/step - loss: 4.3269\n",
      "Epoch 48/100\n",
      "233/233 [==============================] - 152s 651ms/step - loss: 4.2800\n",
      "Epoch 49/100\n",
      "233/233 [==============================] - 149s 638ms/step - loss: 4.2057\n",
      "Epoch 50/100\n",
      "233/233 [==============================] - 162s 697ms/step - loss: 4.2726\n",
      "Epoch 51/100\n",
      "233/233 [==============================] - 152s 654ms/step - loss: 4.2157\n",
      "Epoch 52/100\n",
      "233/233 [==============================] - 148s 637ms/step - loss: 4.2137\n",
      "Epoch 53/100\n",
      "233/233 [==============================] - 150s 642ms/step - loss: 4.3090\n",
      "Epoch 54/100\n",
      "233/233 [==============================] - 164s 704ms/step - loss: 4.2983\n",
      "Epoch 55/100\n",
      "233/233 [==============================] - 173s 744ms/step - loss: 4.2959\n",
      "Epoch 56/100\n",
      "212/233 [==========================>...] - ETA: 17s - loss: 4.3865"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1564, 195)\n",
      "(1564, 195, 31)\n",
      "(1564, 195, 4)\n"
     ]
    }
   ],
   "source": [
    "pad_x_word_test, pad_x_char_test, pad_y_test = tokenizer.padding(x_word_test, x_char_test, y_test)\n",
    "print(pad_x_word_test.shape)\n",
    "print(pad_x_char_test.shape)\n",
    "print(pad_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.327455919395466,\n",
       " 'recall': 0.35911602209944754,\n",
       " 'f1': 0.3425559947299078}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq(pad_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): entry['Attributes'] for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    result_dict[row._id] = result_dict.get(row._id, []) + extracted\n",
    "    \n",
    "# 重複を除外\n",
    "for _id, extracted in result_dict.items():\n",
    "    result_dict[_id] = list(set(extracted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'12437': ['メタノール',\n",
       "  'ナトリウムエトキシド',\n",
       "  'エチレングリコール',\n",
       "  '酢酸',\n",
       "  '硫酸',\n",
       "  'ホルムアルデヒド',\n",
       "  '伯エタノール',\n",
       "  'アルコール',\n",
       "  'アセトアルデヒド',\n",
       "  'エチレン',\n",
       "  '酵素'],\n",
       " '19566': ['シアンメトヘモグロビン',\n",
       "  'メトヘモグロビン',\n",
       "  '酸',\n",
       "  'アクリロニトリル',\n",
       "  'ジシアン',\n",
       "  '硫酸',\n",
       "  'シアン化合物',\n",
       "  'アンモニア',\n",
       "  'ウメ',\n",
       "  '金属イオン',\n",
       "  'シアン化物',\n",
       "  'Fe3',\n",
       "  'ヘモグロビン',\n",
       "  'アミグダリン',\n",
       "  'シアン化ナトリウム',\n",
       "  'メタン'],\n",
       " '29891': ['クラウンエーテル',\n",
       "  '酸',\n",
       "  'ポリエチレングリコール',\n",
       "  'エーテル',\n",
       "  'ハロゲン化合物',\n",
       "  '酸素',\n",
       "  'セレニド',\n",
       "  'オレフィン',\n",
       "  'アルコール',\n",
       "  'アルコキシド'],\n",
       " '31121': ['フロリダ州', '水', 'カリフォルニア', 'サンタクルーズ'],\n",
       " '41685': ['カルボキシル基', 'タンパク質', 'グルタミン酸'],\n",
       " '62444': ['糖',\n",
       "  'アイラ・レムセン',\n",
       "  'アンモニア',\n",
       "  '2-スルホ安息香酸',\n",
       "  '塩素',\n",
       "  'レムセン',\n",
       "  'アントラニル酸',\n",
       "  '亜硝酸',\n",
       "  '二酸化硫黄'],\n",
       " '62464': ['アンモニア', '窒素', 'シアヌル酸', 'シアン酸アンモニウム', '硝酸アンモニウム', '水'],\n",
       " '160786': ['メタノール', 'ケイ皮酸', 'トルエン', 'ベンゼン', 'フェニルカルビノール'],\n",
       " '189601': ['Cigua', '赤痢菌', 'シガテラ'],\n",
       " '217088': [],\n",
       " '248891': [],\n",
       " '293373': ['モルヒネ'],\n",
       " '305316': ['グリセリン', '硫酸', '炭酸カルシウム', '水', '硫酸マグネシウム'],\n",
       " '497499': ['メチルアミン'],\n",
       " '514541': ['水銀(I)塩', '水溶液'],\n",
       " '520951': ['1,2-ジクロロエタン', 'アンモニア'],\n",
       " '523020': ['酸素', '水素化ホウ素ナトリウム', 'ヨウ化メチル', 'α-ハロケトンや α-ハロアセタール', '硫黄'],\n",
       " '616186': ['青酸', 'エムルシン', 'シアン化水素', 'ベンズアルデヒド', 'アミグダリン'],\n",
       " '704021': ['3-（トリメチルシリロキシ）-1-（トリブチルスタンニル）プロピン',\n",
       "  '1,3-ブタジエン',\n",
       "  'マレイン酸ジエチル',\n",
       "  '3-メトキシ-1',\n",
       "  '1,5-ヘキサジイン',\n",
       "  'ベンゼン'],\n",
       " '778300': [],\n",
       " '884244': ['二クロム酸'],\n",
       " '954930': ['ホルトノキ'],\n",
       " '969196': ['天然', 'グルコン酸', 'グルコース', 'グルコノ'],\n",
       " '1024773': ['アントラセン',\n",
       "  '6-メチレン-6',\n",
       "  'ペンタセンキノン',\n",
       "  'シクロヘキサン-1,4-ジオン',\n",
       "  'トルエン',\n",
       "  'アルミニウム',\n",
       "  '一酸化炭素',\n",
       "  'フタルアルデヒド',\n",
       "  '水',\n",
       "  'テトラセン',\n",
       "  '硫黄'],\n",
       " '1050477': ['モルヒネ'],\n",
       " '1067475': ['フルオレセイン',\n",
       "  '塩化亜鉛',\n",
       "  'イソチオシアネート',\n",
       "  'カルボキシフルオレセイン',\n",
       "  'オレゴングリーン',\n",
       "  '無水フタル酸',\n",
       "  'TCSPC',\n",
       "  'ナトリウム',\n",
       "  'レソルシノール'],\n",
       " '1095173': ['シアン化カリウム',\n",
       "  'ニッケル',\n",
       "  'シアン化ナトリウム',\n",
       "  '亜硝酸',\n",
       "  'メトヘモグロビン',\n",
       "  'シアン化銅',\n",
       "  '植物',\n",
       "  'シアノヒドリン',\n",
       "  '青酸カリ',\n",
       "  'ハロゲン化物',\n",
       "  'シアン',\n",
       "  '糖',\n",
       "  'チオ硫酸ナトリウム',\n",
       "  'カルバモイルリン酸',\n",
       "  'アクリロニトリル',\n",
       "  '塩化水素',\n",
       "  'シアン化水素',\n",
       "  'ハロゲン化アルキル',\n",
       "  'Fe3',\n",
       "  'ヘモグロビン',\n",
       "  '亜硝酸アミル',\n",
       "  '炭素'],\n",
       " '1140453': [],\n",
       " '1148105': [],\n",
       " '1193389': ['塩化水素',\n",
       "  'ヘキサクロロロジウム酸(III)カリウム',\n",
       "  '濃塩酸',\n",
       "  '塩化カリウム',\n",
       "  '水酸化カリウム',\n",
       "  '塩素ガス',\n",
       "  'ロジウム(III)',\n",
       "  '硝酸銀',\n",
       "  '水酸化ロジウム(III)'],\n",
       " '1198019': ['アルドール',\n",
       "  'アセトン',\n",
       "  '希酸',\n",
       "  'α-イオノン',\n",
       "  'プソイドイオノン',\n",
       "  'シトラール',\n",
       "  '三フッ化ホウ素',\n",
       "  'リン酸'],\n",
       " '1203950': [],\n",
       " '1273537': ['アジピン', 'ヘキサメチレンジアミン'],\n",
       " '1372856': ['チオール基', 'アゾビスイソブチロニトリル'],\n",
       " '1383346': [],\n",
       " '1454192': [],\n",
       " '1483298': ['白金', 'アダムス', 'アンモニア', '酢酸', '水素', '塩化白金酸', 'V. Voorhees', '硝酸'],\n",
       " '1540694': [],\n",
       " '1560938': [],\n",
       " '1563432': [],\n",
       " '1573835': [],\n",
       " '1591322': ['スクロース', 'グルコース'],\n",
       " '1616237': ['グアニジン', 'ビグアニド'],\n",
       " '1625510': ['ベンズイソオキサゾール', '二酸化炭素', 'アントラニル酸', 'ホスゲン', 'クロロギ酸エチル'],\n",
       " '1658717': ['フッ素', 'ヨウ化パラジウム', '五フッ化ヨウ素'],\n",
       " '1663779': ['ベンゾニトリル', 'アンモニア'],\n",
       " '1690387': ['水銀(I)イオン', 'シアン', 'Mercury', '水銀'],\n",
       " '1787749': ['ホスファチジルイノシトール4,5-ビスリン酸', 'ホスホリパーゼ', 'ホスファチジルイノシトール'],\n",
       " '1793432': [],\n",
       " '1815995': [],\n",
       " '1866600': ['グリオキシム', '亜硝酸エステル', 'ジケトン', '水素', 'ジアセチル', 'メチルエチルケトン', 'サンプル'],\n",
       " '1905810': ['塩基性塩Be2CO3', '二酸化炭素', '四水和物', 'アンモニア水', '水酸化ベリリウム', '水'],\n",
       " '1905814': ['アンモニア水', 'ベリリウム塩水溶液', 'マグネシウム塩'],\n",
       " '1932135': ['ノルトロパン', 'コカイン', 'オルニチン'],\n",
       " '1936318': ['メタリジウム'],\n",
       " '1939275': [],\n",
       " '1977270': ['エドモント', 'アンチモン'],\n",
       " '1980911': ['ホスホールを含む環', 'トリフェニルホスフィン', 'フェニルナトリウム', '1,3-ジエン', 'ピロール', '炭素'],\n",
       " '2061207': [],\n",
       " '2080116': [],\n",
       " '2137257': ['シアン', 'メトヘモグロビン', '酸素', '鉄', '亜硝酸アミル', '亜硝酸', '硝酸'],\n",
       " '2140608': [],\n",
       " '2153576': ['トリグリセリド', 'ブドウ糖'],\n",
       " '2170531': ['ナトリウム'],\n",
       " '2190946': [],\n",
       " '2211690': [],\n",
       " '2234076': ['水素', 'アンモニア'],\n",
       " '2312494': [],\n",
       " '2334030': ['Streptomyces filipinensis', 'フィリピンIII', 'フィリピン', 'en', 'エピメリ'],\n",
       " '2346795': ['沸点', '金属クラスター', 'ジアゾメタン', 'CO'],\n",
       " '2488348': ['グルタラール', 'グルタルアルデヒド'],\n",
       " '2569592': ['臭素', '塩素', '3-ジメチルアミノフェノール', '塩化銀', 'ブロモエタン'],\n",
       " '2614499': [],\n",
       " '2727655': ['コバルト'],\n",
       " '2790003': [\"',N''- トリエチレンホスホルアミド\", 'TEPA', '塩化チオホスホリル', 'アジリジン'],\n",
       " '2795973': ['ゲラニオール', 'シトロネラソウ', '水素'],\n",
       " '2806294': ['パクリタキセル'],\n",
       " '2807214': ['ホダルシン', 'ホズロシド'],\n",
       " '2823300': ['神経'],\n",
       " '2996837': ['アルケン', 'エチレンテトラカルボン酸', 'Taherpour', 'メルドラム酸'],\n",
       " '3020464': ['イソブテン'],\n",
       " '3273180': [],\n",
       " '3283637': ['スモン', 'キノホルム'],\n",
       " '3342427': ['4-ピロン-2,6-ジカルボン酸', 'カルボキシ基', 'ケリドン酸', 'アンモニア'],\n",
       " '3352098': ['UO3', 'HF', '酸化ウラン(VI)', '塩素', '四塩化炭素', '塩化ウラン(V)', '塩化ウラン(IV)'],\n",
       " '3419435': ['オルメサルタン'],\n",
       " '3448226': [],\n",
       " '3460340': ['メサドン'],\n",
       " '3463386': ['炭水化物', 'ヨウ素', '硫黄'],\n",
       " '3472884': ['金属', 'パルス'],\n",
       " '3474570': ['α-ケトグルタル酸', 'オルニチン2-オキソグルタル酸', 'オルニチン'],\n",
       " '3483975': ['四塩化ゲルマニウム', '塩素', 'ゲルマニウム', 'クロロゲルマン'],\n",
       " '3519068': ['アンモニア', 'ジイソプロピルアミン'],\n",
       " '3526170': [],\n",
       " '3529078': [],\n",
       " '3560674': ['カルボン酸', '炭素'],\n",
       " '3570913': ['塩化チオニル', 'メタン'],\n",
       " '3683375': []}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df.copy()\n",
    "_pred = np.argmax(y_pred, -1)\n",
    "_pred = [tags[np.where(tags > 0)[0]] for tags in _pred]\n",
    "\n",
    "df = df.assign(pred_tag = _pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['extracted'] = \\\n",
    "df.apply(\n",
    "    lambda x: extract_string(x.words, x.pred_tag)\n",
    "    , axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    if not row['extracted']:\n",
    "        continue\n",
    "    extracted_patt = [''.join(phrase) for phrase in row['extracted']]\n",
    "    extracted_patt = ['\\s*'.join(flatten(patt)) for patt in extracted_patt] # 元の文に空白が入っている場合を考慮\n",
    "    extracted_patt = '|'.join([escape(patt) for patt in extracted_patt])\n",
    "    \n",
    "    match = re.findall(extracted_patt, row['sentence'])\n",
    "    extracted_dict[row['_id']] = extracted_dict.get(row['_id'], []) + match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_set(true_set, pred_set):\n",
    "    TP = len(true_set & pred_set)\n",
    "    FP = len(pred_set - true_set)\n",
    "    FN = len(true_set - pred_set)\n",
    "    \n",
    "    return TP, FP, FN\n",
    "\n",
    "def precision(TP, FP):\n",
    "        return TP / (TP + FP)\n",
    "    \n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def f1(TP, FP, FN):\n",
    "    return 2 * precision(TP, FP) * recall(TP, FN) / (precision(TP, FP) + recall(TP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'アセトアルデヒド',\n",
       " 'アルコール',\n",
       " 'エチレン',\n",
       " 'エチレングリコール',\n",
       " 'ナトリウムエトキシド',\n",
       " 'ホルムアルデヒド',\n",
       " 'メタノール',\n",
       " '伯エタノール',\n",
       " '硫酸',\n",
       " '酢酸',\n",
       " '酵素'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(extracted_dict.get('12437'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for _id in test_df._id.unique():\n",
    "    train_set = set(train_dict[_id][target_attr])\n",
    "    extracted_set = set(extracted_dict.get(_id, []))\n",
    "    extracted_dict[_id] = list(extracted_set)\n",
    "    \n",
    "    tp, fp, fn = evaluate_set(train_set, extracted_set)\n",
    "    TP += tp\n",
    "    FP += fp\n",
    "    FN += fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 160 Extracted size: 299\n",
      "TP: 89 \tFP: 210 \tFN: 71\n",
      "Precision: 0.2976588628762542\n",
      "Recall: 0.55625\n",
      "F1: 0.38779956427015255\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(flatten([v[target_attr] for k, v in train_dict.items() if k in test_df._id.unique()])), \\\n",
    "      \"Extracted size:\", len(flatten([v for k, v in extracted_dict.items()]))\n",
    "     )\n",
    "print(\"TP:\", TP, \"\\tFP:\", FP, \"\\tFN:\", FN)\n",
    "print(\"Precision:\", precision(TP, FP))\n",
    "print(\"Recall:\", recall(TP, FN))\n",
    "print(\"F1:\", f1(TP, FP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 42 \tFP: 45 \tFN: 44\n",
      "Precision: 0.4827586206896552\n",
      "Recall: 0.4883720930232558\n",
      "F1: 0.48554913294797686\n"
     ]
    }
   ],
   "source": [
    "row_TP = df.apply(lambda x: len(x['extracted']) > 0 and x['label'] == True, axis=1).sum() \n",
    "row_FP = df.apply(lambda x: len(x['extracted']) > 0 and x['label'] == False, axis=1).sum() \n",
    "row_FN = df.apply(lambda x: len(x['extracted']) == 0 and x['label'] == True, axis=1).sum() \n",
    "\n",
    "print(\"TP:\", row_TP, \"\\tFP:\", row_FP, \"\\tFN:\", row_FN)\n",
    "print(\"Precision:\", precision(row_TP, row_FP))\n",
    "print(\"Recall:\", recall(row_TP, row_FN))\n",
    "print(\"F1:\", f1(row_TP, row_FP, row_FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/raw-material_with_repl-compounds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    title = test_df.loc[test_df._id == _id].title.tolist()[0]\n",
    "    result_dict[_id] = {'title': title, 'true': train_dict[_id][target_attr], 'predict': extracted_dict.get(_id, [])}\n",
    "\n",
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
