{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        self.POS = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None, pos_seq=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "        \n",
    "        if pos_seq:\n",
    "            self._fit_pos(pos_seq)\n",
    "            self.POS_size = len(self.POS)\n",
    "    \n",
    "    def transform(self, sentences, tags, row_sentences=None):\n",
    "        word_seq = self._transform_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            char_seq = self._transform_char(row_sentences)\n",
    "        else:\n",
    "            char_seq = self._transform_char(sentences)\n",
    "        \n",
    "        tag_seq = self._transform_tag(tags)\n",
    "        \n",
    "        return word_seq, char_seq, tag_seq\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding(self, word_seq, char_seq, tag_seq):\n",
    "        return self._padding_word(word_seq), self._padding_char(char_seq), self._padding_tag(tag_seq)\n",
    "        \n",
    "    def _padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def _padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def _padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "    \n",
    "    def _padding_pos(self, pos_seq):\n",
    "        return pad_sequences(pos_seq, padding='post')\n",
    "    \n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def _fit_pos(self, pos_seq):\n",
    "        for s_pos in pos_seq:\n",
    "            for pos in s_pos:\n",
    "                if pos in self.POS:\n",
    "                    continue\n",
    "                self.POS[pos] = len(self.POS)\n",
    "                \n",
    "    def _transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def _transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def _transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def _transform_pos(self, pos_seq):\n",
    "        seq = []\n",
    "        for s_pos in pos_seq:\n",
    "            pos_ids = [self.POS[pos] for pos in s_pos]\n",
    "            seq.append(pos_ids)\n",
    "            \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, labels, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(data[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(data[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_data = [np.array(_input)[shuffle_indices] for _input in data]\n",
    "                shuffled_labels = labels[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = data\n",
    "                shuffled_labels = labels\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                X = [_input[start_index: end_index] for _input in shuffled_data]\n",
    "                y = shuffled_labels[start_index: end_index]\n",
    "                \n",
    "                X[0], X[1], y = tokenizer.padding(X[0], X[1], y)\n",
    "                \n",
    "                yield X, y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_f1(y_true, y_pred):\n",
    "    y_true = np.argmax(y_true, -1)\n",
    "    y_true = [tags[np.where(tags > 0)[0]] for tags in y_true]\n",
    "    y_true = tokenizer.inverse_transform_tag(y_true)\n",
    "\n",
    "    y_pred = np.argmax(y_pred, -1)\n",
    "    y_pred = [tags[np.where(tags > 0)[0]] for tags in y_pred]\n",
    "    y_pred = tokenizer.inverse_transform_tag(y_pred)\n",
    "\n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/Production_train_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/Production_test_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=train_df.tag.tolist()\n",
    "    , pos_seq=train_df.POS.tolist() + test_df.POS.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'pos_size': tokenizer.POS_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'pos_emb_dim': 10\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'activation': 'tanh'\n",
    "    , 'optimizer': 'adam'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, None, 2 52975       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     10200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    1172300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 150)    0           time_distributed_1[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 150)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 200)    200800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 4)      804         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      44          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,437,123\n",
      "Trainable params: 1,437,123\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['activation']))(feats)\n",
    "\n",
    "feats = Dense(param['tag_size'])(feats)\n",
    "\n",
    "crf = CRF(param['tag_size'])\n",
    "pred = crf(feats)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=[pred])\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.)\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=crf.loss_function, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_train, x_char_train, y_train = \\\n",
    "tokenizer.transform(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=train_df.tag.tolist()\n",
    ")\n",
    "x_pos_train = tokenizer._transform_pos(train_df.POS.tolist())\n",
    "# one-hot encoding\n",
    "y_train = np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in y_train])\n",
    "\n",
    "x_word_test, x_char_test, y_test = \\\n",
    "tokenizer.transform(\n",
    "    sentences=test_df.repl_words.tolist()\n",
    "    , row_sentences=test_df.words.tolist()\n",
    "    , tags=test_df.tag.tolist()\n",
    ")\n",
    "x_pos_test = tokenizer._transform_pos(test_df.POS.tolist())\n",
    "# one-hot encoding\n",
    "y_test = np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 192322.0, 593.0, 7721.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = np.array([[y[:, 0].sum(), y[:, 1].sum(), y[:, 2].sum(), y[:, 3].sum()] for y in y_train])\n",
    "count[:, 0].sum(), count[:, 1].sum(), count[:, 2].sum(), count[:, 3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "233/233 [==============================] - 158s 677ms/step - loss: 4.5857\n",
      "Epoch 2/100\n",
      "233/233 [==============================] - 153s 655ms/step - loss: 4.3427\n",
      "Epoch 3/100\n",
      "233/233 [==============================] - 137s 588ms/step - loss: 4.4020\n",
      "Epoch 4/100\n",
      "233/233 [==============================] - 152s 652ms/step - loss: 4.3515\n",
      "Epoch 5/100\n",
      "233/233 [==============================] - 164s 703ms/step - loss: 4.3265\n",
      "Epoch 6/100\n",
      "233/233 [==============================] - 135s 580ms/step - loss: 4.3452\n",
      "Epoch 7/100\n",
      "233/233 [==============================] - 144s 618ms/step - loss: 4.2789\n",
      "Epoch 8/100\n",
      "233/233 [==============================] - 161s 693ms/step - loss: 4.2407\n",
      "Epoch 9/100\n",
      "233/233 [==============================] - 136s 583ms/step - loss: 4.2367\n",
      "Epoch 10/100\n",
      "233/233 [==============================] - 136s 585ms/step - loss: 4.3234\n",
      "Epoch 11/100\n",
      "233/233 [==============================] - 168s 721ms/step - loss: 4.1895\n",
      "Epoch 12/100\n",
      "233/233 [==============================] - 170s 730ms/step - loss: 4.3201\n",
      "Epoch 13/100\n",
      "233/233 [==============================] - 136s 583ms/step - loss: 4.3366\n",
      "Epoch 14/100\n",
      "233/233 [==============================] - 133s 570ms/step - loss: 4.2755\n",
      "Epoch 15/100\n",
      "233/233 [==============================] - 153s 657ms/step - loss: 4.2662\n",
      "Epoch 16/100\n",
      "233/233 [==============================] - 141s 605ms/step - loss: 4.3081\n",
      "Epoch 17/100\n",
      "233/233 [==============================] - 145s 621ms/step - loss: 4.2431\n",
      "Epoch 18/100\n",
      "233/233 [==============================] - 144s 619ms/step - loss: 4.2020\n",
      "Epoch 19/100\n",
      "233/233 [==============================] - 138s 594ms/step - loss: 4.3761\n",
      "Epoch 20/100\n",
      "233/233 [==============================] - 145s 622ms/step - loss: 4.2544\n",
      "Epoch 21/100\n",
      "233/233 [==============================] - 144s 619ms/step - loss: 4.2252\n",
      "Epoch 22/100\n",
      "233/233 [==============================] - 164s 704ms/step - loss: 4.3014\n",
      "Epoch 23/100\n",
      "233/233 [==============================] - 144s 620ms/step - loss: 4.2536\n",
      "Epoch 24/100\n",
      "233/233 [==============================] - 140s 600ms/step - loss: 4.2299\n",
      "Epoch 25/100\n",
      "233/233 [==============================] - 157s 673ms/step - loss: 4.2798\n",
      "Epoch 26/100\n",
      "233/233 [==============================] - 158s 678ms/step - loss: 4.2630\n",
      "Epoch 27/100\n",
      "233/233 [==============================] - 160s 688ms/step - loss: 4.3362\n",
      "Epoch 28/100\n",
      "233/233 [==============================] - 143s 614ms/step - loss: 4.2831\n",
      "Epoch 29/100\n",
      "233/233 [==============================] - 143s 615ms/step - loss: 4.1298\n",
      "Epoch 30/100\n",
      "233/233 [==============================] - 151s 650ms/step - loss: 4.2780\n",
      "Epoch 31/100\n",
      "233/233 [==============================] - 185s 795ms/step - loss: 4.1431\n",
      "Epoch 32/100\n",
      "233/233 [==============================] - 156s 668ms/step - loss: 4.2725\n",
      "Epoch 33/100\n",
      "233/233 [==============================] - 138s 594ms/step - loss: 4.2465\n",
      "Epoch 34/100\n",
      "233/233 [==============================] - 142s 608ms/step - loss: 4.3721\n",
      "Epoch 35/100\n",
      "233/233 [==============================] - 146s 626ms/step - loss: 4.3211\n",
      "Epoch 36/100\n",
      "233/233 [==============================] - 142s 610ms/step - loss: 4.2534\n",
      "Epoch 37/100\n",
      "233/233 [==============================] - 142s 610ms/step - loss: 4.2710\n",
      "Epoch 38/100\n",
      "233/233 [==============================] - 141s 606ms/step - loss: 4.2464\n",
      "Epoch 39/100\n",
      "233/233 [==============================] - 143s 613ms/step - loss: 4.2652\n",
      "Epoch 40/100\n",
      "233/233 [==============================] - 144s 619ms/step - loss: 4.3644\n",
      "Epoch 41/100\n",
      "233/233 [==============================] - 143s 615ms/step - loss: 4.3171\n",
      "Epoch 42/100\n",
      "233/233 [==============================] - 143s 615ms/step - loss: 4.2048\n",
      "Epoch 43/100\n",
      "233/233 [==============================] - 142s 610ms/step - loss: 4.3193\n",
      "Epoch 44/100\n",
      "233/233 [==============================] - 143s 614ms/step - loss: 4.2587\n",
      "Epoch 45/100\n",
      "233/233 [==============================] - 145s 622ms/step - loss: 4.3437\n",
      "Epoch 46/100\n",
      "233/233 [==============================] - 141s 607ms/step - loss: 4.2331\n",
      "Epoch 47/100\n",
      "233/233 [==============================] - 149s 639ms/step - loss: 4.2823\n",
      "Epoch 48/100\n",
      "233/233 [==============================] - 142s 611ms/step - loss: 4.2921\n",
      "Epoch 49/100\n",
      "233/233 [==============================] - 144s 618ms/step - loss: 4.2872\n",
      "Epoch 50/100\n",
      "233/233 [==============================] - 143s 614ms/step - loss: 4.2718\n",
      "Epoch 51/100\n",
      "233/233 [==============================] - 146s 627ms/step - loss: 4.3355\n",
      "Epoch 52/100\n",
      "233/233 [==============================] - 145s 622ms/step - loss: 4.2555\n",
      "Epoch 53/100\n",
      "233/233 [==============================] - 146s 628ms/step - loss: 4.2880\n",
      "Epoch 54/100\n",
      "233/233 [==============================] - 144s 616ms/step - loss: 4.2524\n",
      "Epoch 55/100\n",
      "233/233 [==============================] - 145s 622ms/step - loss: 4.3468\n",
      "Epoch 56/100\n",
      "233/233 [==============================] - 144s 617ms/step - loss: 4.2725\n",
      "Epoch 57/100\n",
      "233/233 [==============================] - 145s 624ms/step - loss: 4.2598\n",
      "Epoch 58/100\n",
      "233/233 [==============================] - 144s 618ms/step - loss: 4.2325\n",
      "Epoch 59/100\n",
      "233/233 [==============================] - 146s 626ms/step - loss: 4.2192\n",
      "Epoch 60/100\n",
      "233/233 [==============================] - 147s 633ms/step - loss: 4.2953\n",
      "Epoch 61/100\n",
      "233/233 [==============================] - 146s 629ms/step - loss: 4.2634\n",
      "Epoch 62/100\n",
      "233/233 [==============================] - 143s 614ms/step - loss: 4.1878\n",
      "Epoch 63/100\n",
      "233/233 [==============================] - 145s 624ms/step - loss: 4.2202\n",
      "Epoch 64/100\n",
      "233/233 [==============================] - 145s 623ms/step - loss: 4.2517\n",
      "Epoch 65/100\n",
      "233/233 [==============================] - 146s 628ms/step - loss: 4.2340\n",
      "Epoch 66/100\n",
      "233/233 [==============================] - 150s 644ms/step - loss: 4.2607\n",
      "Epoch 67/100\n",
      "233/233 [==============================] - 145s 622ms/step - loss: 4.2882\n",
      "Epoch 68/100\n",
      "233/233 [==============================] - 147s 630ms/step - loss: 4.3703\n",
      "Epoch 69/100\n",
      "233/233 [==============================] - 146s 627ms/step - loss: 4.2678\n",
      "Epoch 70/100\n",
      "233/233 [==============================] - 148s 634ms/step - loss: 4.2772\n",
      "Epoch 71/100\n",
      "233/233 [==============================] - 149s 639ms/step - loss: 4.3312\n",
      "Epoch 72/100\n",
      "233/233 [==============================] - 148s 637ms/step - loss: 4.2032\n",
      "Epoch 73/100\n",
      "233/233 [==============================] - 149s 641ms/step - loss: 4.3049\n",
      "Epoch 74/100\n",
      "233/233 [==============================] - 148s 634ms/step - loss: 4.2244\n",
      "Epoch 75/100\n",
      "233/233 [==============================] - 145s 623ms/step - loss: 4.1874\n",
      "Epoch 76/100\n",
      "233/233 [==============================] - 148s 634ms/step - loss: 4.2152\n",
      "Epoch 77/100\n",
      "233/233 [==============================] - 149s 639ms/step - loss: 4.3312\n",
      "Epoch 78/100\n",
      "233/233 [==============================] - 148s 635ms/step - loss: 4.2484\n",
      "Epoch 79/100\n",
      "233/233 [==============================] - 146s 628ms/step - loss: 4.1682\n",
      "Epoch 80/100\n",
      "233/233 [==============================] - 149s 641ms/step - loss: 4.2078\n",
      "Epoch 81/100\n",
      "233/233 [==============================] - 150s 644ms/step - loss: 4.3028\n",
      "Epoch 82/100\n",
      "233/233 [==============================] - 151s 646ms/step - loss: 4.2073\n",
      "Epoch 83/100\n",
      "233/233 [==============================] - 148s 637ms/step - loss: 4.3169\n",
      "Epoch 84/100\n",
      "233/233 [==============================] - 151s 648ms/step - loss: 4.2707\n",
      "Epoch 85/100\n",
      "233/233 [==============================] - 149s 640ms/step - loss: 4.3209\n",
      "Epoch 86/100\n",
      "233/233 [==============================] - 146s 628ms/step - loss: 4.1656\n",
      "Epoch 87/100\n",
      "233/233 [==============================] - 149s 638ms/step - loss: 4.2848\n",
      "Epoch 88/100\n",
      "233/233 [==============================] - 150s 644ms/step - loss: 4.3555\n",
      "Epoch 89/100\n",
      "233/233 [==============================] - 149s 638ms/step - loss: 4.2278\n",
      "Epoch 90/100\n",
      "233/233 [==============================] - 150s 645ms/step - loss: 4.2208\n",
      "Epoch 91/100\n",
      "233/233 [==============================] - 149s 638ms/step - loss: 4.1602\n",
      "Epoch 92/100\n",
      "233/233 [==============================] - 154s 661ms/step - loss: 4.2766\n",
      "Epoch 93/100\n",
      "233/233 [==============================] - 151s 648ms/step - loss: 4.2724\n",
      "Epoch 94/100\n",
      "233/233 [==============================] - 150s 642ms/step - loss: 4.2603\n",
      "Epoch 95/100\n",
      "233/233 [==============================] - 149s 641ms/step - loss: 4.2648\n",
      "Epoch 96/100\n",
      "233/233 [==============================] - 153s 655ms/step - loss: 4.3425\n",
      "Epoch 97/100\n",
      "233/233 [==============================] - 152s 653ms/step - loss: 4.1884\n",
      "Epoch 98/100\n",
      "233/233 [==============================] - 151s 650ms/step - loss: 4.2472\n",
      "Epoch 99/100\n",
      "233/233 [==============================] - 153s 656ms/step - loss: 4.3110\n",
      "Epoch 100/100\n",
      "233/233 [==============================] - 150s 644ms/step - loss: 4.2449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e75a198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/BiLSTM_CRF_NER_with_POS.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/BiLSTM_CRF_NER_with_POS.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1564, 195)\n",
      "(1564, 195, 31)\n",
      "(1564, 195, 4)\n"
     ]
    }
   ],
   "source": [
    "pad_x_word_test, pad_x_char_test, pad_y_test = tokenizer.padding(x_word_test, x_char_test, y_test)\n",
    "print(pad_x_word_test.shape)\n",
    "print(pad_x_char_test.shape)\n",
    "print(pad_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24338624338624337"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_f1(pad_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): entry['Attributes'] for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0, 'O': 1, 'B': 2, 'I': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_phrase(word_seq, tag_seq):\n",
    "    phrase_list = []\n",
    "    phrase = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == 2 or tag == 3:\n",
    "            phrase.append(word)\n",
    "        elif phrase:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = []\n",
    "\n",
    "    if phrase:\n",
    "        phrase_list.append(phrase)\n",
    "        \n",
    "    return phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    #_s = _s.replace(r'*', r'\\*')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    return _s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df.copy()\n",
    "_pred = np.argmax(y_pred, -1)\n",
    "_pred = [tags[np.where(tags > 0)[0]] for tags in _pred]\n",
    "\n",
    "df = df.assign(pred_tag = _pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['extracted'] = \\\n",
    "df.apply(\n",
    "    lambda x: extract_phrase(x.words, x.pred_tag)\n",
    "    , axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_dict = {}\n",
    "for i, row in df.iterrows():\n",
    "    if not row['extracted']:\n",
    "        continue\n",
    "    extracted_patt = [''.join(phrase) for phrase in row['extracted']]\n",
    "    extracted_patt = ['\\s*'.join(flatten(patt)) for patt in extracted_patt] # 元の文に空白が入っている場合を考慮\n",
    "    extracted_patt = '|'.join([escape(patt) for patt in extracted_patt])\n",
    "    \n",
    "    match = re.findall(extracted_patt, row['sentence'])\n",
    "    extracted_dict[row['_id']] = extracted_dict.get(row['_id'], []) + match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_set(true_set, pred_set):\n",
    "    TP = len(true_set & pred_set)\n",
    "    FP = len(pred_set - true_set)\n",
    "    FN = len(true_set - pred_set)\n",
    "    \n",
    "    return TP, FP, FN\n",
    "\n",
    "def precision(TP, FP):\n",
    "        return TP / (TP + FP)\n",
    "    \n",
    "def recall(TP, FN):\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def f1(TP, FP, FN):\n",
    "    return 2 * precision(TP, FP) * recall(TP, FN) / (precision(TP, FP) + recall(TP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "for _id in test_df._id.unique():\n",
    "    train_set = set(train_dict[_id]['製造方法'])\n",
    "    extracted_set = set(extracted_dict.get(_id, []))\n",
    "    \n",
    "    tp, fp, fn = evaluate_set(train_set, extracted_set)\n",
    "    TP += tp\n",
    "    FP += fp\n",
    "    FN += fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 94 Extracted size: 93\n",
      "TP: 11 \tFP: 82 \tFN: 83\n",
      "Precision: 0.11827956989247312\n",
      "Recall: 0.11702127659574468\n",
      "F1: 0.11764705882352942\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size:\", len(flatten([v['製造方法'] for k, v in train_dict.items() if k in test_df._id.unique()])), \\\n",
    "      \"Extracted size:\", len(flatten([v for k, v in extracted_dict.items()]))\n",
    "     )\n",
    "print(\"TP:\", TP, \"\\tFP:\", FP, \"\\tFN:\", FN)\n",
    "print(\"Precision:\", precision(TP, FP))\n",
    "print(\"Recall:\", recall(TP, FN))\n",
    "print(\"F1:\", f1(TP, FP, FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 42 \tFP: 45 \tFN: 44\n",
      "Precision: 0.4827586206896552\n",
      "Recall: 0.4883720930232558\n",
      "F1: 0.48554913294797686\n"
     ]
    }
   ],
   "source": [
    "row_TP = df.apply(lambda x: len(x['extracted']) > 0 and x['label'] == True, axis=1).sum() \n",
    "row_FP = df.apply(lambda x: len(x['extracted']) > 0 and x['label'] == False, axis=1).sum() \n",
    "row_FN = df.apply(lambda x: len(x['extracted']) == 0 and x['label'] == True, axis=1).sum() \n",
    "\n",
    "print(\"TP:\", row_TP, \"\\tFP:\", row_FP, \"\\tFN:\", row_FN)\n",
    "print(\"Precision:\", precision(row_TP, row_FP))\n",
    "print(\"Recall:\", recall(row_TP, row_FN))\n",
    "print(\"F1:\", f1(row_TP, row_FP, row_FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    title = test_df.loc[test_df._id == _id].title.tolist()[0]\n",
    "    result_dict[_id] = {'title': title, 'true': train_dict[_id]['製造方法'], 'predict': extracted_dict.get(_id, [])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/type-production_with_repl-compounds.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
