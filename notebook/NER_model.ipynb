{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(X[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(X[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_X = [np.array(_input)[shuffle_indices] for _input in X]\n",
    "                shuffled_y = [np.array(target)[shuffle_indices] for target in y]\n",
    "            else:\n",
    "                shuffled_data = X\n",
    "                shuffled_labels = y\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                batch_X = [_input[start_index: end_index] for _input in shuffled_X]\n",
    "                batch_y = [target[start_index: end_index] for target in shuffled_y]\n",
    "                \n",
    "                batch_X[0] = tokenizer.padding_word(batch_X[0])\n",
    "                batch_X[1] = tokenizer.padding_char(batch_X[1])\n",
    "                batch_y = [tokenizer.padding_tag(attr_y) for attr_y in batch_y]\n",
    "                \n",
    "                yield batch_X, batch_y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, None, 2 52975       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     10200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    1172300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 150)    0           time_distributed_1[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 150)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 200)    200800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 4)      404         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 4)      404         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      44          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, None, 4)      44          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,477,371\n",
      "Trainable params: 1,477,371\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "feats1 = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats1 = Dense(param['tag_size'])(feats1)\n",
    "crf1 = CRF(param['tag_size'])\n",
    "pred1 = crf1(feats1)\n",
    "\n",
    "feats2 = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats2 = Dense(param['tag_size'])(feats2)\n",
    "crf2 = CRF(param['tag_size'])\n",
    "pred2 = crf2(feats2)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=[pred1, pred2])\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=[crf1.loss_function, crf2.loss_function], optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr_list = [\"原材料\", \"製造方法\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_onehot(tag_seq, tokenizer):\n",
    "    return np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_train = tokenizer.transform_word(train_df.repl_words.tolist())\n",
    "x_char_train = tokenizer.transform_char(train_df.words.tolist())\n",
    "y_train = [tokenizer.transform_tag(train_df[target_col_name[attr]].tolist()) for attr in target_attr_list]\n",
    "# one-hot encoding\n",
    "y_train = np.array([encoding_onehot(tag_seq, tokenizer) for tag_seq in y_train])\n",
    "\n",
    "x_word_test = tokenizer.transform_word(test_df.repl_words.tolist())\n",
    "x_char_test = tokenizer.transform_char(test_df.words.tolist())\n",
    "y_test = [tokenizer.transform_tag(test_df[target_col_name[attr]].tolist()) for attr in target_attr_list]\n",
    "# one-hot encoding\n",
    "y_test = np.array([encoding_onehot(tag_seq, tokenizer) for tag_seq in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[[0, 0, 0, 1],\n        [0, 0, 0, 1],\n        [0, 0, 0, 1],\n        ...,\n        [0, 0, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 0, 0]],\n\n       [[0, 0, 0, 1],\n        [0, 0, 0, 1],\n        ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-75da9920e6bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(train_batches, train_steps\n\u001b[1;32m      2\u001b[0m                     \u001b[0;31m#, validation_data=valid_batches, validation_steps=valid_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     \u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                    )\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[[0, 0, 0, 1],\n        [0, 0, 0, 1],\n        [0, 0, 0, 1],\n        ...,\n        [0, 0, 0, 0],\n        [0, 0, 0, 0],\n        [0, 0, 0, 0]],\n\n       [[0, 0, 0, 1],\n        [0, 0, 0, 1],\n        ..."
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_x_word_test = tokenizer.padding_word(x_word_test)\n",
    "pad_x_char_test = tokenizer.padding_char(x_char_test)\n",
    "pad_y_test = [tokenizer.padding_tag(target) for target in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): {'title': entry['Name'], 'attributes': entry['Attributes']} for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []\n",
    "\n",
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s\n",
    "\n",
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt\n",
    "\n",
    "def evaluate_exact_match(result_dict):\n",
    "    annotation_size = 0\n",
    "    extracted_size = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for _id, val in result_dict.items():\n",
    "        true_set = set(val['true'])\n",
    "        pred_set = set(val['predict'])\n",
    "        \n",
    "        annotation_size += len(true_set)\n",
    "        extracted_size += len(pred_set)\n",
    "        TP += len(true_set & pred_set)\n",
    "        FP += len(pred_set - true_set)\n",
    "        FN += len(true_set - pred_set)\n",
    "\n",
    "    def precision(TP, FP):\n",
    "            return TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "    def recall(TP, FN):\n",
    "        return TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "    def f1(precision, recall):\n",
    "        return 2 * precision * recall / (precision + recall) \\\n",
    "            if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    score = {\n",
    "        'annotation_size': annotation_size\n",
    "        , 'extracted_size': extracted_size\n",
    "        , 'TP': TP\n",
    "        , 'FP': FP\n",
    "        , 'FN': FN\n",
    "        , 'precision': precision(TP, FP)\n",
    "        , 'recall': recall(TP, FN)\n",
    "        , 'f1': f1(precision(TP, FP), recall(TP, FN))\n",
    "    }\n",
    "    \n",
    "    return score\n",
    "\n",
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]\n",
    "\n",
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attr = \"原材料\"\n",
    "attr_y_pred = y_pred[target_attr_list.index(target_attr)]\n",
    "attr_y_test = y_test[target_attr_list.index(target_attr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (24,4) (37,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d3fd32578b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-8b10473f4bca>\u001b[0m in \u001b[0;36mevaluate_seq\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0m_y_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0m_y_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_y_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0m_y_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_y_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8b10473f4bca>\u001b[0m in \u001b[0;36monehot2id\u001b[0;34m(onehot_seq)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0monehot2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_pad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/var/pyenv/versions/3.6.1/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (24,4) (37,4) "
     ]
    }
   ],
   "source": [
    "evaluate_seq(attr_y_test, attr_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(attr_y_pred))\n",
    "\n",
    "extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    \n",
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotation_size': 160,\n",
       " 'extracted_size': 2,\n",
       " 'TP': 0,\n",
       " 'FP': 2,\n",
       " 'FN': 160,\n",
       " 'precision': 0.0,\n",
       " 'recall': 0.0,\n",
       " 'f1': 0.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_exact_match(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/raw-material_with_repl-compounds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
