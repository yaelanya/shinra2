{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(X[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(X[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_X = [np.array(_input)[shuffle_indices] for _input in X]\n",
    "                shuffled_y = np.array(y)[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = X\n",
    "                shuffled_labels = y\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                batch_X = [_input[start_index: end_index] for _input in shuffled_X]\n",
    "                batch_y = shuffled_y[start_index: end_index]\n",
    "                \n",
    "                batch_X[0] = tokenizer.padding_word(batch_X[0])\n",
    "                batch_X[1] = tokenizer.padding_char(batch_X[1])\n",
    "                \n",
    "                batch_y = tokenizer.padding_tag(batch_y)\n",
    "                \n",
    "                yield batch_X, batch_y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}\n",
    "\n",
    "target_headline_name = {\n",
    "    '原材料': \"cat_raw_material_headline\"\n",
    "    , '製造方法': \"cat_production_headline\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### CAUTION\n",
    "###\n",
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound-list+pubchem.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound-list+pubchem.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### CAUTION\n",
    "###\n",
    "# cheking words or repl_words\n",
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_pubchem.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, None, 2 52975       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 50)     10200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 100)    1171500     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 150)    0           time_distributed_3[0][0]         \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 150)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, None, 200)    200800      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 100)    20100       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 4)      404         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_3 (CRF)                     (None, None, 4)      44          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,456,023\n",
      "Trainable params: 1,456,023\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "\n",
    "feats = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats = Dense(param['tag_size'])(feats)\n",
    "\n",
    "crf = CRF(param['tag_size'])\n",
    "pred = crf(feats)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=pred)\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=crf.loss_function, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr = \"原材料\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_onehot(tag_seq, tokenizer):\n",
    "    return np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### CAUTION\n",
    "###\n",
    "# cheking words or repl_words\n",
    "x_word_train = tokenizer.transform_word(train_df.repl_pubchem.tolist())\n",
    "x_char_train = tokenizer.transform_char(train_df.words.tolist())\n",
    "y_train = tokenizer.transform_tag(train_df[target_col_name[target_attr]].tolist())\n",
    "\n",
    "x_word_test = tokenizer.transform_word(test_df.repl_pubchem.tolist())\n",
    "x_char_test = tokenizer.transform_char(test_df.words.tolist())\n",
    "y_test = tokenizer.transform_tag(test_df[target_col_name[target_attr]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y_train = encoding_onehot(y_train, tokenizer)\n",
    "y_test = encoding_onehot(y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "59/59 [==============================] - 41s 703ms/step - loss: 7.7037\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 38s 641ms/step - loss: 7.3449\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 37s 631ms/step - loss: 7.1901\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 38s 638ms/step - loss: 7.3397\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 38s 651ms/step - loss: 7.5706\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 7.1491\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 37s 635ms/step - loss: 7.2646\n",
      "Epoch 8/100\n",
      "27/59 [============>.................] - ETA: 18s - loss: 6.0000"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/bilstm-crf_raw-material_using_compound-list+pubchem.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/bilstm-crf_raw-material_using_compound-list+pubchem.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_x_word_test = tokenizer.padding_word(x_word_test)\n",
    "pad_x_char_test = tokenizer.padding_char(x_char_test)\n",
    "pad_y_test = tokenizer.padding_tag(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): {'title': entry['Name'], 'attributes': entry['Attributes']} for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I'] or tag in ['B', 'I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []\n",
    "\n",
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s\n",
    "\n",
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt\n",
    "\n",
    "def evaluate_exact_match(result_dict):\n",
    "    annotation_size = 0\n",
    "    extracted_size = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for _id, val in result_dict.items():\n",
    "        true_set = set(val['true'])\n",
    "        pred_set = set(val['predict'])\n",
    "        \n",
    "        annotation_size += len(true_set)\n",
    "        extracted_size += len(pred_set)\n",
    "        TP += len(true_set & pred_set)\n",
    "        FP += len(pred_set - true_set)\n",
    "        FN += len(true_set - pred_set)\n",
    "\n",
    "    def precision(TP, FP):\n",
    "            return TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "    def recall(TP, FN):\n",
    "        return TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "    def f1(precision, recall):\n",
    "        return 2 * precision * recall / (precision + recall) \\\n",
    "            if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    score = {\n",
    "        'annotation_size': annotation_size\n",
    "        , 'extracted_size': extracted_size\n",
    "        , 'TP': TP\n",
    "        , 'FP': FP\n",
    "        , 'FN': FN\n",
    "        , 'precision': precision(TP, FP)\n",
    "        , 'recall': recall(TP, FN)\n",
    "        , 'f1': f1(precision(TP, FP), recall(TP, FN))\n",
    "    }\n",
    "    \n",
    "    return score\n",
    "\n",
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]\n",
    "\n",
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }\n",
    "\n",
    "def filtering_ids(df):\n",
    "    ids = []\n",
    "    for _id, g in df.groupby('_id'):\n",
    "        if (g.cat_raw_material_headline == 1).any():\n",
    "            ids.append(_id)\n",
    "            \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.3425559947299078,\n",
       " 'precision': 0.327455919395466,\n",
       " 'recall': 0.35911602209944754}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq(pad_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_filter = True\n",
    "\n",
    "pred_tag_seq = remove_pad(onehot2id(y_pred))\n",
    "\n",
    "ids = filtering_ids(test_df)\n",
    "extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    if is_filter:\n",
    "        if ((row._id in ids) and (row.loc[target_headline_name[target_attr]] == 1)) or (row._id not in ids):\n",
    "            extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    else:\n",
    "        extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    \n",
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/result/raw-material.json\", 'r') as f:\n",
    "    extracted_dict = json.load(f)\n",
    "\n",
    "ids = filtering_ids(test_df)\n",
    "result_dict = {}\n",
    "for _id in ids:\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict[_id]['predict']))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(y_pred))\n",
    "\n",
    "extracted_dict = {}\n",
    "true_extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    if row.loc[target_headline_name[target_attr]] == 1:\n",
    "        extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "        extracted = extract_strings(row.sentence, extracted)\n",
    "\n",
    "        true_extracted = extract_words(row.words, row.raw_material_tag_seq)\n",
    "        true_extracted = extract_strings(row.sentence, true_extracted)\n",
    "        \n",
    "        extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "        true_extracted_dict[row._id] = true_extracted_dict.get(row._id, []) + true_extracted\n",
    "\n",
    "ids = filtering_ids(test_df)\n",
    "result_dict = {}\n",
    "for _id in ids:\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': list(set(true_extracted_dict[_id]))\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1171"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[df.cat_raw_material_headline != 2].groupby(['_id', 'h2']).count().title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FN': 58,\n",
       " 'FP': 111,\n",
       " 'TP': 57,\n",
       " 'annotation_size': 115,\n",
       " 'extracted_size': 168,\n",
       " 'f1': 0.40282685512367494,\n",
       " 'precision': 0.3392857142857143,\n",
       " 'recall': 0.4956521739130435}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_exact_match(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/result/raw-material_using_compound-list+PubChem+filter.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
