{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(X[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(X[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_X = [np.array(_input)[shuffle_indices] for _input in X]\n",
    "                shuffled_y = np.array(y)[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = X\n",
    "                shuffled_labels = y\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                batch_X = [_input[start_index: end_index] for _input in shuffled_X]\n",
    "                batch_y = shuffled_y[start_index: end_index]\n",
    "                \n",
    "                batch_X[0] = tokenizer.padding_word(batch_X[0])\n",
    "                batch_X[1] = tokenizer.padding_char(batch_X[1])\n",
    "                batch_X[2] = np.array([[batch_X[2][i]] * len(x) for i, x in enumerate(batch_X[0])])\n",
    "                \n",
    "                batch_y = tokenizer.padding_tag(batch_y)\n",
    "                \n",
    "                yield batch_X, batch_y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}\n",
    "\n",
    "target_headline_name = {\n",
    "    '原材料': \"cat_raw_material_headline\"\n",
    "    , '製造方法': \"cat_production_headline\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, None, 2 52975       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     10200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    1172300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 3)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 153)    0           time_distributed_1[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 153)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 200)    203200      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 4)      404         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      44          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,459,223\n",
      "Trainable params: 1,459,223\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "headline_input = Input(shape=(None, 3))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb, headline_input])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "\n",
    "feats = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats = Dense(param['tag_size'])(feats)\n",
    "\n",
    "crf = CRF(param['tag_size'])\n",
    "pred = crf(feats)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input, headline_input], outputs=pred)\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=crf.loss_function, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr = \"原材料\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_onehot(tag_seq, tokenizer):\n",
    "    return np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_train = tokenizer.transform_word(train_df.repl_words.tolist())\n",
    "x_char_train = tokenizer.transform_char(train_df.words.tolist())\n",
    "x_headline_train = pd.get_dummies(train_df[target_headline_name[target_attr]]).values\n",
    "y_train = tokenizer.transform_tag(train_df[target_col_name[target_attr]].tolist())\n",
    "\n",
    "x_word_test = tokenizer.transform_word(test_df.repl_words.tolist())\n",
    "x_char_test = tokenizer.transform_char(test_df.words.tolist())\n",
    "x_headline_test = pd.get_dummies(test_df[target_headline_name[target_attr]]).values\n",
    "y_test = tokenizer.transform_tag(test_df[target_col_name[target_attr]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y_train = encoding_onehot(y_train, tokenizer)\n",
    "y_test = encoding_onehot(y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train, x_headline_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test, x_headline_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 7.5378\n",
      "Epoch 2/50\n",
      "59/59 [==============================] - 35s 585ms/step - loss: 6.7077\n",
      "Epoch 3/50\n",
      "59/59 [==============================] - 36s 606ms/step - loss: 6.9977\n",
      "Epoch 4/50\n",
      "59/59 [==============================] - 37s 629ms/step - loss: 7.3254\n",
      "Epoch 5/50\n",
      "59/59 [==============================] - 36s 609ms/step - loss: 7.0610\n",
      "Epoch 6/50\n",
      "59/59 [==============================] - 37s 629ms/step - loss: 7.4737\n",
      "Epoch 7/50\n",
      "59/59 [==============================] - 34s 584ms/step - loss: 6.6747\n",
      "Epoch 8/50\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 7.1262\n",
      "Epoch 9/50\n",
      "59/59 [==============================] - 42s 710ms/step - loss: 7.4239\n",
      "Epoch 10/50\n",
      "59/59 [==============================] - 39s 654ms/step - loss: 6.7546\n",
      "Epoch 11/50\n",
      "59/59 [==============================] - 43s 733ms/step - loss: 7.7853\n",
      "Epoch 12/50\n",
      "59/59 [==============================] - 37s 628ms/step - loss: 6.3820\n",
      "Epoch 13/50\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 7.1347\n",
      "Epoch 14/50\n",
      "59/59 [==============================] - 40s 683ms/step - loss: 7.2429\n",
      "Epoch 15/50\n",
      "59/59 [==============================] - 40s 682ms/step - loss: 7.1401\n",
      "Epoch 16/50\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 6.7481\n",
      "Epoch 17/50\n",
      "59/59 [==============================] - 41s 698ms/step - loss: 7.2979\n",
      "Epoch 18/50\n",
      "59/59 [==============================] - 38s 650ms/step - loss: 6.6279\n",
      "Epoch 19/50\n",
      "59/59 [==============================] - 43s 724ms/step - loss: 7.7686\n",
      "Epoch 20/50\n",
      "59/59 [==============================] - 35s 601ms/step - loss: 6.0626\n",
      "Epoch 21/50\n",
      "59/59 [==============================] - 40s 680ms/step - loss: 7.0251\n",
      "Epoch 22/50\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 7.0864\n",
      "Epoch 23/50\n",
      "59/59 [==============================] - 41s 693ms/step - loss: 7.2734\n",
      "Epoch 24/50\n",
      "59/59 [==============================] - 39s 655ms/step - loss: 6.7696\n",
      "Epoch 25/50\n",
      "59/59 [==============================] - 42s 715ms/step - loss: 7.6267\n",
      "Epoch 26/50\n",
      "59/59 [==============================] - 40s 685ms/step - loss: 7.2026\n",
      "Epoch 27/50\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 6.7628\n",
      "Epoch 28/50\n",
      "59/59 [==============================] - 43s 727ms/step - loss: 7.7670\n",
      "Epoch 29/50\n",
      "59/59 [==============================] - 36s 616ms/step - loss: 6.1259\n",
      "Epoch 30/50\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 7.1084\n",
      "Epoch 31/50\n",
      "59/59 [==============================] - 43s 722ms/step - loss: 7.6316\n",
      "Epoch 32/50\n",
      "59/59 [==============================] - 37s 623ms/step - loss: 6.2743\n",
      "Epoch 33/50\n",
      "59/59 [==============================] - 40s 683ms/step - loss: 7.0582\n",
      "Epoch 34/50\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 6.8168\n",
      "Epoch 35/50\n",
      "59/59 [==============================] - 42s 715ms/step - loss: 7.4740\n",
      "Epoch 36/50\n",
      "59/59 [==============================] - 39s 654ms/step - loss: 6.7731\n",
      "Epoch 37/50\n",
      "59/59 [==============================] - 43s 727ms/step - loss: 7.7808\n",
      "Epoch 38/50\n",
      "59/59 [==============================] - 38s 638ms/step - loss: 6.4764\n",
      "Epoch 39/50\n",
      "59/59 [==============================] - 41s 696ms/step - loss: 7.3868\n",
      "Epoch 40/50\n",
      "59/59 [==============================] - 41s 688ms/step - loss: 7.3349\n",
      "Epoch 41/50\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 6.8890\n",
      "Epoch 42/50\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 6.9082\n",
      "Epoch 43/50\n",
      "59/59 [==============================] - 42s 712ms/step - loss: 7.4069\n",
      "Epoch 44/50\n",
      "59/59 [==============================] - 43s 723ms/step - loss: 7.6174\n",
      "Epoch 45/50\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 6.4946\n",
      "Epoch 46/50\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 7.0065\n",
      "Epoch 47/50\n",
      "59/59 [==============================] - 40s 682ms/step - loss: 7.5356\n",
      "Epoch 48/50\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 6.5031\n",
      "Epoch 49/50\n",
      "59/59 [==============================] - 37s 635ms/step - loss: 7.2260\n",
      "Epoch 50/50\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 7.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b68d33748>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=50\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/raw-material_with_repl-compound_headline.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_x_word_test = tokenizer.padding_word(x_word_test)\n",
    "pad_x_char_test = tokenizer.padding_char(x_char_test)\n",
    "pad_x_headline_test = np.array([[x_headline_test[i]] * len(x) for i, x in enumerate(pad_x_word_test)])\n",
    "pad_y_test = tokenizer.padding_tag(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test, pad_x_headline_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): {'title': entry['Name'], 'attributes': entry['Attributes']} for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []\n",
    "\n",
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s\n",
    "\n",
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt\n",
    "\n",
    "def evaluate_exact_match(result_dict):\n",
    "    annotation_size = 0\n",
    "    extracted_size = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for _id, val in result_dict.items():\n",
    "        true_set = set(val['true'])\n",
    "        pred_set = set(val['predict'])\n",
    "        \n",
    "        annotation_size += len(true_set)\n",
    "        extracted_size += len(pred_set)\n",
    "        TP += len(true_set & pred_set)\n",
    "        FP += len(pred_set - true_set)\n",
    "        FN += len(true_set - pred_set)\n",
    "\n",
    "    def precision(TP, FP):\n",
    "            return TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "    def recall(TP, FN):\n",
    "        return TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "    def f1(precision, recall):\n",
    "        return 2 * precision * recall / (precision + recall) \\\n",
    "            if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    score = {\n",
    "        'annotation_size': annotation_size\n",
    "        , 'extracted_size': extracted_size\n",
    "        , 'TP': TP\n",
    "        , 'FP': FP\n",
    "        , 'FN': FN\n",
    "        , 'precision': precision(TP, FP)\n",
    "        , 'recall': recall(TP, FN)\n",
    "        , 'f1': f1(precision(TP, FP), recall(TP, FN))\n",
    "    }\n",
    "    \n",
    "    return score\n",
    "\n",
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]\n",
    "\n",
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.2790014684287812,\n",
       " 'precision': 0.29780564263322884,\n",
       " 'recall': 0.26243093922651933}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq(pad_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(y_pred))\n",
    "\n",
    "extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    \n",
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1024773': {'predict': ['トルエン',\n",
       "   'ペンタセン',\n",
       "   'ペンタセンキノン',\n",
       "   'フタルアルデヒド',\n",
       "   'ベンゼン',\n",
       "   'アルミニウム',\n",
       "   'アセン',\n",
       "   '一酸化炭素',\n",
       "   '-6',\n",
       "   'シクロヘキサン-1,4-ジオン'],\n",
       "  'title': 'ペンタセン',\n",
       "  'true': ['シクロヘキサン-1,4-ジオン',\n",
       "   'フタルアルデヒド',\n",
       "   'アルミニウムアマルガム',\n",
       "   'ペンタセンキノン',\n",
       "   '水',\n",
       "   'エチレン',\n",
       "   'テトラハロゲノベンゼン']},\n",
       " '1050477': {'predict': ['エステラーゼ', 'モルヒネ'], 'title': 'レミフェンタニル', 'true': []},\n",
       " '1067475': {'predict': ['レソルシノール',\n",
       "   '塩化亜鉛',\n",
       "   'オレゴングリーン',\n",
       "   '無水フタル酸',\n",
       "   'カルボキシフルオレセイン',\n",
       "   'フルオレセイン',\n",
       "   'イソチオシアネート'],\n",
       "  'title': 'フルオレセイン',\n",
       "  'true': ['レソルシノール', '塩化亜鉛', '無水フタル酸', 'スルホン']},\n",
       " '1095173': {'predict': ['サバ',\n",
       "   'メトヘモグロビン',\n",
       "   '金属',\n",
       "   'シアン化水素ガス',\n",
       "   '糖',\n",
       "   '亜硝酸',\n",
       "   'シアン化銅',\n",
       "   'シアノヒドリン',\n",
       "   '酸素',\n",
       "   'シアン化水素',\n",
       "   'ハロゲン化物',\n",
       "   '植物の',\n",
       "   '食物',\n",
       "   'シアン',\n",
       "   'アクリロニトリル',\n",
       "   'Fe3',\n",
       "   '亜硝酸アミル',\n",
       "   'シアン化物'],\n",
       "  'title': 'シアン化物',\n",
       "  'true': []},\n",
       " '1140453': {'predict': [], 'title': 'ピパンペロン', 'true': []},\n",
       " '1148105': {'predict': [], 'title': 'グラニセトロン', 'true': []},\n",
       " '1193389': {'predict': ['塩素ガス',\n",
       "   '塩化カリウム',\n",
       "   '濃塩酸',\n",
       "   '(III',\n",
       "   '塩化水素',\n",
       "   'ヘキサクロロロジウム酸(III)カリウム',\n",
       "   '硝酸銀',\n",
       "   'ロジウム(III)',\n",
       "   '水',\n",
       "   '水酸化カリウム'],\n",
       "  'title': '塩化ロジウム(III)',\n",
       "  'true': ['塩化水素',\n",
       "   'ロジウム',\n",
       "   'ヘキサクロロロジウム酸(III)カリウム',\n",
       "   '水酸化ロジウム(III)',\n",
       "   '水酸化カリウム',\n",
       "   '塩化カリウム',\n",
       "   '濃塩酸',\n",
       "   '塩化ロジウム三水和物',\n",
       "   '塩素ガス']},\n",
       " '1198019': {'predict': ['希酸',\n",
       "   'アセトン',\n",
       "   'シトラール',\n",
       "   '塩基触媒',\n",
       "   '三フッ化ホウ素',\n",
       "   'アルドール',\n",
       "   'ヒト'],\n",
       "  'title': 'イオノン',\n",
       "  'true': ['プソイドイオノン',\n",
       "   '植物の精油',\n",
       "   '硫酸',\n",
       "   '三フッ化ホウ素',\n",
       "   'リン酸',\n",
       "   '希酸',\n",
       "   'シトラール',\n",
       "   'アセトン']},\n",
       " '1203950': {'predict': ['メグルミン'], 'title': 'フルニキシン', 'true': []},\n",
       " '12437': {'predict': ['クリーンエア・アクト',\n",
       "   'ナトリウムエトキシド',\n",
       "   'メタノール',\n",
       "   '酢酸',\n",
       "   'ホルムアルデヒド',\n",
       "   'エチレン',\n",
       "   '酵素',\n",
       "   'エチレングリコール',\n",
       "   '水',\n",
       "   'アセトアルデヒド',\n",
       "   '濃硫酸',\n",
       "   '石油',\n",
       "   'アルコール'],\n",
       "  'title': 'エタノール',\n",
       "  'true': ['アルコール', 'エチレン']},\n",
       " '1273537': {'predict': ['ヘキサメチレンジアミン'],\n",
       "  'title': '塩化アジポイル',\n",
       "  'true': ['アジピン酸']},\n",
       " '1372856': {'predict': [], 'title': 'チオ酢酸', 'true': []},\n",
       " '1383346': {'predict': [], 'title': '一酸化銀', 'true': []},\n",
       " '1454192': {'predict': [], 'title': 'テトラヒドロゾリン', 'true': []},\n",
       " '1483298': {'predict': ['水素', 'アンモニア', '酢酸', 'アダムス'],\n",
       "  'title': 'アダムス触媒',\n",
       "  'true': ['塩化白金酸アンモニウム', '硝酸ナトリウム', '塩化白金酸']},\n",
       " '1540694': {'predict': [], 'title': '窒化ベリリウム', 'true': []},\n",
       " '1560938': {'predict': ['オンダンセトロン'], 'title': 'オンダンセトロン', 'true': []},\n",
       " '1563432': {'predict': [], 'title': 'リゾホスファチジン酸', 'true': ['リゾホスファチジルコリン']},\n",
       " '1573835': {'predict': [], 'title': '安息香酸カルシウム', 'true': []},\n",
       " '1591322': {'predict': [],\n",
       "  'title': 'デキストラン',\n",
       "  'true': ['Leuconostoc mesenteroides が生産する高分子デキストラン', '高分子デキストラン', 'スクロース']},\n",
       " '160786': {'predict': ['フェニルカルビノール', 'アセトフェノン', 'トルエン', '塩基', 'メタノール'],\n",
       "  'title': 'スチレン',\n",
       "  'true': ['エチルベンゼン', 'ケイ皮酸脱炭酸酵素', 'ケイ皮酸']},\n",
       " '1616237': {'predict': ['ビグアニド', 'グアニジン'], 'title': 'フェンホルミン', 'true': []},\n",
       " '1625510': {'predict': ['塩基', 'クロロギ酸エチル', 'アントラニル酸', 'ホスゲン'],\n",
       "  'title': 'イサト酸無水物',\n",
       "  'true': ['アントラニル酸ナトリウム', 'ホスゲン', 'アントラニル酸', 'クロロギ酸エチル', 'ベンズイソオキサゾール']},\n",
       " '1658717': {'predict': ['五フッ化ヨウ素', 'フッ素', 'ヨウ化カリウム'],\n",
       "  'title': '七フッ化ヨウ素',\n",
       "  'true': ['五フッ化ヨウ素', 'フッ素']},\n",
       " '1663779': {'predict': ['四級', 'アンモニア'],\n",
       "  'title': 'ベンジルアミン',\n",
       "  'true': ['ベンゾニトリル', '水素']},\n",
       " '1690387': {'predict': ['水銀', '水銀(', 'シアン'],\n",
       "  'title': 'シアン化水銀(II)',\n",
       "  'true': ['シアン', '水銀']},\n",
       " '1787749': {'predict': ['ホスファチジルイノシトール', 'ビスリン酸', 'ジアシルグリセロール', 'ホスホリパーゼ'],\n",
       "  'title': 'イノシトールトリスリン酸',\n",
       "  'true': ['ホスファチジルイノシトール4,5-ビスリン酸']},\n",
       " '1793432': {'predict': [], 'title': 'ヨウ化亜鉛', 'true': []},\n",
       " '1815995': {'predict': [], 'title': 'フッ化亜鉛', 'true': []},\n",
       " '1866600': {'predict': ['メチルエチルケトン', '水素イオン', '亜硝酸エステル', 'ジアセチル', '水素'],\n",
       "  'title': 'ジメチルグリオキシム',\n",
       "  'true': ['オキシム', 'ヒドロキシルアミン硫酸ナトリウム', 'メチルエチルケトン', '亜硝酸エステル']},\n",
       " '189601': {'predict': ['菌', 'シガテラ'],\n",
       "  'title': 'シガトキシン',\n",
       "  'true': ['藻類', '有毒渦鞭毛藻', 'ポリケチド']},\n",
       " '1905810': {'predict': ['塩基性塩Be2CO3', '二酸化炭素', '四水和物', '水酸化ベリリウム', 'Be'],\n",
       "  'title': '炭酸ベリリウム',\n",
       "  'true': ['二酸化炭素', '水', '水酸化ベリリウム']},\n",
       " '1905814': {'predict': ['ベリリウム塩'],\n",
       "  'title': '水酸化ベリリウム',\n",
       "  'true': ['水酸化ナトリウム', 'ベリリウム塩', 'アンモニア 水']},\n",
       " '1932135': {'predict': [], 'title': 'トロパン', 'true': ['オルニチン']},\n",
       " '1936318': {'predict': [],\n",
       "  'title': 'スワインソニン',\n",
       "  'true': ['菌',\n",
       "   'メタリジウム',\n",
       "   'Astragalus lentiginosus',\n",
       "   'Swainosona canescens (Benth.) A Lee',\n",
       "   'Rhizoctonia leguminicola',\n",
       "   '被子植物']},\n",
       " '1939275': {'predict': [], 'title': 'パントイン酸', 'true': []},\n",
       " '19566': {'predict': ['金属イオン',\n",
       "   'シアン化ナトリウム',\n",
       "   'アミグダリン',\n",
       "   'ジシアン',\n",
       "   '細菌',\n",
       "   'アンモニア',\n",
       "   'シアン化物',\n",
       "   '酸',\n",
       "   'シアンメトヘモグロビン',\n",
       "   'シアン',\n",
       "   '金',\n",
       "   '窒素',\n",
       "   'Fe3',\n",
       "   'アクリロニトリル',\n",
       "   'メトヘモグロビン'],\n",
       "  'title': 'シアン化水素',\n",
       "  'true': ['空気', 'アンモニア', 'メタン', 'シアン化ナトリウム', '酸']},\n",
       " '1977270': {'predict': ['アンチモン'],\n",
       "  'title': '三フッ化アンチモン',\n",
       "  'true': ['三酸化アンチモン', 'フッ化水素']},\n",
       " '1980911': {'predict': ['トリフェニルホスフィン', 'フェニルナトリウム', '1,3-ジエン'],\n",
       "  'title': 'ホスホール',\n",
       "  'true': []},\n",
       " '2061207': {'predict': [], 'title': 'ヘキサフルオロプロペン', 'true': []},\n",
       " '2080116': {'predict': [], 'title': 'カナジン', 'true': []},\n",
       " '2137257': {'predict': ['鉄', '亜硝酸アミル', 'シアン', '亜硝酸', 'メトヘモグロビン'],\n",
       "  'title': 'メトヘモグロビン',\n",
       "  'true': []},\n",
       " '2140608': {'predict': [], 'title': 'プロトカテク酸', 'true': []},\n",
       " '2153576': {'predict': ['シタグリプチン', 'トリグリセリド'], 'title': 'リラグルチド', 'true': []},\n",
       " '2170531': {'predict': [], 'title': 'スルファメトキサゾール', 'true': []},\n",
       " '217088': {'predict': [], 'title': 'エナラプリル', 'true': []},\n",
       " '2190946': {'predict': [], 'title': 'フェニルアセトン', 'true': []},\n",
       " '2211690': {'predict': [], 'title': 'パクロブトラゾール', 'true': []},\n",
       " '2234076': {'predict': ['水素', 'アンモニア'],\n",
       "  'title': 'イソブチルアミン',\n",
       "  'true': ['水素', 'イソプロパノール', 'アンモニア']},\n",
       " '2312494': {'predict': [], 'title': 'リンモリブデン酸', 'true': []},\n",
       " '2334030': {'predict': ['フィリピンIII'],\n",
       "  'title': 'フィリピン (化合物)',\n",
       "  'true': ['放線菌Streptomyces filipinensisの菌糸体および培養濾液']},\n",
       " '2346795': {'predict': ['ジアゾメタン', 'Os3', 'CO', 'Os', 'オクタン溶液', '金属', '活性'],\n",
       "  'title': 'デカカルボニルジヒドリド三オスミウム',\n",
       "  'true': []},\n",
       " '2488348': {'predict': ['N-アセチルシステイン'], 'title': 'オルトフタルアルデヒド', 'true': []},\n",
       " '248891': {'predict': [], 'title': 'マイトトキシン', 'true': ['ポリエーテル', '硫酸']},\n",
       " '2569592': {'predict': ['3-ジメチルアミノフェノール', '塩化銀', 'ブロモエタン', '臭素'],\n",
       "  'title': 'エドロホニウム',\n",
       "  'true': ['塩化銀', 'エドロホニウム臭化物', 'ブロモエタン', '3-ジメチルアミノフェノール']},\n",
       " '2614499': {'predict': [], 'title': '2-シクロプロペンカルボン酸', 'true': []},\n",
       " '2727655': {'predict': [], 'title': 'リン化コバルト(II)', 'true': []},\n",
       " '2790003': {'predict': ['アジリジン', 'TEPA', \"',N''- トリエチレンホスホルアミド\"],\n",
       "  'title': 'チオテパ',\n",
       "  'true': ['アジリジン', '塩化チオホスホリル']},\n",
       " '2795973': {'predict': ['ジヒドロゲラニオール', 'ゲラニオール', 'シトロネラソウ', '天然', 'シトロネロール'],\n",
       "  'title': 'シトロネロール',\n",
       "  'true': ['ゲラニオール']},\n",
       " '2806294': {'predict': ['パクリタキセル'], 'title': 'パチョロール', 'true': []},\n",
       " '2807214': {'predict': ['ホズロシド'], 'title': 'ホズルシン', 'true': []},\n",
       " '2823300': {'predict': ['神経'], 'title': 'ベラトリジン', 'true': []},\n",
       " '293373': {'predict': ['ミネソタ'], 'title': 'フェンタニル', 'true': []},\n",
       " '29891': {'predict': ['エーテル',\n",
       "   '酸',\n",
       "   'ポリプロピレングリコール',\n",
       "   'セレニド',\n",
       "   'ハロゲン化合物',\n",
       "   '酸素',\n",
       "   'ポリエチレングリコール',\n",
       "   'アルコール'],\n",
       "  'title': 'エーテル (化学)',\n",
       "  'true': ['オレフィン', '求電子剤', '有機ハロゲン化合物', 'アルコキシド', 'アルコール']},\n",
       " '2996837': {'predict': ['固体のメルドラム酸', 'エチレンテトラカルボン酸'],\n",
       "  'title': 'エチレンテトラカルボン酸二無水物',\n",
       "  'true': ['エチレンテトラカルボン酸', 'メルドラム酸']},\n",
       " '3020464': {'predict': [], 'title': 'オクテン', 'true': []},\n",
       " '305316': {'predict': ['水和物', 'グリセリン', '酸化カルシウム', '硫酸', '水', 'ギプス'],\n",
       "  'title': '硫酸カルシウム',\n",
       "  'true': ['カルシウム塩の水溶液', '希硫酸', '硫酸塩水溶液', '水酸化カルシウム', '硫酸']},\n",
       " '31121': {'predict': ['水', 'カリフォルニア', 'サンタ'], 'title': 'DHMO', 'true': []},\n",
       " '3273180': {'predict': [], 'title': 'フルフェナム酸', 'true': []},\n",
       " '3283637': {'predict': ['クリオキノール'], 'title': 'クリオキノール', 'true': []},\n",
       " '3342427': {'predict': ['ケリドン酸', '4-ピロン-2,6-ジカルボン酸', 'アンモニア'],\n",
       "  'title': 'ケリダム酸',\n",
       "  'true': ['4-ピロン-2,6-ジカルボン酸', 'アンモニア', 'ケリドン酸']},\n",
       " '3352098': {'predict': ['酸化ウラン(VI)',\n",
       "   '塩化ウラン(IV)',\n",
       "   '塩素',\n",
       "   'HF',\n",
       "   '塩化ウラン(V)',\n",
       "   '四塩化炭素'],\n",
       "  'title': '塩化ウラン(VI)',\n",
       "  'true': ['酸化ウラン(VI)', '塩素', '四塩化炭素', 'UO3']},\n",
       " '3419435': {'predict': [], 'title': 'アゼルニジピン', 'true': []},\n",
       " '3448226': {'predict': [], 'title': 'マンノサミン', 'true': []},\n",
       " '3460340': {'predict': [], 'title': 'ネビラピン', 'true': []},\n",
       " '3463386': {'predict': ['ヨウ素'],\n",
       "  'title': 'トリフルオロメタンスルホン酸銀',\n",
       "  'true': ['トリフラート', '銀']},\n",
       " '3472884': {'predict': ['MOCVD', '金属'], 'title': 'REBCO', 'true': []},\n",
       " '3474570': {'predict': [],\n",
       "  'title': 'オルニチンα‐ケトグルタル酸',\n",
       "  'true': ['オルニチン', 'α-ケトグルタル酸']},\n",
       " '3483975': {'predict': [],\n",
       "  'title': '二塩化ゲルマニウム',\n",
       "  'true': ['四塩化ゲルマニウム', 'クロロゲルマン', 'GeCl4', 'ゲルマニウム金属']},\n",
       " '3519068': {'predict': ['ジイソプロピルアミン', 'アンモニア'],\n",
       "  'title': 'トリイソプロピルアミン',\n",
       "  'true': ['ジイソプロピルアミン']},\n",
       " '3526170': {'predict': [], 'title': 'リン酸三カルシウム', 'true': ['リン酸', 'カルシウム']},\n",
       " '3529078': {'predict': [], 'title': 'ビベンジル', 'true': ['フェニル基', 'エタン']},\n",
       " '3560674': {'predict': [], 'title': '2-モルホリノエタノール', 'true': []},\n",
       " '3570913': {'predict': ['神経剤', 'メタン'],\n",
       "  'title': 'メチルホスホン酸ジメチル',\n",
       "  'true': ['亜リン酸トリメチル', 'ハロメタン', 'ヨードメタン']},\n",
       " '3683375': {'predict': [],\n",
       "  'title': 'ミトコンドリアフェリチン',\n",
       "  'true': ['βシート', 'ヘリックス', 'αヘリックス']},\n",
       " '41685': {'predict': ['カルボキシル基', 'グルタミン酸'], 'title': 'Γ-アミノ酪酸', 'true': []},\n",
       " '497499': {'predict': ['メチルアミン'],\n",
       "  'title': 'N-メチルピロリドン',\n",
       "  'true': ['γ-ブチロラクトン', 'メチルアミン']},\n",
       " '514541': {'predict': ['水銀', '水銀(I)塩の水溶液', '酸化水銀(II)'],\n",
       "  'title': '酸化水銀',\n",
       "  'true': []},\n",
       " '520951': {'predict': ['アンモニア'],\n",
       "  'title': 'ピペラジン',\n",
       "  'true': ['水酸化ナトリウム', '1,2-ジクロロエタン', 'アンモニア']},\n",
       " '523020': {'predict': ['酸素', '水素化ホウ素ナトリウム', '硫黄'],\n",
       "  'title': 'チオアミド',\n",
       "  'true': ['アミド', '五硫化二リン', '硫黄', 'P2S5', 'ローソン試薬']},\n",
       " '616186': {'predict': ['マンデロニトリル', 'エムルシン', 'ベンズアルデヒド', 'アミグダリン', 'シアン化水素'],\n",
       "  'title': 'アミグダリン',\n",
       "  'true': []},\n",
       " '62444': {'predict': ['アンモニア', '物', 'アイラ・レムセン', 'ファールバーグ', 'アントラニル酸', '亜硝酸'],\n",
       "  'title': 'サッカリン',\n",
       "  'true': ['塩素', 'アントラニル酸', '二酸化硫黄', 'トルエン', 'アンモニア', '亜硝酸', '2-クロロトルエン']},\n",
       " '62464': {'predict': ['アンモニア',\n",
       "   'シアン酸アンモニウム',\n",
       "   '尿素水',\n",
       "   '窒素',\n",
       "   '水',\n",
       "   'ヒト',\n",
       "   '硝酸アンモニウム'],\n",
       "  'title': '尿素',\n",
       "  'true': ['シアン酸アンモニウム', 'アンモニア', '窒素']},\n",
       " '704021': {'predict': ['1,5-ヘキサジイン',\n",
       "   '1,3-ブタジエン',\n",
       "   '3-メトキシ-',\n",
       "   '-ブチルリチウム',\n",
       "   'マレイン酸ジエチル',\n",
       "   'コバルト',\n",
       "   'トリメチルシリロキシ',\n",
       "   '-（トリブチルスタンニル）プロピン'],\n",
       "  'title': 'ロケッテン',\n",
       "  'true': ['n -ブチルリチウム',\n",
       "   '1,3-ブタジエン',\n",
       "   'シクロブタベンゼン誘導体',\n",
       "   'マレイン酸ジエチル',\n",
       "   '3-（トリメチルシリロキシ）-1-（トリブチルスタンニル）プロピン',\n",
       "   '1,5-ヘキサジイン',\n",
       "   '3-メトキシ-1-トリメチルシリルプロピン',\n",
       "   'コバルト錯体']},\n",
       " '778300': {'predict': [], 'title': 'Ε-アミノカプロン酸', 'true': []},\n",
       " '884244': {'predict': ['二クロム酸'], 'title': 'クロム酸', 'true': []},\n",
       " '954930': {'predict': [], 'title': 'ククルビタシン', 'true': []},\n",
       " '969196': {'predict': ['グルコノ'], 'title': 'グルコノラクトン', 'true': ['グルコース']}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FN': 98,\n",
       " 'FP': 175,\n",
       " 'TP': 62,\n",
       " 'annotation_size': 160,\n",
       " 'extracted_size': 237,\n",
       " 'f1': 0.3123425692695214,\n",
       " 'precision': 0.2616033755274262,\n",
       " 'recall': 0.3875}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_exact_match(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/raw-material_with_repl-compounds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
