{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(X[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(X[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_X = [np.array(_input)[shuffle_indices] for _input in X]\n",
    "                shuffled_y = [np.array(target)[shuffle_indices] for target in y]\n",
    "            else:\n",
    "                shuffled_data = X\n",
    "                shuffled_labels = y\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                batch_X = [_input[start_index: end_index] for _input in shuffled_X]\n",
    "                batch_y = [target[start_index: end_index] for target in shuffled_y]\n",
    "                \n",
    "                batch_X[0] = tokenizer.padding_word(batch_X[0])\n",
    "                batch_X[1] = tokenizer.padding_char(batch_X[1])\n",
    "                batch_y = [tokenizer.padding_tag(attr_y) for attr_y in batch_y]\n",
    "                \n",
    "                yield batch_X, batch_y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, None, 2 52975       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     10200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    1172300     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 150)    0           time_distributed_1[0][0]         \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 150)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 200)    200800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 100)    20100       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 4)      404         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 4)      404         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, None, 4)      44          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, None, 4)      44          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,477,371\n",
      "Trainable params: 1,477,371\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "feats1 = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats1 = Dense(param['tag_size'])(feats1)\n",
    "crf1 = CRF(param['tag_size'])\n",
    "pred1 = crf1(feats1)\n",
    "\n",
    "feats2 = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats2 = Dense(param['tag_size'])(feats2)\n",
    "crf2 = CRF(param['tag_size'])\n",
    "pred2 = crf2(feats2)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=[pred1, pred2])\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=[crf1.loss_function, crf2.loss_function], optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr_list = [\"原材料\", \"製造方法\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_onehot(tag_seq, tokenizer):\n",
    "    return np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_train = tokenizer.transform_word(train_df.repl_words.tolist())\n",
    "x_char_train = tokenizer.transform_char(train_df.words.tolist())\n",
    "y_train = [tokenizer.transform_tag(train_df[target_col_name[attr]].tolist()) for attr in target_attr_list]\n",
    "# one-hot encoding\n",
    "y_train = np.array([encoding_onehot(tag_seq, tokenizer) for tag_seq in y_train])\n",
    "\n",
    "x_word_test = tokenizer.transform_word(test_df.repl_words.tolist())\n",
    "x_char_test = tokenizer.transform_char(test_df.words.tolist())\n",
    "y_test = [tokenizer.transform_tag(test_df[target_col_name[attr]].tolist()) for attr in target_attr_list]\n",
    "# one-hot encoding\n",
    "y_test = np.array([encoding_onehot(tag_seq, tokenizer) for tag_seq in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "59/59 [==============================] - 46s 784ms/step - loss: 15.9324 - crf_1_loss: 7.7635 - crf_2_loss: 8.1689\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 39s 657ms/step - loss: 14.4167 - crf_1_loss: 7.1559 - crf_2_loss: 7.2608\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 38s 650ms/step - loss: 13.8080 - crf_1_loss: 6.8729 - crf_2_loss: 6.9351\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 14.6615 - crf_1_loss: 7.3166 - crf_2_loss: 7.3448\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.4601 - crf_1_loss: 7.2216 - crf_2_loss: 7.2385\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 40s 674ms/step - loss: 14.2687 - crf_1_loss: 7.1297 - crf_2_loss: 7.1391\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 39s 665ms/step - loss: 14.2410 - crf_1_loss: 7.1170 - crf_2_loss: 7.1239\n",
      "Epoch 8/100\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 14.4879 - crf_1_loss: 7.2428 - crf_2_loss: 7.2451\n",
      "Epoch 9/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.2947 - crf_1_loss: 7.1463 - crf_2_loss: 7.1484\n",
      "Epoch 10/100\n",
      "59/59 [==============================] - 40s 684ms/step - loss: 14.6098 - crf_1_loss: 7.3054 - crf_2_loss: 7.3044\n",
      "Epoch 11/100\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 14.3653 - crf_1_loss: 7.1844 - crf_2_loss: 7.1809\n",
      "Epoch 12/100\n",
      "59/59 [==============================] - 39s 655ms/step - loss: 13.7937 - crf_1_loss: 6.8981 - crf_2_loss: 6.8956\n",
      "Epoch 13/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.2401 - crf_1_loss: 7.1211 - crf_2_loss: 7.1190\n",
      "Epoch 14/100\n",
      "59/59 [==============================] - 38s 644ms/step - loss: 13.7662 - crf_1_loss: 6.8854 - crf_2_loss: 6.8808\n",
      "Epoch 15/100\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 14.1718 - crf_1_loss: 7.0873 - crf_2_loss: 7.0845\n",
      "Epoch 16/100\n",
      "59/59 [==============================] - 39s 659ms/step - loss: 13.9548 - crf_1_loss: 6.9786 - crf_2_loss: 6.9762\n",
      "Epoch 17/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.1410 - crf_1_loss: 7.0716 - crf_2_loss: 7.0695\n",
      "Epoch 18/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 14.1792 - crf_1_loss: 7.0909 - crf_2_loss: 7.0884\n",
      "Epoch 19/100\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 14.4912 - crf_1_loss: 7.2470 - crf_2_loss: 7.2442\n",
      "Epoch 20/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.5098 - crf_1_loss: 7.2562 - crf_2_loss: 7.2536\n",
      "Epoch 21/100\n",
      "59/59 [==============================] - 39s 657ms/step - loss: 13.8068 - crf_1_loss: 6.9047 - crf_2_loss: 6.9022\n",
      "Epoch 22/100\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 14.4535 - crf_1_loss: 7.2276 - crf_2_loss: 7.2259\n",
      "Epoch 23/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.2881 - crf_1_loss: 7.1450 - crf_2_loss: 7.1431\n",
      "Epoch 24/100\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 14.6482 - crf_1_loss: 7.3250 - crf_2_loss: 7.3232\n",
      "Epoch 25/100\n",
      "59/59 [==============================] - 39s 666ms/step - loss: 14.1577 - crf_1_loss: 7.0799 - crf_2_loss: 7.0778\n",
      "Epoch 26/100\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 14.2055 - crf_1_loss: 7.1033 - crf_2_loss: 7.1022\n",
      "Epoch 27/100\n",
      "59/59 [==============================] - 40s 674ms/step - loss: 14.2657 - crf_1_loss: 7.1334 - crf_2_loss: 7.1323\n",
      "Epoch 28/100\n",
      "59/59 [==============================] - 39s 662ms/step - loss: 14.1245 - crf_1_loss: 7.0628 - crf_2_loss: 7.0617\n",
      "Epoch 29/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.5527 - crf_1_loss: 7.2773 - crf_2_loss: 7.2755\n",
      "Epoch 30/100\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 13.3320 - crf_1_loss: 6.6663 - crf_2_loss: 6.6657\n",
      "Epoch 31/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.4052 - crf_1_loss: 7.2032 - crf_2_loss: 7.2020\n",
      "Epoch 32/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 14.1218 - crf_1_loss: 7.0615 - crf_2_loss: 7.0603\n",
      "Epoch 33/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.2248 - crf_1_loss: 7.1136 - crf_2_loss: 7.1112\n",
      "Epoch 34/100\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 14.5439 - crf_1_loss: 7.2721 - crf_2_loss: 7.2718\n",
      "Epoch 35/100\n",
      "59/59 [==============================] - 39s 669ms/step - loss: 14.5537 - crf_1_loss: 7.2774 - crf_2_loss: 7.2763\n",
      "Epoch 36/100\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 14.4718 - crf_1_loss: 7.2366 - crf_2_loss: 7.2352\n",
      "Epoch 37/100\n",
      "59/59 [==============================] - 39s 654ms/step - loss: 13.7084 - crf_1_loss: 6.8550 - crf_2_loss: 6.8535\n",
      "Epoch 38/100\n",
      "59/59 [==============================] - 39s 662ms/step - loss: 14.1184 - crf_1_loss: 7.0599 - crf_2_loss: 7.0585\n",
      "Epoch 39/100\n",
      "59/59 [==============================] - 38s 642ms/step - loss: 13.4179 - crf_1_loss: 6.7095 - crf_2_loss: 6.7084\n",
      "Epoch 40/100\n",
      "59/59 [==============================] - 40s 684ms/step - loss: 14.6710 - crf_1_loss: 7.3357 - crf_2_loss: 7.3352\n",
      "Epoch 41/100\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 14.4864 - crf_1_loss: 7.2431 - crf_2_loss: 7.2433\n",
      "Epoch 42/100\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 14.3313 - crf_1_loss: 7.1657 - crf_2_loss: 7.1657\n",
      "Epoch 43/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 13.9841 - crf_1_loss: 6.9925 - crf_2_loss: 6.9916\n",
      "Epoch 44/100\n",
      "59/59 [==============================] - 39s 665ms/step - loss: 14.1827 - crf_1_loss: 7.0924 - crf_2_loss: 7.0904\n",
      "Epoch 45/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.2955 - crf_1_loss: 7.1483 - crf_2_loss: 7.1472\n",
      "Epoch 46/100\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 14.1848 - crf_1_loss: 7.0925 - crf_2_loss: 7.0922\n",
      "Epoch 47/100\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 14.0959 - crf_1_loss: 7.0481 - crf_2_loss: 7.0478\n",
      "Epoch 48/100\n",
      "59/59 [==============================] - 40s 676ms/step - loss: 14.3286 - crf_1_loss: 7.1648 - crf_2_loss: 7.1638\n",
      "Epoch 49/100\n",
      "59/59 [==============================] - 39s 664ms/step - loss: 14.1113 - crf_1_loss: 7.0562 - crf_2_loss: 7.0550\n",
      "Epoch 50/100\n",
      "59/59 [==============================] - 38s 649ms/step - loss: 13.8355 - crf_1_loss: 6.9184 - crf_2_loss: 6.9171\n",
      "Epoch 51/100\n",
      "59/59 [==============================] - 39s 660ms/step - loss: 14.3167 - crf_1_loss: 7.1588 - crf_2_loss: 7.1579\n",
      "Epoch 52/100\n",
      "59/59 [==============================] - 41s 691ms/step - loss: 14.2409 - crf_1_loss: 7.1207 - crf_2_loss: 7.1202\n",
      "Epoch 53/100\n",
      "59/59 [==============================] - 42s 716ms/step - loss: 14.4683 - crf_1_loss: 7.2342 - crf_2_loss: 7.2341\n",
      "Epoch 54/100\n",
      "59/59 [==============================] - 42s 711ms/step - loss: 14.1736 - crf_1_loss: 7.0873 - crf_2_loss: 7.0863\n",
      "Epoch 55/100\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 14.0071 - crf_1_loss: 7.0040 - crf_2_loss: 7.0031\n",
      "Epoch 56/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 14.2059 - crf_1_loss: 7.1034 - crf_2_loss: 7.1025\n",
      "Epoch 57/100\n",
      "59/59 [==============================] - 40s 676ms/step - loss: 14.2563 - crf_1_loss: 7.1281 - crf_2_loss: 7.1282\n",
      "Epoch 58/100\n",
      "59/59 [==============================] - 40s 681ms/step - loss: 14.1582 - crf_1_loss: 7.0796 - crf_2_loss: 7.0786\n",
      "Epoch 59/100\n",
      "59/59 [==============================] - 42s 703ms/step - loss: 14.3765 - crf_1_loss: 7.1883 - crf_2_loss: 7.1882\n",
      "Epoch 60/100\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 13.9651 - crf_1_loss: 6.9827 - crf_2_loss: 6.9825\n",
      "Epoch 61/100\n",
      "59/59 [==============================] - 40s 685ms/step - loss: 14.4045 - crf_1_loss: 7.2024 - crf_2_loss: 7.2020\n",
      "Epoch 62/100\n",
      "59/59 [==============================] - 42s 706ms/step - loss: 14.7140 - crf_1_loss: 7.3571 - crf_2_loss: 7.3569\n",
      "Epoch 63/100\n",
      "59/59 [==============================] - 39s 655ms/step - loss: 13.8850 - crf_1_loss: 6.9426 - crf_2_loss: 6.9424\n",
      "Epoch 64/100\n",
      "59/59 [==============================] - 41s 692ms/step - loss: 14.0946 - crf_1_loss: 7.0473 - crf_2_loss: 7.0472\n",
      "Epoch 65/100\n",
      "59/59 [==============================] - 39s 669ms/step - loss: 13.8472 - crf_1_loss: 6.9237 - crf_2_loss: 6.9235\n",
      "Epoch 66/100\n",
      "59/59 [==============================] - 41s 690ms/step - loss: 13.8893 - crf_1_loss: 6.9447 - crf_2_loss: 6.9446\n",
      "Epoch 67/100\n",
      "59/59 [==============================] - 40s 670ms/step - loss: 14.1131 - crf_1_loss: 7.0570 - crf_2_loss: 7.0561\n",
      "Epoch 68/100\n",
      "59/59 [==============================] - 39s 667ms/step - loss: 14.2601 - crf_1_loss: 7.1302 - crf_2_loss: 7.1299\n",
      "Epoch 69/100\n",
      "59/59 [==============================] - 40s 675ms/step - loss: 14.5355 - crf_1_loss: 7.2680 - crf_2_loss: 7.2675\n",
      "Epoch 70/100\n",
      "59/59 [==============================] - 38s 650ms/step - loss: 13.7430 - crf_1_loss: 6.8715 - crf_2_loss: 6.8715\n",
      "Epoch 71/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 13.9702 - crf_1_loss: 6.9852 - crf_2_loss: 6.9850\n",
      "Epoch 72/100\n",
      "59/59 [==============================] - 40s 678ms/step - loss: 14.4929 - crf_1_loss: 7.2465 - crf_2_loss: 7.2464\n",
      "Epoch 73/100\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 14.4182 - crf_1_loss: 7.2093 - crf_2_loss: 7.2088\n",
      "Epoch 74/100\n",
      "59/59 [==============================] - 40s 681ms/step - loss: 14.7269 - crf_1_loss: 7.3638 - crf_2_loss: 7.3631\n",
      "Epoch 75/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.1003 - crf_1_loss: 7.0503 - crf_2_loss: 7.0499\n",
      "Epoch 76/100\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 13.8980 - crf_1_loss: 6.9490 - crf_2_loss: 6.9490\n",
      "Epoch 77/100\n",
      "59/59 [==============================] - 39s 660ms/step - loss: 13.8226 - crf_1_loss: 6.9111 - crf_2_loss: 6.9116\n",
      "Epoch 78/100\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 14.0703 - crf_1_loss: 7.0350 - crf_2_loss: 7.0352\n",
      "Epoch 79/100\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 13.7374 - crf_1_loss: 6.8688 - crf_2_loss: 6.8686\n",
      "Epoch 80/100\n",
      "59/59 [==============================] - 39s 658ms/step - loss: 13.9186 - crf_1_loss: 6.9593 - crf_2_loss: 6.9594\n",
      "Epoch 81/100\n",
      "59/59 [==============================] - 39s 664ms/step - loss: 13.9431 - crf_1_loss: 6.9715 - crf_2_loss: 6.9716\n",
      "Epoch 82/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.1758 - crf_1_loss: 7.0878 - crf_2_loss: 7.0881\n",
      "Epoch 83/100\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 14.3797 - crf_1_loss: 7.1898 - crf_2_loss: 7.1899\n",
      "Epoch 84/100\n",
      "59/59 [==============================] - 40s 678ms/step - loss: 14.6129 - crf_1_loss: 7.3062 - crf_2_loss: 7.3068\n",
      "Epoch 85/100\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 14.3569 - crf_1_loss: 7.1784 - crf_2_loss: 7.1785\n",
      "Epoch 86/100\n",
      "59/59 [==============================] - 38s 652ms/step - loss: 13.9478 - crf_1_loss: 6.9739 - crf_2_loss: 6.9739\n",
      "Epoch 87/100\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 13.7798 - crf_1_loss: 6.8899 - crf_2_loss: 6.8899\n",
      "Epoch 88/100\n",
      "59/59 [==============================] - 40s 676ms/step - loss: 14.3182 - crf_1_loss: 7.1589 - crf_2_loss: 7.1593\n",
      "Epoch 89/100\n",
      "59/59 [==============================] - 39s 664ms/step - loss: 14.1681 - crf_1_loss: 7.0841 - crf_2_loss: 7.0840\n",
      "Epoch 90/100\n",
      "59/59 [==============================] - 39s 662ms/step - loss: 14.1351 - crf_1_loss: 7.0677 - crf_2_loss: 7.0673\n",
      "Epoch 91/100\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 13.9366 - crf_1_loss: 6.9687 - crf_2_loss: 6.9679\n",
      "Epoch 92/100\n",
      "59/59 [==============================] - 39s 666ms/step - loss: 13.9193 - crf_1_loss: 6.9597 - crf_2_loss: 6.9596\n",
      "Epoch 93/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 14.3805 - crf_1_loss: 7.1904 - crf_2_loss: 7.1901\n",
      "Epoch 94/100\n",
      "59/59 [==============================] - 38s 652ms/step - loss: 13.8673 - crf_1_loss: 6.9338 - crf_2_loss: 6.9335\n",
      "Epoch 95/100\n",
      "59/59 [==============================] - 39s 657ms/step - loss: 13.9348 - crf_1_loss: 6.9675 - crf_2_loss: 6.9674\n",
      "Epoch 96/100\n",
      "59/59 [==============================] - 40s 674ms/step - loss: 14.4644 - crf_1_loss: 7.2322 - crf_2_loss: 7.2322\n",
      "Epoch 97/100\n",
      "59/59 [==============================] - 40s 674ms/step - loss: 14.3116 - crf_1_loss: 7.1560 - crf_2_loss: 7.1556\n",
      "Epoch 98/100\n",
      "59/59 [==============================] - 39s 655ms/step - loss: 13.7314 - crf_1_loss: 6.8659 - crf_2_loss: 6.8655\n",
      "Epoch 99/100\n",
      "59/59 [==============================] - 40s 675ms/step - loss: 14.3840 - crf_1_loss: 7.1921 - crf_2_loss: 7.1918\n",
      "Epoch 100/100\n",
      "59/59 [==============================] - 40s 681ms/step - loss: 14.5418 - crf_1_loss: 7.2709 - crf_2_loss: 7.2709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a3c26d0b8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/multi-crf_with_production_raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_x_word_test = tokenizer.padding_word(x_word_test)\n",
    "pad_x_char_test = tokenizer.padding_char(x_char_test)\n",
    "pad_y_test = [tokenizer.padding_tag(target) for target in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): {'title': entry['Name'], 'attributes': entry['Attributes']} for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []\n",
    "\n",
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s\n",
    "\n",
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt\n",
    "\n",
    "def evaluate_exact_match(result_dict):\n",
    "    annotation_size = 0\n",
    "    extracted_size = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for _id, val in result_dict.items():\n",
    "        true_set = set(val['true'])\n",
    "        pred_set = set(val['predict'])\n",
    "        \n",
    "        annotation_size += len(true_set)\n",
    "        extracted_size += len(pred_set)\n",
    "        TP += len(true_set & pred_set)\n",
    "        FP += len(pred_set - true_set)\n",
    "        FN += len(true_set - pred_set)\n",
    "\n",
    "    def precision(TP, FP):\n",
    "            return TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "    def recall(TP, FN):\n",
    "        return TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "    def f1(precision, recall):\n",
    "        return 2 * precision * recall / (precision + recall) \\\n",
    "            if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    score = {\n",
    "        'annotation_size': annotation_size\n",
    "        , 'extracted_size': extracted_size\n",
    "        , 'TP': TP\n",
    "        , 'FP': FP\n",
    "        , 'FN': FN\n",
    "        , 'precision': precision(TP, FP)\n",
    "        , 'recall': recall(TP, FN)\n",
    "        , 'f1': f1(precision(TP, FP), recall(TP, FN))\n",
    "    }\n",
    "    \n",
    "    return score\n",
    "\n",
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]\n",
    "\n",
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0, 'B': 1, 'I': 2, 'O': 3}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_attr = \"原材料\"\n",
    "attr_y_pred = y_pred[target_attr_list.index(target_attr)]\n",
    "attr_y_test = pad_y_test[target_attr_list.index(target_attr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.31345353675450766,\n",
       " 'precision': 0.3147632311977716,\n",
       " 'recall': 0.31215469613259667}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq(attr_y_test, attr_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(attr_y_pred))\n",
    "\n",
    "extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    \n",
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FN': 88,\n",
       " 'FP': 197,\n",
       " 'TP': 72,\n",
       " 'annotation_size': 160,\n",
       " 'extracted_size': 269,\n",
       " 'f1': 0.3356643356643357,\n",
       " 'precision': 0.26765799256505574,\n",
       " 'recall': 0.45}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_exact_match(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/raw-material_with_repl-compounds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
