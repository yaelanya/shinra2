{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(X[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(X[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_X = [np.array(_input)[shuffle_indices] for _input in X]\n",
    "                shuffled_y = np.array(y)[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = X\n",
    "                shuffled_labels = y\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                batch_X = [_input[start_index: end_index] for _input in shuffled_X]\n",
    "                batch_y = shuffled_y[start_index: end_index]\n",
    "                \n",
    "                batch_X[0] = tokenizer.padding_word(batch_X[0])\n",
    "                batch_X[1] = tokenizer.padding_char(batch_X[1])\n",
    "                \n",
    "                batch_y = tokenizer.padding_tag(batch_y)\n",
    "                \n",
    "                yield batch_X, batch_y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}\n",
    "\n",
    "target_headline_name = {\n",
    "    '原材料': \"cat_raw_material_headline\"\n",
    "    , '製造方法': \"cat_production_headline\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION\n",
    "# cheking words or repl_words\n",
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, None, 2 52975       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 50)     10200       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 100)    1428900     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 150)    0           time_distributed_3[0][0]         \n",
      "                                                                 embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 150)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, None, 200)    200800      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 100)    20100       bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 4)      404         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_3 (CRF)                     (None, None, 4)      44          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,713,423\n",
      "Trainable params: 1,713,423\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "\n",
    "feats = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats = Dense(param['tag_size'])(feats)\n",
    "\n",
    "crf = CRF(param['tag_size'])\n",
    "pred = crf(feats)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=pred)\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=crf.loss_function, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr = \"原材料\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_onehot(tag_seq, tokenizer):\n",
    "    return np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### CAUTION\n",
    "###\n",
    "# cheking words or repl_words\n",
    "x_word_train = tokenizer.transform_word(train_df.words.tolist())\n",
    "x_char_train = tokenizer.transform_char(train_df.words.tolist())\n",
    "y_train = tokenizer.transform_tag(train_df[target_col_name[target_attr]].tolist())\n",
    "\n",
    "x_word_test = tokenizer.transform_word(test_df.words.tolist())\n",
    "x_char_test = tokenizer.transform_char(test_df.words.tolist())\n",
    "y_test = tokenizer.transform_tag(test_df[target_col_name[target_attr]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y_train = encoding_onehot(y_train, tokenizer)\n",
    "y_test = encoding_onehot(y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "59/59 [==============================] - 38s 646ms/step - loss: 7.2658\n",
      "Epoch 2/100\n",
      "59/59 [==============================] - 35s 592ms/step - loss: 7.2380\n",
      "Epoch 3/100\n",
      "59/59 [==============================] - 35s 601ms/step - loss: 7.1938\n",
      "Epoch 4/100\n",
      "59/59 [==============================] - 36s 604ms/step - loss: 7.2946\n",
      "Epoch 5/100\n",
      "59/59 [==============================] - 36s 603ms/step - loss: 7.3005\n",
      "Epoch 6/100\n",
      "59/59 [==============================] - 36s 609ms/step - loss: 7.4197\n",
      "Epoch 7/100\n",
      "59/59 [==============================] - 35s 596ms/step - loss: 7.2056\n",
      "Epoch 8/100\n",
      "59/59 [==============================] - 34s 577ms/step - loss: 6.8938\n",
      "Epoch 9/100\n",
      "59/59 [==============================] - 34s 578ms/step - loss: 6.8491\n",
      "Epoch 10/100\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 7.3257\n",
      "Epoch 11/100\n",
      "59/59 [==============================] - 36s 607ms/step - loss: 7.3753\n",
      "Epoch 12/100\n",
      "59/59 [==============================] - 35s 592ms/step - loss: 7.1655\n",
      "Epoch 13/100\n",
      "59/59 [==============================] - 36s 602ms/step - loss: 7.2760\n",
      "Epoch 14/100\n",
      "59/59 [==============================] - 35s 598ms/step - loss: 7.1121\n",
      "Epoch 15/100\n",
      "59/59 [==============================] - 36s 610ms/step - loss: 7.0348\n",
      "Epoch 16/100\n",
      "59/59 [==============================] - 35s 595ms/step - loss: 6.8645\n",
      "Epoch 17/100\n",
      "59/59 [==============================] - 36s 616ms/step - loss: 7.1732\n",
      "Epoch 18/100\n",
      "59/59 [==============================] - 36s 613ms/step - loss: 6.8035\n",
      "Epoch 19/100\n",
      "59/59 [==============================] - 37s 630ms/step - loss: 7.0543\n",
      "Epoch 20/100\n",
      "59/59 [==============================] - 37s 630ms/step - loss: 7.1439\n",
      "Epoch 21/100\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 7.2071\n",
      "Epoch 22/100\n",
      "59/59 [==============================] - 36s 617ms/step - loss: 7.0196\n",
      "Epoch 23/100\n",
      "59/59 [==============================] - 37s 632ms/step - loss: 7.0544\n",
      "Epoch 24/100\n",
      "59/59 [==============================] - 36s 615ms/step - loss: 6.9655\n",
      "Epoch 25/100\n",
      "59/59 [==============================] - 38s 645ms/step - loss: 7.3402\n",
      "Epoch 26/100\n",
      "59/59 [==============================] - 37s 626ms/step - loss: 7.0277\n",
      "Epoch 27/100\n",
      "59/59 [==============================] - 37s 628ms/step - loss: 7.0310\n",
      "Epoch 28/100\n",
      "59/59 [==============================] - 37s 632ms/step - loss: 7.1437\n",
      "Epoch 29/100\n",
      "59/59 [==============================] - 36s 613ms/step - loss: 6.8374\n",
      "Epoch 30/100\n",
      "59/59 [==============================] - 37s 634ms/step - loss: 7.1325\n",
      "Epoch 31/100\n",
      "59/59 [==============================] - 38s 638ms/step - loss: 7.1285\n",
      "Epoch 32/100\n",
      "59/59 [==============================] - 37s 635ms/step - loss: 7.1855\n",
      "Epoch 33/100\n",
      "59/59 [==============================] - 37s 626ms/step - loss: 7.1028\n",
      "Epoch 34/100\n",
      "59/59 [==============================] - 37s 623ms/step - loss: 6.9945\n",
      "Epoch 35/100\n",
      "59/59 [==============================] - 37s 625ms/step - loss: 6.9248\n",
      "Epoch 36/100\n",
      "59/59 [==============================] - 37s 630ms/step - loss: 7.0791\n",
      "Epoch 37/100\n",
      "59/59 [==============================] - 37s 629ms/step - loss: 7.1581\n",
      "Epoch 38/100\n",
      "59/59 [==============================] - 37s 628ms/step - loss: 7.0758\n",
      "Epoch 39/100\n",
      "59/59 [==============================] - 38s 637ms/step - loss: 7.1340\n",
      "Epoch 40/100\n",
      "59/59 [==============================] - 37s 626ms/step - loss: 7.0286\n",
      "Epoch 41/100\n",
      "59/59 [==============================] - 37s 627ms/step - loss: 7.0368\n",
      "Epoch 42/100\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 7.2783\n",
      "Epoch 43/100\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 7.2864\n",
      "Epoch 44/100\n",
      "59/59 [==============================] - 37s 627ms/step - loss: 6.9866\n",
      "Epoch 45/100\n",
      "59/59 [==============================] - 38s 643ms/step - loss: 7.3267\n",
      "Epoch 46/100\n",
      "59/59 [==============================] - 38s 636ms/step - loss: 7.1212\n",
      "Epoch 47/100\n",
      "59/59 [==============================] - 37s 626ms/step - loss: 7.0591\n",
      "Epoch 48/100\n",
      "59/59 [==============================] - 37s 628ms/step - loss: 7.0332\n",
      "Epoch 49/100\n",
      "59/59 [==============================] - 37s 622ms/step - loss: 7.0118\n",
      "Epoch 50/100\n",
      "59/59 [==============================] - 37s 634ms/step - loss: 7.0489\n",
      "Epoch 51/100\n",
      "59/59 [==============================] - 37s 625ms/step - loss: 7.0578\n",
      "Epoch 52/100\n",
      "59/59 [==============================] - 38s 637ms/step - loss: 7.0113\n",
      "Epoch 53/100\n",
      "59/59 [==============================] - 37s 630ms/step - loss: 7.0266\n",
      "Epoch 54/100\n",
      "59/59 [==============================] - 38s 649ms/step - loss: 7.1553\n",
      "Epoch 55/100\n",
      "59/59 [==============================] - 38s 637ms/step - loss: 7.1513\n",
      "Epoch 56/100\n",
      "59/59 [==============================] - 38s 636ms/step - loss: 7.1178\n",
      "Epoch 57/100\n",
      "59/59 [==============================] - 38s 649ms/step - loss: 7.3844\n",
      "Epoch 58/100\n",
      "59/59 [==============================] - 38s 647ms/step - loss: 7.3292\n",
      "Epoch 59/100\n",
      "59/59 [==============================] - 37s 626ms/step - loss: 6.9979\n",
      "Epoch 60/100\n",
      "59/59 [==============================] - 38s 645ms/step - loss: 7.2495\n",
      "Epoch 61/100\n",
      "59/59 [==============================] - 38s 648ms/step - loss: 7.1825\n",
      "Epoch 62/100\n",
      "59/59 [==============================] - 37s 634ms/step - loss: 7.0433\n",
      "Epoch 63/100\n",
      "59/59 [==============================] - 38s 640ms/step - loss: 7.2022\n",
      "Epoch 64/100\n",
      "59/59 [==============================] - 37s 624ms/step - loss: 6.8531\n",
      "Epoch 65/100\n",
      "59/59 [==============================] - 38s 637ms/step - loss: 7.0732\n",
      "Epoch 66/100\n",
      "59/59 [==============================] - 37s 627ms/step - loss: 6.9372\n",
      "Epoch 67/100\n",
      "59/59 [==============================] - 37s 629ms/step - loss: 6.9358\n",
      "Epoch 68/100\n",
      "59/59 [==============================] - 41s 687ms/step - loss: 7.1581\n",
      "Epoch 69/100\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 7.2097\n",
      "Epoch 70/100\n",
      "59/59 [==============================] - 37s 634ms/step - loss: 7.1687\n",
      "Epoch 71/100\n",
      "59/59 [==============================] - 36s 606ms/step - loss: 7.4011\n",
      "Epoch 72/100\n",
      "59/59 [==============================] - 33s 555ms/step - loss: 6.5054\n",
      "Epoch 73/100\n",
      "59/59 [==============================] - 36s 606ms/step - loss: 7.1834\n",
      "Epoch 74/100\n",
      "59/59 [==============================] - 36s 607ms/step - loss: 7.2641\n",
      "Epoch 75/100\n",
      "59/59 [==============================] - 35s 595ms/step - loss: 7.0803\n",
      "Epoch 76/100\n",
      "59/59 [==============================] - 35s 594ms/step - loss: 7.0161\n",
      "Epoch 77/100\n",
      "59/59 [==============================] - 35s 599ms/step - loss: 7.1454\n",
      "Epoch 78/100\n",
      "59/59 [==============================] - 37s 634ms/step - loss: 6.9526\n",
      "Epoch 79/100\n",
      "59/59 [==============================] - 35s 589ms/step - loss: 6.9114\n",
      "Epoch 80/100\n",
      "59/59 [==============================] - 38s 645ms/step - loss: 7.1627\n",
      "Epoch 81/100\n",
      "59/59 [==============================] - 37s 624ms/step - loss: 7.0187\n",
      "Epoch 82/100\n",
      "59/59 [==============================] - 37s 636ms/step - loss: 7.2039\n",
      "Epoch 83/100\n",
      "59/59 [==============================] - 36s 610ms/step - loss: 7.0493\n",
      "Epoch 84/100\n",
      "59/59 [==============================] - 39s 666ms/step - loss: 7.0989\n",
      "Epoch 85/100\n",
      "59/59 [==============================] - 39s 653ms/step - loss: 6.9816\n",
      "Epoch 86/100\n",
      "59/59 [==============================] - 40s 671ms/step - loss: 7.2884\n",
      "Epoch 87/100\n",
      "59/59 [==============================] - 39s 659ms/step - loss: 7.0441\n",
      "Epoch 88/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 7.1683\n",
      "Epoch 89/100\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 7.1311\n",
      "Epoch 90/100\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 7.2878\n",
      "Epoch 91/100\n",
      "59/59 [==============================] - 39s 660ms/step - loss: 7.0340\n",
      "Epoch 92/100\n",
      "59/59 [==============================] - 40s 684ms/step - loss: 7.3623\n",
      "Epoch 93/100\n",
      "59/59 [==============================] - 39s 664ms/step - loss: 7.1405\n",
      "Epoch 94/100\n",
      "59/59 [==============================] - 39s 669ms/step - loss: 7.1365\n",
      "Epoch 95/100\n",
      "59/59 [==============================] - 39s 660ms/step - loss: 7.0325\n",
      "Epoch 96/100\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 7.0523\n",
      "Epoch 97/100\n",
      "59/59 [==============================] - 39s 666ms/step - loss: 7.0850\n",
      "Epoch 98/100\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 7.2463\n",
      "Epoch 99/100\n",
      "59/59 [==============================] - 38s 641ms/step - loss: 6.9138\n",
      "Epoch 100/100\n",
      "59/59 [==============================] - 38s 644ms/step - loss: 6.8060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcd31f8bf60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=100\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/bilstm-crf_raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_x_word_test = tokenizer.padding_word(x_word_test)\n",
    "pad_x_char_test = tokenizer.padding_char(x_char_test)\n",
    "pad_y_test = tokenizer.padding_tag(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): {'title': entry['Name'], 'attributes': entry['Attributes']} for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []\n",
    "\n",
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s\n",
    "\n",
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt\n",
    "\n",
    "def evaluate_exact_match(result_dict):\n",
    "    annotation_size = 0\n",
    "    extracted_size = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for _id, val in result_dict.items():\n",
    "        true_set = set(val['true'])\n",
    "        pred_set = set(val['predict'])\n",
    "        \n",
    "        annotation_size += len(true_set)\n",
    "        extracted_size += len(pred_set)\n",
    "        TP += len(true_set & pred_set)\n",
    "        FP += len(pred_set - true_set)\n",
    "        FN += len(true_set - pred_set)\n",
    "\n",
    "    def precision(TP, FP):\n",
    "            return TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "    def recall(TP, FN):\n",
    "        return TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "    def f1(precision, recall):\n",
    "        return 2 * precision * recall / (precision + recall) \\\n",
    "            if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    score = {\n",
    "        'annotation_size': annotation_size\n",
    "        , 'extracted_size': extracted_size\n",
    "        , 'TP': TP\n",
    "        , 'FP': FP\n",
    "        , 'FN': FN\n",
    "        , 'precision': precision(TP, FP)\n",
    "        , 'recall': recall(TP, FN)\n",
    "        , 'f1': f1(precision(TP, FP), recall(TP, FN))\n",
    "    }\n",
    "    \n",
    "    return score\n",
    "\n",
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]\n",
    "\n",
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.3425559947299078,\n",
       " 'precision': 0.327455919395466,\n",
       " 'recall': 0.35911602209944754}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq(pad_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_ids(df):\n",
    "    ids = []\n",
    "    for _id, g in df.groupby('_id'):\n",
    "        if (g.cat_raw_material_headline == 1).any():\n",
    "            ids.append(_id)\n",
    "            \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(y_pred))\n",
    "\n",
    "ids = filtering_ids(test_df)\n",
    "extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    # if ((row._id in ids) and (row.loc[target_headline_name[target_attr]] == 1)) or (row._id not in ids):\n",
    "    #    extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    \n",
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/result/raw-material.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
