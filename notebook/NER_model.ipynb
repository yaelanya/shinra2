{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Embedding, Bidirectional, TimeDistributed, LSTM, Dense, concatenate, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_contrib.layers import CRF\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed.\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.UNK = '<UNK>'\n",
    "        self.PAD = '<PAD>'\n",
    "        self.vocab_word = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_char = {self.PAD: 0, self.UNK: 1}\n",
    "        self.vocab_tag = {self.PAD: 0}\n",
    "        \n",
    "    def fit(self, sentences, tags, row_sentences=None):\n",
    "        self._fit_word(sentences)\n",
    "        \n",
    "        if row_sentences:\n",
    "            self._fit_char(row_sentences)\n",
    "        else:\n",
    "            self._fit_char(sentences)\n",
    "        \n",
    "        self._fit_tag(tags)\n",
    "        \n",
    "        self.vocab_word_size = len(self.vocab_word)\n",
    "        self.vocab_char_size = len(self.vocab_char)\n",
    "        self.vocab_tag_size = len(self.vocab_tag)\n",
    "    \n",
    "    def inverse_transform_tag(self, tag_id_seq):\n",
    "        seq = []\n",
    "        inv_vocab_tag = {v: k for k, v in self.vocab_tag.items()}\n",
    "        for tag_ids in tag_id_seq:\n",
    "            tags = [inv_vocab_tag[tag_id] for tag_id in tag_ids]\n",
    "            seq.append(tags)\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def padding_word(self, word_seq):\n",
    "        return pad_sequences(word_seq, padding='post')\n",
    "    \n",
    "    def padding_char(self, char_seq):\n",
    "        char_max = max([len(max(char_seq_in_sent, key=len)) for char_seq_in_sent in char_seq])\n",
    "        pad_seq = [pad_sequences(char_seq_in_sent, maxlen=char_max, padding='post') for char_seq_in_sent in char_seq]\n",
    "        \n",
    "        # 文の長さも揃える\n",
    "        return pad_sequences(pad_seq, padding='post')\n",
    "    \n",
    "    def padding_tag(self, tag_seq):\n",
    "        return pad_sequences(tag_seq, padding='post')\n",
    "\n",
    "    def _fit_word(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                if w in self.vocab_word:\n",
    "                    continue\n",
    "                self.vocab_word[w] = len(self.vocab_word)\n",
    "                \n",
    "    def _fit_char(self, sentences):\n",
    "        for s in sentences:\n",
    "            for w in s:\n",
    "                for c in w:\n",
    "                    if c in self.vocab_char:\n",
    "                        continue\n",
    "                    self.vocab_char[c] = len(self.vocab_char)\n",
    "                    \n",
    "    def _fit_tag(self, tag_seq):\n",
    "        for tags in tag_seq:\n",
    "            for tag in tags:\n",
    "                if tag in self.vocab_tag:\n",
    "                    continue\n",
    "                self.vocab_tag[tag] = len(self.vocab_tag)\n",
    "                \n",
    "    def transform_word(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            word_ids = [self.vocab_word.get(w, self.vocab_word[self.UNK]) for w in s]\n",
    "            seq.append(word_ids)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_char(self, sentences):\n",
    "        seq = []\n",
    "        for s in sentences:\n",
    "            char_seq = []\n",
    "            for w in s:\n",
    "                char_ids = [self.vocab_char.get(c, self.vocab_char[self.UNK]) for c in w]\n",
    "                char_seq.append(char_ids)\n",
    "            seq.append(char_seq)\n",
    "            \n",
    "        return seq\n",
    "    \n",
    "    def transform_tag(self, tag_seq):\n",
    "        seq = []\n",
    "        for tags in tag_seq:\n",
    "            tag_ids = [self.vocab_tag[tag] for tag in tags]\n",
    "            seq.append(tag_ids)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X, y, batch_size, tokenizer, shuffle=True):\n",
    "    num_batches_per_epoch = int((len(X[0]) - 1) / batch_size) + 1\n",
    "\n",
    "    def data_generator():\n",
    "        data_size = len(X[0])\n",
    "        while True:\n",
    "            # Shuffle the data at each epoch\n",
    "            if shuffle:\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "                shuffled_X = [np.array(_input)[shuffle_indices] for _input in X]\n",
    "                shuffled_y = np.array(y)[shuffle_indices]\n",
    "            else:\n",
    "                shuffled_data = X\n",
    "                shuffled_labels = y\n",
    "\n",
    "            for batch_num in range(num_batches_per_epoch):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "                batch_X = [_input[start_index: end_index] for _input in shuffled_X]\n",
    "                batch_y = shuffled_y[start_index: end_index]\n",
    "                \n",
    "                batch_X[0] = tokenizer.padding_word(batch_X[0])\n",
    "                batch_X[1] = tokenizer.padding_char(batch_X[1])\n",
    "                batch_X[2] = np.array([[batch_X[2][i]] * len(x) for i, x in enumerate(batch_X[0])])\n",
    "                \n",
    "                batch_y = tokenizer.padding_tag(batch_y)\n",
    "                \n",
    "                yield batch_X, batch_y\n",
    "\n",
    "    return num_batches_per_epoch, data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col_name = {\n",
    "    'ふりがな': \"production_tag_seq\"\n",
    "    , '別称': \"another_name_tag_seq\"\n",
    "    , '用途': \"use_tag_seq\"\n",
    "    , '種類': \"type_tag_seq\"\n",
    "    , '商標名': \"trademark_tag_seq\"\n",
    "    , '特性': \"property_tag_seq\"\n",
    "    , '原材料': \"raw_material_tag_seq\"\n",
    "    , '製造方法': \"production_tag_seq\"\n",
    "    , '生成化合物': \"formation_tag_seq\"\n",
    "    , 'CAS番号': \"cas_tag_seq\"\n",
    "    , '化学式': \"chemical_formula_tag_seq\"\n",
    "    , '密度': \"density_tag_seq\"\n",
    "    , '融点': \"melting_tag_seq\"\n",
    "    , '沸点': \"boiling_tag_seq\"\n",
    "    , '示性式': \"rational_formula_tag_seq\"\n",
    "}\n",
    "\n",
    "target_headline_name = {\n",
    "    '原材料': \"cat_raw_material_headline\"\n",
    "    , '製造方法': \"cat_production_headline\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_pickle(\"../data/train_IOB_repl_compound.pkl\")\n",
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MyTokenizer()\n",
    "tokenizer.fit(\n",
    "    sentences=train_df.repl_words.tolist()\n",
    "    , row_sentences=train_df.words.tolist()\n",
    "    , tags=['B', 'I', 'O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'char_vocab_size': tokenizer.vocab_char_size\n",
    "    , 'word_vocab_size':tokenizer.vocab_word_size\n",
    "    , 'tag_size': tokenizer.vocab_tag_size\n",
    "    , 'char_emb_dim': 25\n",
    "    , 'word_emb_dim': 100\n",
    "    , 'char_lstm_units': 25\n",
    "    , 'word_lstm_units': 100\n",
    "    , 'dropout_rate': 0.5\n",
    "    , 'lstm_activation': 'tanh'\n",
    "    , 'fc_activation': 'tanh'\n",
    "    , 'fc_units': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, None, 2 52975       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 50)     10200       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 100)    1172300     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 150)    0           time_distributed_2[0][0]         \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 150)    0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, None, 200)    200800      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 100)    20100       bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 4)      404         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "crf_2 (CRF)                     (None, None, 4)      44          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,456,823\n",
      "Trainable params: 1,456,823\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_input = Input(shape=(None, None))\n",
    "word_input = Input(shape=(None,))\n",
    "\n",
    "char_emb = Embedding(input_dim=param['char_vocab_size']\n",
    "                     , output_dim=param['char_emb_dim']\n",
    "                     , mask_zero=True)(char_input)\n",
    "char_emb = TimeDistributed(Bidirectional(LSTM(units=param['char_lstm_units'], activation=param['lstm_activation'])))(char_emb)\n",
    "\n",
    "word_emb = Embedding(input_dim=param['word_vocab_size']\n",
    "                     , output_dim=param['word_emb_dim']\n",
    "                     , mask_zero=True)(word_input)\n",
    "\n",
    "feats = concatenate([char_emb, word_emb])\n",
    "\n",
    "feats = Dropout(param['dropout_rate'])(feats)\n",
    "\n",
    "feats = Bidirectional(LSTM(units=param['word_lstm_units'], return_sequences=True, activation=param['lstm_activation']))(feats)\n",
    "\n",
    "\n",
    "feats = Dense(param['fc_units'], activation=param['fc_activation'])(feats)\n",
    "feats = Dense(param['tag_size'])(feats)\n",
    "\n",
    "crf = CRF(param['tag_size'])\n",
    "pred = crf(feats)\n",
    "\n",
    "model = Model(inputs=[word_input, char_input], outputs=pred)\n",
    "\n",
    "sgd = SGD(lr=0.01, clipvalue=5.) # original paper\n",
    "adam = Adam()\n",
    "\n",
    "model.compile(loss=crf.loss_function, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽出対象の属性を指定\n",
    "target_attr = \"原材料\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_onehot(tag_seq, tokenizer):\n",
    "    return np.array([np.identity(tokenizer.vocab_tag_size)[tags] for tags in tag_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word_train = tokenizer.transform_word(train_df.repl_words.tolist())\n",
    "x_char_train = tokenizer.transform_char(train_df.words.tolist())\n",
    "y_train = tokenizer.transform_tag(train_df[target_col_name[target_attr]].tolist())\n",
    "\n",
    "x_word_test = tokenizer.transform_word(test_df.repl_words.tolist())\n",
    "x_char_test = tokenizer.transform_char(test_df.words.tolist())\n",
    "y_test = tokenizer.transform_tag(test_df[target_col_name[target_attr]].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "y_train = encoding_onehot(y_train, tokenizer)\n",
    "y_test = encoding_onehot(y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_steps, train_batches = batch_iter([x_word_train, x_char_train], y_train, batch_size, tokenizer)\n",
    "valid_steps, valid_batches = batch_iter([x_word_test, x_char_test], y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "59/59 [==============================] - 38s 639ms/step - loss: 7.5378\n",
      "Epoch 2/50\n",
      "59/59 [==============================] - 35s 585ms/step - loss: 6.7077\n",
      "Epoch 3/50\n",
      "59/59 [==============================] - 36s 606ms/step - loss: 6.9977\n",
      "Epoch 4/50\n",
      "59/59 [==============================] - 37s 629ms/step - loss: 7.3254\n",
      "Epoch 5/50\n",
      "59/59 [==============================] - 36s 609ms/step - loss: 7.0610\n",
      "Epoch 6/50\n",
      "59/59 [==============================] - 37s 629ms/step - loss: 7.4737\n",
      "Epoch 7/50\n",
      "59/59 [==============================] - 34s 584ms/step - loss: 6.6747\n",
      "Epoch 8/50\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 7.1262\n",
      "Epoch 9/50\n",
      "59/59 [==============================] - 42s 710ms/step - loss: 7.4239\n",
      "Epoch 10/50\n",
      "59/59 [==============================] - 39s 654ms/step - loss: 6.7546\n",
      "Epoch 11/50\n",
      "59/59 [==============================] - 43s 733ms/step - loss: 7.7853\n",
      "Epoch 12/50\n",
      "59/59 [==============================] - 37s 628ms/step - loss: 6.3820\n",
      "Epoch 13/50\n",
      "59/59 [==============================] - 40s 677ms/step - loss: 7.1347\n",
      "Epoch 14/50\n",
      "59/59 [==============================] - 40s 683ms/step - loss: 7.2429\n",
      "Epoch 15/50\n",
      "59/59 [==============================] - 40s 682ms/step - loss: 7.1401\n",
      "Epoch 16/50\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 6.7481\n",
      "Epoch 17/50\n",
      "59/59 [==============================] - 41s 698ms/step - loss: 7.2979\n",
      "Epoch 18/50\n",
      "59/59 [==============================] - 38s 650ms/step - loss: 6.6279\n",
      "Epoch 19/50\n",
      "59/59 [==============================] - 43s 724ms/step - loss: 7.7686\n",
      "Epoch 20/50\n",
      "59/59 [==============================] - 35s 601ms/step - loss: 6.0626\n",
      "Epoch 21/50\n",
      "59/59 [==============================] - 40s 680ms/step - loss: 7.0251\n",
      "Epoch 22/50\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 7.0864\n",
      "Epoch 23/50\n",
      "59/59 [==============================] - 41s 693ms/step - loss: 7.2734\n",
      "Epoch 24/50\n",
      "59/59 [==============================] - 39s 655ms/step - loss: 6.7696\n",
      "Epoch 25/50\n",
      "59/59 [==============================] - 42s 715ms/step - loss: 7.6267\n",
      "Epoch 26/50\n",
      "59/59 [==============================] - 40s 685ms/step - loss: 7.2026\n",
      "Epoch 27/50\n",
      "59/59 [==============================] - 39s 661ms/step - loss: 6.7628\n",
      "Epoch 28/50\n",
      "59/59 [==============================] - 43s 727ms/step - loss: 7.7670\n",
      "Epoch 29/50\n",
      "59/59 [==============================] - 36s 616ms/step - loss: 6.1259\n",
      "Epoch 30/50\n",
      "59/59 [==============================] - 40s 672ms/step - loss: 7.1084\n",
      "Epoch 31/50\n",
      "59/59 [==============================] - 43s 722ms/step - loss: 7.6316\n",
      "Epoch 32/50\n",
      "59/59 [==============================] - 37s 623ms/step - loss: 6.2743\n",
      "Epoch 33/50\n",
      "59/59 [==============================] - 40s 683ms/step - loss: 7.0582\n",
      "Epoch 34/50\n",
      "59/59 [==============================] - 39s 656ms/step - loss: 6.8168\n",
      "Epoch 35/50\n",
      "59/59 [==============================] - 42s 715ms/step - loss: 7.4740\n",
      "Epoch 36/50\n",
      "59/59 [==============================] - 39s 654ms/step - loss: 6.7731\n",
      "Epoch 37/50\n",
      "59/59 [==============================] - 43s 727ms/step - loss: 7.7808\n",
      "Epoch 38/50\n",
      "59/59 [==============================] - 38s 638ms/step - loss: 6.4764\n",
      "Epoch 39/50\n",
      "59/59 [==============================] - 41s 696ms/step - loss: 7.3868\n",
      "Epoch 40/50\n",
      "59/59 [==============================] - 41s 688ms/step - loss: 7.3349\n",
      "Epoch 41/50\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 6.8890\n",
      "Epoch 42/50\n",
      "59/59 [==============================] - 39s 668ms/step - loss: 6.9082\n",
      "Epoch 43/50\n",
      "59/59 [==============================] - 42s 712ms/step - loss: 7.4069\n",
      "Epoch 44/50\n",
      "59/59 [==============================] - 43s 723ms/step - loss: 7.6174\n",
      "Epoch 45/50\n",
      "59/59 [==============================] - 39s 663ms/step - loss: 6.4946\n",
      "Epoch 46/50\n",
      "59/59 [==============================] - 40s 673ms/step - loss: 7.0065\n",
      "Epoch 47/50\n",
      "59/59 [==============================] - 40s 682ms/step - loss: 7.5356\n",
      "Epoch 48/50\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 6.5031\n",
      "Epoch 49/50\n",
      "59/59 [==============================] - 37s 635ms/step - loss: 7.2260\n",
      "Epoch 50/50\n",
      "59/59 [==============================] - 36s 608ms/step - loss: 7.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7b68d33748>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_batches, train_steps\n",
    "                    #, validation_data=valid_batches, validation_steps=valid_batches\n",
    "                    , epochs=50\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../model/raw-material_with_repl-compound_headline.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"../model/raw-material.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_x_word_test = tokenizer.padding_word(x_word_test)\n",
    "pad_x_char_test = tokenizer.padding_char(x_char_test)\n",
    "pad_y_test = tokenizer.padding_tag(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([pad_x_word_test, pad_x_char_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/compound_train.json\", 'r') as f:\n",
    "    raw_train = json.load(f)\n",
    "    train_dict = {str(entry['WikipediaID']): {'title': entry['Name'], 'attributes': entry['Attributes']} for entry in raw_train['entry']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [i for sub_l in l for i in sub_l]\n",
    "\n",
    "def extract_words(word_seq, tag_seq):\n",
    "    words_list = []\n",
    "    words = []\n",
    "    for word, tag in zip(word_seq, tag_seq):\n",
    "        '''\n",
    "        if ((tag == 2) and (len(phrase) == 0)) or ((tag == 3) and (len(phrase) > 0)):\n",
    "            phrase.append(word)\n",
    "        elif tag == 2 and len(phrase) > 0:\n",
    "            phrase_list.append(phrase)\n",
    "            phrase = [word]\n",
    "        '''\n",
    "        if tag == tokenizer.vocab_tag['B'] or tag == tokenizer.vocab_tag['I']:\n",
    "            words.append(word)\n",
    "        elif words:\n",
    "            words_list.append(words)\n",
    "            words = []\n",
    "\n",
    "    if words:\n",
    "        words_list.append(words)\n",
    "        \n",
    "    return words_list\n",
    "\n",
    "def extract_strings(sentence, extracted_words):\n",
    "    if extracted_words:\n",
    "        patt = extract_pattern(extracted_words)\n",
    "        return re.findall(patt, sentence)\n",
    "    return []\n",
    "\n",
    "def escape(s):\n",
    "    _s = s.replace(r'.', r'\\.')\n",
    "    _s = _s.replace(r'+', r'\\+')\n",
    "    _s = _s.replace(r'-', r'\\-')\n",
    "    _s = _s.replace(r'^', r'\\^')\n",
    "    _s = _s.replace(r'?', r'\\?')\n",
    "    _s = _s.replace(r'$', r'\\$')\n",
    "    _s = _s.replace(r'|', r'\\|')\n",
    "    _s = _s.replace(r'(', r'\\(').replace(r')', r'\\)')\n",
    "    _s = _s.replace(r'[', r'\\[').replace(r']', r'\\]')\n",
    "    _s = _s.replace(r'{', r'\\{').replace(r'}', r'\\}')\n",
    "    \n",
    "    _s = _s.replace(r'*', '\\*')\n",
    "    _s = re.sub(r'\\\\s\\\\\\*', '\\s*', _s)\n",
    "    \n",
    "    return _s\n",
    "\n",
    "def extract_pattern(chunks):\n",
    "    patt = [''.join(chunk) for chunk in chunks]\n",
    "    patt = ['\\s*'.join(list(p)) for p in patt] # 元の文に空白が入っている場合を考慮\n",
    "    patt = '|'.join([escape(p) for p in patt])\n",
    "    \n",
    "    return patt\n",
    "\n",
    "def evaluate_exact_match(result_dict):\n",
    "    annotation_size = 0\n",
    "    extracted_size = 0\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for _id, val in result_dict.items():\n",
    "        true_set = set(val['true'])\n",
    "        pred_set = set(val['predict'])\n",
    "        \n",
    "        annotation_size += len(true_set)\n",
    "        extracted_size += len(pred_set)\n",
    "        TP += len(true_set & pred_set)\n",
    "        FP += len(pred_set - true_set)\n",
    "        FN += len(true_set - pred_set)\n",
    "\n",
    "    def precision(TP, FP):\n",
    "            return TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "\n",
    "    def recall(TP, FN):\n",
    "        return TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "\n",
    "    def f1(precision, recall):\n",
    "        return 2 * precision * recall / (precision + recall) \\\n",
    "            if (precision + recall) != 0 else 0.0\n",
    "    \n",
    "    score = {\n",
    "        'annotation_size': annotation_size\n",
    "        , 'extracted_size': extracted_size\n",
    "        , 'TP': TP\n",
    "        , 'FP': FP\n",
    "        , 'FN': FN\n",
    "        , 'precision': precision(TP, FP)\n",
    "        , 'recall': recall(TP, FN)\n",
    "        , 'f1': f1(precision(TP, FP), recall(TP, FN))\n",
    "    }\n",
    "    \n",
    "    return score\n",
    "\n",
    "def onehot2id(onehot_seq):\n",
    "    return np.argmax(onehot_seq, -1)\n",
    "\n",
    "def remove_pad(tag_seq):\n",
    "    return [tags[np.where(tags > 0)[0]] for tags in tag_seq]\n",
    "\n",
    "def evaluate_seq(y_true, y_pred):\n",
    "    _y_true = onehot2id(y_true)\n",
    "    _y_true = remove_pad(_y_true)\n",
    "    _y_true = tokenizer.inverse_transform_tag(_y_true)\n",
    "\n",
    "    _y_pred = onehot2id(y_pred)\n",
    "    _y_pred = remove_pad(_y_pred)\n",
    "    _y_pred = tokenizer.inverse_transform_tag(_y_pred)\n",
    "\n",
    "    return {'precision': precision_score(_y_true, _y_pred)\n",
    "            , 'recall': recall_score(_y_true, _y_pred)\n",
    "            , 'f1': f1_score(_y_true, _y_pred)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.3425559947299078,\n",
       " 'precision': 0.327455919395466,\n",
       " 'recall': 0.35911602209944754}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_seq(pad_y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering_ids(df):\n",
    "    ids = []\n",
    "    for _id, g in df.groupby('_id'):\n",
    "        if (g.cat_raw_material_headline == 1).any():\n",
    "            ids.append(_id)\n",
    "            \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_pickle(\"../data/test_IOB_repl_compound.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tag_seq = remove_pad(onehot2id(y_pred))\n",
    "\n",
    "ids = filtering_ids(test_df)\n",
    "extracted_dict = {}\n",
    "for i, (_, row) in enumerate(test_df.iterrows()):\n",
    "    extracted = extract_words(row.words, pred_tag_seq[i])\n",
    "    extracted = extract_strings(row.sentence, extracted)\n",
    "    if ((row._id in ids) and (row.loc[target_headline_name[target_attr]] == 1)) or (row._id not in ids):\n",
    "        extracted_dict[row._id] = extracted_dict.get(row._id, []) + extracted\n",
    "    \n",
    "result_dict = {}\n",
    "for _id in test_df._id.unique():\n",
    "    result_dict[_id] = \\\n",
    "    {'title': train_dict[_id]['title']\n",
    "     , 'true': train_dict[_id]['attributes'][target_attr]\n",
    "     , 'predict': list(set(extracted_dict.get(_id, [])))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FN': 78,\n",
       " 'FP': 152,\n",
       " 'TP': 82,\n",
       " 'annotation_size': 160,\n",
       " 'extracted_size': 234,\n",
       " 'f1': 0.416243654822335,\n",
       " 'precision': 0.3504273504273504,\n",
       " 'recall': 0.5125}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_exact_match(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1024773': {'predict': ['ペンタセンキノン',\n",
       "   'フタルアルデヒド',\n",
       "   'アルミニウム',\n",
       "   '一酸化炭素',\n",
       "   '水',\n",
       "   'シクロヘキサン-1,4-ジオン'],\n",
       "  'title': 'ペンタセン',\n",
       "  'true': ['シクロヘキサン-1,4-ジオン',\n",
       "   'フタルアルデヒド',\n",
       "   'アルミニウムアマルガム',\n",
       "   'ペンタセンキノン',\n",
       "   '水',\n",
       "   'エチレン',\n",
       "   'テトラハロゲノベンゼン']},\n",
       " '1050477': {'predict': ['モルヒネ'], 'title': 'レミフェンタニル', 'true': []},\n",
       " '1067475': {'predict': ['TCSPC',\n",
       "   'レソルシノール',\n",
       "   'ナトリウム',\n",
       "   '塩化亜鉛',\n",
       "   '無水フタル酸',\n",
       "   'イソチオシアネート'],\n",
       "  'title': 'フルオレセイン',\n",
       "  'true': ['レソルシノール', '塩化亜鉛', '無水フタル酸', 'スルホン']},\n",
       " '1095173': {'predict': ['シアン化ナトリウム',\n",
       "   '炭素',\n",
       "   'シアン化銅',\n",
       "   'シアノヒドリン',\n",
       "   '塩化水素',\n",
       "   'シアン化水素',\n",
       "   'ハロゲン化物',\n",
       "   'シアン化カリウム'],\n",
       "  'title': 'シアン化物',\n",
       "  'true': []},\n",
       " '1140453': {'predict': [], 'title': 'ピパンペロン', 'true': []},\n",
       " '1148105': {'predict': [], 'title': 'グラニセトロン', 'true': []},\n",
       " '1193389': {'predict': ['水酸化ロジウム(III)',\n",
       "   '塩素ガス',\n",
       "   '塩化カリウム',\n",
       "   '濃塩酸',\n",
       "   '塩化水素',\n",
       "   'ヘキサクロロロジウム酸(III)カリウム',\n",
       "   '硝酸銀',\n",
       "   '水酸化カリウム'],\n",
       "  'title': '塩化ロジウム(III)',\n",
       "  'true': ['塩化水素',\n",
       "   'ロジウム',\n",
       "   'ヘキサクロロロジウム酸(III)カリウム',\n",
       "   '水酸化ロジウム(III)',\n",
       "   '水酸化カリウム',\n",
       "   '塩化カリウム',\n",
       "   '濃塩酸',\n",
       "   '塩化ロジウム三水和物',\n",
       "   '塩素ガス']},\n",
       " '1198019': {'predict': ['希酸',\n",
       "   'アセトン',\n",
       "   'シトラール',\n",
       "   '三フッ化ホウ素',\n",
       "   'α-イオノン',\n",
       "   'アルドール',\n",
       "   'リン酸',\n",
       "   'プソイドイオノン'],\n",
       "  'title': 'イオノン',\n",
       "  'true': ['プソイドイオノン',\n",
       "   '植物の精油',\n",
       "   '硫酸',\n",
       "   '三フッ化ホウ素',\n",
       "   'リン酸',\n",
       "   '希酸',\n",
       "   'シトラール',\n",
       "   'アセトン']},\n",
       " '1203950': {'predict': [], 'title': 'フルニキシン', 'true': []},\n",
       " '12437': {'predict': ['ナトリウムエトキシド',\n",
       "   'ホルムアルデヒド',\n",
       "   'メタノール',\n",
       "   '酵素',\n",
       "   '硫酸',\n",
       "   'エチレングリコール',\n",
       "   '酢酸',\n",
       "   'アセトアルデヒド',\n",
       "   '伯エタノール'],\n",
       "  'title': 'エタノール',\n",
       "  'true': ['アルコール', 'エチレン']},\n",
       " '1273537': {'predict': ['ヘキサメチレンジアミン'],\n",
       "  'title': '塩化アジポイル',\n",
       "  'true': ['アジピン酸']},\n",
       " '1372856': {'predict': ['アゾビスイソブチロニトリル', 'チオール基'],\n",
       "  'title': 'チオ酢酸',\n",
       "  'true': []},\n",
       " '1383346': {'predict': [], 'title': '一酸化銀', 'true': []},\n",
       " '1454192': {'predict': [], 'title': 'テトラヒドロゾリン', 'true': []},\n",
       " '1483298': {'predict': ['白金', 'アンモニア', '塩化白金酸', 'V. Voorhees', '硝酸', 'アダムス'],\n",
       "  'title': 'アダムス触媒',\n",
       "  'true': ['塩化白金酸アンモニウム', '硝酸ナトリウム', '塩化白金酸']},\n",
       " '1540694': {'predict': [], 'title': '窒化ベリリウム', 'true': []},\n",
       " '1560938': {'predict': [], 'title': 'オンダンセトロン', 'true': []},\n",
       " '1563432': {'predict': [], 'title': 'リゾホスファチジン酸', 'true': ['リゾホスファチジルコリン']},\n",
       " '1573835': {'predict': [], 'title': '安息香酸カルシウム', 'true': []},\n",
       " '1591322': {'predict': [],\n",
       "  'title': 'デキストラン',\n",
       "  'true': ['Leuconostoc mesenteroides が生産する高分子デキストラン', '高分子デキストラン', 'スクロース']},\n",
       " '160786': {'predict': ['フェニルカルビノール', 'ベンゼン', 'トルエン', 'ケイ皮酸', 'メタノール'],\n",
       "  'title': 'スチレン',\n",
       "  'true': ['エチルベンゼン', 'ケイ皮酸脱炭酸酵素', 'ケイ皮酸']},\n",
       " '1616237': {'predict': ['グアニジン'], 'title': 'フェンホルミン', 'true': []},\n",
       " '1625510': {'predict': ['クロロギ酸エチル',\n",
       "   'ベンズイソオキサゾール',\n",
       "   'ホスゲン',\n",
       "   '二酸化炭素',\n",
       "   'アントラニル酸'],\n",
       "  'title': 'イサト酸無水物',\n",
       "  'true': ['アントラニル酸ナトリウム', 'ホスゲン', 'アントラニル酸', 'クロロギ酸エチル', 'ベンズイソオキサゾール']},\n",
       " '1658717': {'predict': ['ヨウ化パラジウム', '五フッ化ヨウ素', 'フッ素'],\n",
       "  'title': '七フッ化ヨウ素',\n",
       "  'true': ['五フッ化ヨウ素', 'フッ素']},\n",
       " '1663779': {'predict': ['ベンゾニトリル', 'アンモニア'],\n",
       "  'title': 'ベンジルアミン',\n",
       "  'true': ['ベンゾニトリル', '水素']},\n",
       " '1690387': {'predict': ['Mercury', '水銀', 'シアン', '水銀(I)イオン'],\n",
       "  'title': 'シアン化水銀(II)',\n",
       "  'true': ['シアン', '水銀']},\n",
       " '1787749': {'predict': ['ホスファチジルイノシトール', 'ホスホリパーゼ', 'ホスファチジルイノシトール4,5-ビスリン酸'],\n",
       "  'title': 'イノシトールトリスリン酸',\n",
       "  'true': ['ホスファチジルイノシトール4,5-ビスリン酸']},\n",
       " '1793432': {'predict': [], 'title': 'ヨウ化亜鉛', 'true': []},\n",
       " '1815995': {'predict': [], 'title': 'フッ化亜鉛', 'true': []},\n",
       " '1866600': {'predict': ['メチルエチルケトン', 'グリオキシム', '亜硝酸エステル', 'ジケトン'],\n",
       "  'title': 'ジメチルグリオキシム',\n",
       "  'true': ['オキシム', 'ヒドロキシルアミン硫酸ナトリウム', 'メチルエチルケトン', '亜硝酸エステル']},\n",
       " '189601': {'predict': [],\n",
       "  'title': 'シガトキシン',\n",
       "  'true': ['藻類', '有毒渦鞭毛藻', 'ポリケチド']},\n",
       " '1905810': {'predict': ['二酸化炭素',\n",
       "   '水酸化ベリリウム',\n",
       "   'アンモニア水',\n",
       "   '塩基性塩Be2CO3',\n",
       "   '水',\n",
       "   '四水和物'],\n",
       "  'title': '炭酸ベリリウム',\n",
       "  'true': ['二酸化炭素', '水', '水酸化ベリリウム']},\n",
       " '1905814': {'predict': ['アンモニア水', 'ベリリウム塩水溶液', 'マグネシウム塩'],\n",
       "  'title': '水酸化ベリリウム',\n",
       "  'true': ['水酸化ナトリウム', 'ベリリウム塩', 'アンモニア 水']},\n",
       " '1932135': {'predict': ['ノルトロパン', 'オルニチン', 'コカイン'],\n",
       "  'title': 'トロパン',\n",
       "  'true': ['オルニチン']},\n",
       " '1936318': {'predict': ['メタリジウム'],\n",
       "  'title': 'スワインソニン',\n",
       "  'true': ['菌',\n",
       "   'メタリジウム',\n",
       "   'Astragalus lentiginosus',\n",
       "   'Swainosona canescens (Benth.) A Lee',\n",
       "   'Rhizoctonia leguminicola',\n",
       "   '被子植物']},\n",
       " '1939275': {'predict': [], 'title': 'パントイン酸', 'true': []},\n",
       " '19566': {'predict': ['金属イオン',\n",
       "   'シアン化ナトリウム',\n",
       "   'アミグダリン',\n",
       "   'シアン化合物',\n",
       "   'メタン',\n",
       "   'ウメ',\n",
       "   'アンモニア',\n",
       "   '酸',\n",
       "   'シアンメトヘモグロビン',\n",
       "   '硫酸',\n",
       "   'アクリロニトリル',\n",
       "   'Fe3',\n",
       "   'ヘモグロビン',\n",
       "   'メトヘモグロビン'],\n",
       "  'title': 'シアン化水素',\n",
       "  'true': ['空気', 'アンモニア', 'メタン', 'シアン化ナトリウム', '酸']},\n",
       " '1977270': {'predict': ['アンチモン', 'エドモント'],\n",
       "  'title': '三フッ化アンチモン',\n",
       "  'true': ['三酸化アンチモン', 'フッ化水素']},\n",
       " '1980911': {'predict': ['トリフェニルホスフィン',\n",
       "   '1,3-ジエン',\n",
       "   '炭素',\n",
       "   'ピロール',\n",
       "   'ホスホールを含む環',\n",
       "   'フェニルナトリウム'],\n",
       "  'title': 'ホスホール',\n",
       "  'true': []},\n",
       " '2061207': {'predict': [], 'title': 'ヘキサフルオロプロペン', 'true': []},\n",
       " '2080116': {'predict': [], 'title': 'カナジン', 'true': []},\n",
       " '2137257': {'predict': ['鉄', '亜硝酸', '硝酸', 'シアン', '酸素', '亜硝酸アミル', 'メトヘモグロビン'],\n",
       "  'title': 'メトヘモグロビン',\n",
       "  'true': []},\n",
       " '2140608': {'predict': [], 'title': 'プロトカテク酸', 'true': []},\n",
       " '2153576': {'predict': ['トリグリセリド', 'ブドウ糖'], 'title': 'リラグルチド', 'true': []},\n",
       " '2170531': {'predict': ['ナトリウム'], 'title': 'スルファメトキサゾール', 'true': []},\n",
       " '217088': {'predict': [], 'title': 'エナラプリル', 'true': []},\n",
       " '2190946': {'predict': [], 'title': 'フェニルアセトン', 'true': []},\n",
       " '2211690': {'predict': [], 'title': 'パクロブトラゾール', 'true': []},\n",
       " '2234076': {'predict': ['水素', 'アンモニア'],\n",
       "  'title': 'イソブチルアミン',\n",
       "  'true': ['水素', 'イソプロパノール', 'アンモニア']},\n",
       " '2312494': {'predict': [], 'title': 'リンモリブデン酸', 'true': []},\n",
       " '2334030': {'predict': [],\n",
       "  'title': 'フィリピン (化合物)',\n",
       "  'true': ['放線菌Streptomyces filipinensisの菌糸体および培養濾液']},\n",
       " '2346795': {'predict': ['CO', 'ジアゾメタン', '金属クラスター', '沸点'],\n",
       "  'title': 'デカカルボニルジヒドリド三オスミウム',\n",
       "  'true': []},\n",
       " '2488348': {'predict': ['グルタラール', 'グルタルアルデヒド'],\n",
       "  'title': 'オルトフタルアルデヒド',\n",
       "  'true': []},\n",
       " '248891': {'predict': [], 'title': 'マイトトキシン', 'true': ['ポリエーテル', '硫酸']},\n",
       " '2569592': {'predict': ['3-ジメチルアミノフェノール', '塩化銀', 'ブロモエタン', '塩素', '臭素'],\n",
       "  'title': 'エドロホニウム',\n",
       "  'true': ['塩化銀', 'エドロホニウム臭化物', 'ブロモエタン', '3-ジメチルアミノフェノール']},\n",
       " '2614499': {'predict': [], 'title': '2-シクロプロペンカルボン酸', 'true': []},\n",
       " '2727655': {'predict': ['コバルト'], 'title': 'リン化コバルト(II)', 'true': []},\n",
       " '2790003': {'predict': [], 'title': 'チオテパ', 'true': ['アジリジン', '塩化チオホスホリル']},\n",
       " '2795973': {'predict': [], 'title': 'シトロネロール', 'true': ['ゲラニオール']},\n",
       " '2806294': {'predict': ['パクリタキセル'], 'title': 'パチョロール', 'true': []},\n",
       " '2807214': {'predict': [], 'title': 'ホズルシン', 'true': []},\n",
       " '2823300': {'predict': [], 'title': 'ベラトリジン', 'true': []},\n",
       " '293373': {'predict': ['モルヒネ'], 'title': 'フェンタニル', 'true': []},\n",
       " '29891': {'predict': ['オレフィン',\n",
       "   'エーテル',\n",
       "   '酸',\n",
       "   'ハロゲン化合物',\n",
       "   '酸素',\n",
       "   'アルコキシド',\n",
       "   'アルコール'],\n",
       "  'title': 'エーテル (化学)',\n",
       "  'true': ['オレフィン', '求電子剤', '有機ハロゲン化合物', 'アルコキシド', 'アルコール']},\n",
       " '2996837': {'predict': ['エチレンテトラカルボン酸', 'Taherpour', 'メルドラム酸', 'アルケン'],\n",
       "  'title': 'エチレンテトラカルボン酸二無水物',\n",
       "  'true': ['エチレンテトラカルボン酸', 'メルドラム酸']},\n",
       " '3020464': {'predict': ['イソブテン'], 'title': 'オクテン', 'true': []},\n",
       " '305316': {'predict': ['グリセリン', '硫酸マグネシウム', '水'],\n",
       "  'title': '硫酸カルシウム',\n",
       "  'true': ['カルシウム塩の水溶液', '希硫酸', '硫酸塩水溶液', '水酸化カルシウム', '硫酸']},\n",
       " '31121': {'predict': ['水', 'カリフォルニア', 'フロリダ州', 'サンタクルーズ'],\n",
       "  'title': 'DHMO',\n",
       "  'true': []},\n",
       " '3273180': {'predict': [], 'title': 'フルフェナム酸', 'true': []},\n",
       " '3283637': {'predict': ['スモン', 'キノホルム'], 'title': 'クリオキノール', 'true': []},\n",
       " '3342427': {'predict': ['ケリドン酸', '4-ピロン-2,6-ジカルボン酸', 'カルボキシ基', 'アンモニア'],\n",
       "  'title': 'ケリダム酸',\n",
       "  'true': ['4-ピロン-2,6-ジカルボン酸', 'アンモニア', 'ケリドン酸']},\n",
       " '3352098': {'predict': ['酸化ウラン(VI)', '塩素', 'HF', '塩化ウラン(V)', 'UO3', '四塩化炭素'],\n",
       "  'title': '塩化ウラン(VI)',\n",
       "  'true': ['酸化ウラン(VI)', '塩素', '四塩化炭素', 'UO3']},\n",
       " '3419435': {'predict': ['オルメサルタン'], 'title': 'アゼルニジピン', 'true': []},\n",
       " '3448226': {'predict': [], 'title': 'マンノサミン', 'true': []},\n",
       " '3460340': {'predict': ['メサドン'], 'title': 'ネビラピン', 'true': []},\n",
       " '3463386': {'predict': ['炭水化物', 'ヨウ素', '硫黄'],\n",
       "  'title': 'トリフルオロメタンスルホン酸銀',\n",
       "  'true': ['トリフラート', '銀']},\n",
       " '3472884': {'predict': ['パルス', '金属'], 'title': 'REBCO', 'true': []},\n",
       " '3474570': {'predict': ['α-ケトグルタル酸', 'オルニチン', 'オルニチン2-オキソグルタル酸'],\n",
       "  'title': 'オルニチンα‐ケトグルタル酸',\n",
       "  'true': ['オルニチン', 'α-ケトグルタル酸']},\n",
       " '3483975': {'predict': ['四塩化ゲルマニウム', 'クロロゲルマン'],\n",
       "  'title': '二塩化ゲルマニウム',\n",
       "  'true': ['四塩化ゲルマニウム', 'クロロゲルマン', 'GeCl4', 'ゲルマニウム金属']},\n",
       " '3519068': {'predict': ['ジイソプロピルアミン', 'アンモニア'],\n",
       "  'title': 'トリイソプロピルアミン',\n",
       "  'true': ['ジイソプロピルアミン']},\n",
       " '3526170': {'predict': [], 'title': 'リン酸三カルシウム', 'true': ['リン酸', 'カルシウム']},\n",
       " '3529078': {'predict': [], 'title': 'ビベンジル', 'true': ['フェニル基', 'エタン']},\n",
       " '3560674': {'predict': ['炭素', 'カルボン酸'], 'title': '2-モルホリノエタノール', 'true': []},\n",
       " '3570913': {'predict': ['メタン', '塩化チオニル'],\n",
       "  'title': 'メチルホスホン酸ジメチル',\n",
       "  'true': ['亜リン酸トリメチル', 'ハロメタン', 'ヨードメタン']},\n",
       " '3683375': {'predict': [],\n",
       "  'title': 'ミトコンドリアフェリチン',\n",
       "  'true': ['βシート', 'ヘリックス', 'αヘリックス']},\n",
       " '41685': {'predict': ['グルタミン酸', 'カルボキシル基', 'タンパク質'],\n",
       "  'title': 'Γ-アミノ酪酸',\n",
       "  'true': []},\n",
       " '497499': {'predict': ['メチルアミン'],\n",
       "  'title': 'N-メチルピロリドン',\n",
       "  'true': ['γ-ブチロラクトン', 'メチルアミン']},\n",
       " '514541': {'predict': ['水銀(I)塩', '水溶液'], 'title': '酸化水銀', 'true': []},\n",
       " '520951': {'predict': ['1,2-ジクロロエタン', 'アンモニア'],\n",
       "  'title': 'ピペラジン',\n",
       "  'true': ['水酸化ナトリウム', '1,2-ジクロロエタン', 'アンモニア']},\n",
       " '523020': {'predict': ['酸素',\n",
       "   '水素化ホウ素ナトリウム',\n",
       "   'α-ハロケトンや α-ハロアセタール',\n",
       "   '硫黄',\n",
       "   'ヨウ化メチル'],\n",
       "  'title': 'チオアミド',\n",
       "  'true': ['アミド', '五硫化二リン', '硫黄', 'P2S5', 'ローソン試薬']},\n",
       " '616186': {'predict': ['エムルシン', 'ベンズアルデヒド', 'アミグダリン', 'シアン化水素', '青酸'],\n",
       "  'title': 'アミグダリン',\n",
       "  'true': []},\n",
       " '62444': {'predict': ['二酸化硫黄',\n",
       "   'アンモニア',\n",
       "   '塩素',\n",
       "   '糖',\n",
       "   'アイラ・レムセン',\n",
       "   'レムセン',\n",
       "   'アントラニル酸',\n",
       "   '亜硝酸'],\n",
       "  'title': 'サッカリン',\n",
       "  'true': ['塩素', 'アントラニル酸', '二酸化硫黄', 'トルエン', 'アンモニア', '亜硝酸', '2-クロロトルエン']},\n",
       " '62464': {'predict': ['アンモニア', 'シアン酸アンモニウム', '窒素', '水', 'シアヌル酸', '硝酸アンモニウム'],\n",
       "  'title': '尿素',\n",
       "  'true': ['シアン酸アンモニウム', 'アンモニア', '窒素']},\n",
       " '704021': {'predict': ['3-（トリメチルシリロキシ）-1-（トリブチルスタンニル）プロピン',\n",
       "   '1,5-ヘキサジイン',\n",
       "   'マレイン酸ジエチル',\n",
       "   '1,3-ブタジエン',\n",
       "   '3-メトキシ-1'],\n",
       "  'title': 'ロケッテン',\n",
       "  'true': ['n -ブチルリチウム',\n",
       "   '1,3-ブタジエン',\n",
       "   'シクロブタベンゼン誘導体',\n",
       "   'マレイン酸ジエチル',\n",
       "   '3-（トリメチルシリロキシ）-1-（トリブチルスタンニル）プロピン',\n",
       "   '1,5-ヘキサジイン',\n",
       "   '3-メトキシ-1-トリメチルシリルプロピン',\n",
       "   'コバルト錯体']},\n",
       " '778300': {'predict': [], 'title': 'Ε-アミノカプロン酸', 'true': []},\n",
       " '884244': {'predict': ['二クロム酸'], 'title': 'クロム酸', 'true': []},\n",
       " '954930': {'predict': [], 'title': 'ククルビタシン', 'true': []},\n",
       " '969196': {'predict': ['グルコン酸', 'グルコノ', 'グルコース', '天然'],\n",
       "  'title': 'グルコノラクトン',\n",
       "  'true': ['グルコース']}}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 抽出結果をjsonファイルに出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = \"../output/raw-material_with_repl-compounds.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(result_dict, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
